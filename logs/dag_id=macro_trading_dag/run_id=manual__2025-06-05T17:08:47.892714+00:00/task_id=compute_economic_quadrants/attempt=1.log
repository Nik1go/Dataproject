[2025-06-05T19:09:07.326+0200] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-06-05T19:09:07.338+0200] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: macro_trading_dag.compute_economic_quadrants manual__2025-06-05T17:08:47.892714+00:00 [queued]>
[2025-06-05T19:09:07.341+0200] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: macro_trading_dag.compute_economic_quadrants manual__2025-06-05T17:08:47.892714+00:00 [queued]>
[2025-06-05T19:09:07.341+0200] {taskinstance.py:2303} INFO - Starting attempt 1 of 3
[2025-06-05T19:09:07.355+0200] {taskinstance.py:2327} INFO - Executing <Task(SparkSubmitOperator): compute_economic_quadrants> on 2025-06-05 17:08:47.892714+00:00
[2025-06-05T19:09:07.361+0200] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'macro_trading_dag', 'compute_economic_quadrants', 'manual__2025-06-05T17:08:47.892714+00:00', '--job-id', '288', '--raw', '--subdir', 'DAGS_FOLDER/macro_trading_dag.py', '--cfg-path', '/tmp/tmpz40_xbrs']
[2025-06-05T19:09:07.362+0200] {standard_task_runner.py:91} INFO - Job 288: Subtask compute_economic_quadrants
[2025-06-05T19:09:07.369+0200] {logging_mixin.py:188} WARNING - /home/leoja/airflow/airflow_venv/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=543474) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-06-05T19:09:07.369+0200] {standard_task_runner.py:63} INFO - Started process 543485 to run task
[2025-06-05T19:09:07.401+0200] {task_command.py:426} INFO - Running <TaskInstance: macro_trading_dag.compute_economic_quadrants manual__2025-06-05T17:08:47.892714+00:00 [running]> on host N15I711-16GR512.localdomain
[2025-06-05T19:09:07.513+0200] {taskinstance.py:2644} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='macro_trading_dag' AIRFLOW_CTX_TASK_ID='compute_economic_quadrants' AIRFLOW_CTX_EXECUTION_DATE='2025-06-05T17:08:47.892714+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-06-05T17:08:47.892714+00:00'
[2025-06-05T19:09:07.515+0200] {taskinstance.py:430} INFO - ::endgroup::
[2025-06-05T19:09:07.520+0200] {base.py:84} INFO - Using connection ID 'spark_local' for task execution.
[2025-06-05T19:09:07.520+0200] {spark_submit.py:351} INFO - Spark-Submit cmd: spark-submit --master local[*] --conf spark.pyspark.python=/home/leoja/airflow_venv/bin/python --conf spark.pyspark.driver.python=/home/leoja/airflow_venv/bin/python --name compute_economic_quadrants /home/leoja/airflow/spark_jobs/compute_quadrants.py /home/leoja/airflow/data/Indicators.parquet /home/leoja/airflow/data/quadrants.parquet
[2025-06-05T19:09:09.185+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:09 WARN Utils: Your hostname, N15I711-16GR512 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
[2025-06-05T19:09:09.187+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2025-06-05T19:09:10.297+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:10 INFO SparkContext: Running Spark version 3.5.5
[2025-06-05T19:09:10.298+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:10 INFO SparkContext: OS info Linux, 6.6.87.1-microsoft-standard-WSL2, amd64
[2025-06-05T19:09:10.298+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:10 INFO SparkContext: Java version 11.0.27
[2025-06-05T19:09:10.406+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-06-05T19:09:10.511+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:10 INFO ResourceUtils: ==============================================================
[2025-06-05T19:09:10.512+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:10 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-06-05T19:09:10.512+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:10 INFO ResourceUtils: ==============================================================
[2025-06-05T19:09:10.513+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:10 INFO SparkContext: Submitted application: ComputeEconomicQuadrantsIncremental
[2025-06-05T19:09:10.544+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:10 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-06-05T19:09:10.558+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:10 INFO ResourceProfile: Limiting resource is cpu
[2025-06-05T19:09:10.559+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:10 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-06-05T19:09:10.615+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:10 INFO SecurityManager: Changing view acls to: leoja
[2025-06-05T19:09:10.615+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:10 INFO SecurityManager: Changing modify acls to: leoja
[2025-06-05T19:09:10.616+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:10 INFO SecurityManager: Changing view acls groups to:
[2025-06-05T19:09:10.616+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:10 INFO SecurityManager: Changing modify acls groups to:
[2025-06-05T19:09:10.617+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: leoja; groups with view permissions: EMPTY; users with modify permissions: leoja; groups with modify permissions: EMPTY
[2025-06-05T19:09:10.915+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:10 INFO Utils: Successfully started service 'sparkDriver' on port 39859.
[2025-06-05T19:09:10.957+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:10 INFO SparkEnv: Registering MapOutputTracker
[2025-06-05T19:09:11.008+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:11 INFO SparkEnv: Registering BlockManagerMaster
[2025-06-05T19:09:11.037+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:11 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-06-05T19:09:11.037+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:11 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-06-05T19:09:11.042+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:11 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-06-05T19:09:11.066+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:11 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-acd21ca6-bfa3-4ef2-87b1-6c00842bb598
[2025-06-05T19:09:11.078+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:11 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-06-05T19:09:11.092+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:11 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-06-05T19:09:11.221+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:11 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-06-05T19:09:11.274+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-06-05T19:09:11.365+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:11 INFO Executor: Starting executor ID driver on host 10.255.255.254
[2025-06-05T19:09:11.366+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:11 INFO Executor: OS info Linux, 6.6.87.1-microsoft-standard-WSL2, amd64
[2025-06-05T19:09:11.366+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:11 INFO Executor: Java version 11.0.27
[2025-06-05T19:09:11.372+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:11 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-06-05T19:09:11.372+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:11 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@3c562730 for default.
[2025-06-05T19:09:11.399+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43723.
[2025-06-05T19:09:11.399+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:11 INFO NettyBlockTransferService: Server created on 10.255.255.254:43723
[2025-06-05T19:09:11.401+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-06-05T19:09:11.408+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.255.255.254, 43723, None)
[2025-06-05T19:09:11.412+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:11 INFO BlockManagerMasterEndpoint: Registering block manager 10.255.255.254:43723 with 434.4 MiB RAM, BlockManagerId(driver, 10.255.255.254, 43723, None)
[2025-06-05T19:09:11.414+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.255.255.254, 43723, None)
[2025-06-05T19:09:11.415+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.255.255.254, 43723, None)
[2025-06-05T19:09:11.859+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:11 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-06-05T19:09:11.867+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:11 INFO SharedState: Warehouse path is 'file:/home/leoja/airflow/airflow_venv/spark-warehouse'.
[2025-06-05T19:09:12.718+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:12 INFO InMemoryFileIndex: It took 48 ms to list leaf files for 1 paths.
[2025-06-05T19:09:13.206+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:13 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-06-05T19:09:13.223+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:13 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-06-05T19:09:13.224+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:13 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
[2025-06-05T19:09:13.224+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:13 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T19:09:13.225+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:13 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:13.229+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:13 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-06-05T19:09:13.313+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 103.5 KiB, free 434.3 MiB)
[2025-06-05T19:09:13.350+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:13 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.3 MiB)
[2025-06-05T19:09:13.352+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.255.255.254:43723 (size: 37.3 KiB, free: 434.4 MiB)
[2025-06-05T19:09:13.356+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:13 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:13.374+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:13.376+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:13 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-06-05T19:09:13.440+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:13 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9142 bytes)
[2025-06-05T19:09:13.457+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:13 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-06-05T19:09:13.807+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:13 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1931 bytes result sent to driver
[2025-06-05T19:09:13.818+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 401 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:13.819+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:13 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-06-05T19:09:13.823+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:13 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.579 s
[2025-06-05T19:09:13.826+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:13 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T19:09:13.827+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2025-06-05T19:09:13.839+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:13 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.632273 s
[2025-06-05T19:09:13.862+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:13 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.255.255.254:43723 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-06-05T19:09:15.130+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:15 INFO FileSourceStrategy: Pushed Filters:
[2025-06-05T19:09:15.132+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:15 INFO FileSourceStrategy: Post-Scan Filters:
[2025-06-05T19:09:15.690+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:15 INFO CodeGenerator: Code generated in 264.140307 ms
[2025-06-05T19:09:15.727+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:15 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 202.1 KiB, free 434.2 MiB)
[2025-06-05T19:09:15.740+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.3 KiB, free 434.2 MiB)
[2025-06-05T19:09:15.741+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.255.255.254:43723 (size: 35.3 KiB, free: 434.4 MiB)
[2025-06-05T19:09:15.742+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:15 INFO SparkContext: Created broadcast 1 from javaToPython at NativeMethodAccessorImpl.java:0
[2025-06-05T19:09:15.764+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-06-05T19:09:15.840+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:15 INFO CodeGenerator: Code generated in 14.597125 ms
[2025-06-05T19:09:15.926+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:15 INFO SparkContext: Starting job: javaToPython at NativeMethodAccessorImpl.java:0
[2025-06-05T19:09:15.928+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:15 INFO DAGScheduler: Got job 1 (javaToPython at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-06-05T19:09:15.928+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:15 INFO DAGScheduler: Final stage: ResultStage 1 (javaToPython at NativeMethodAccessorImpl.java:0)
[2025-06-05T19:09:15.928+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:15 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T19:09:15.928+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:15 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:15.930+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:15 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-06-05T19:09:15.936+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:15 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 22.7 KiB, free 434.1 MiB)
[2025-06-05T19:09:15.943+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:15 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 434.1 MiB)
[2025-06-05T19:09:15.944+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:15 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.255.255.254:43723 (size: 9.2 KiB, free: 434.4 MiB)
[2025-06-05T19:09:15.945+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:15 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:15.946+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:15.946+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:15 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2025-06-05T19:09:15.951+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:15 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9612 bytes)
[2025-06-05T19:09:15.953+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:15 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2025-06-05T19:09:16.061+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO CodeGenerator: Code generated in 31.313789 ms
[2025-06-05T19:09:16.090+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO CodeGenerator: Code generated in 12.351761 ms
[2025-06-05T19:09:16.095+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T19:09:16.245+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO CodecPool: Got brand-new decompressor [.snappy]
[2025-06-05T19:09:16.455+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 10110 bytes result sent to driver
[2025-06-05T19:09:16.460+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 513 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:16.461+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2025-06-05T19:09:16.463+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO DAGScheduler: ResultStage 1 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.530 s
[2025-06-05T19:09:16.463+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T19:09:16.463+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2025-06-05T19:09:16.464+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO DAGScheduler: Job 1 finished: javaToPython at NativeMethodAccessorImpl.java:0, took 0.537485 s
[2025-06-05T19:09:16.493+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO DAGScheduler: Registering RDD 8 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2025-06-05T19:09:16.499+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO DAGScheduler: Got map stage job 2 (javaToPython at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-06-05T19:09:16.499+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (javaToPython at NativeMethodAccessorImpl.java:0)
[2025-06-05T19:09:16.500+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T19:09:16.501+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:16.503+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[8] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-06-05T19:09:16.520+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 30.9 KiB, free 434.1 MiB)
[2025-06-05T19:09:16.523+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 11.2 KiB, free 434.1 MiB)
[2025-06-05T19:09:16.523+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.255.255.254:43723 (size: 11.2 KiB, free: 434.3 MiB)
[2025-06-05T19:09:16.524+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:16.528+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[8] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:16.528+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2025-06-05T19:09:16.533+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9601 bytes)
[2025-06-05T19:09:16.534+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2025-06-05T19:09:16.560+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO CodeGenerator: Code generated in 6.165821 ms
[2025-06-05T19:09:16.581+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T19:09:16.729+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2242 bytes result sent to driver
[2025-06-05T19:09:16.736+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 204 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:16.736+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-06-05T19:09:16.739+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO DAGScheduler: ShuffleMapStage 2 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.233 s
[2025-06-05T19:09:16.739+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T19:09:16.740+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO DAGScheduler: running: Set()
[2025-06-05T19:09:16.740+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO DAGScheduler: waiting: Set()
[2025-06-05T19:09:16.740+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO DAGScheduler: failed: Set()
[2025-06-05T19:09:16.780+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-06-05T19:09:16.814+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO CodeGenerator: Code generated in 16.723834 ms
[2025-06-05T19:09:16.934+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181
[2025-06-05T19:09:16.939+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO DAGScheduler: Got job 3 (runJob at PythonRDD.scala:181) with 1 output partitions
[2025-06-05T19:09:16.940+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO DAGScheduler: Final stage: ResultStage 4 (runJob at PythonRDD.scala:181)
[2025-06-05T19:09:16.940+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
[2025-06-05T19:09:16.943+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:16.944+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO DAGScheduler: Submitting ResultStage 4 (PythonRDD[14] at RDD at PythonRDD.scala:53), which has no missing parents
[2025-06-05T19:09:16.966+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 38.9 KiB, free 434.1 MiB)
[2025-06-05T19:09:16.970+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 18.9 KiB, free 434.0 MiB)
[2025-06-05T19:09:16.972+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.255.255.254:43723 (size: 18.9 KiB, free: 434.3 MiB)
[2025-06-05T19:09:16.974+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:16.975+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (PythonRDD[14] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:16.975+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2025-06-05T19:09:16.981+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8999 bytes)
[2025-06-05T19:09:16.982+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:16 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
[2025-06-05T19:09:17.063+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:17 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.255.255.254:43723 in memory (size: 9.2 KiB, free: 434.3 MiB)
[2025-06-05T19:09:17.069+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:17 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.255.255.254:43723 in memory (size: 11.2 KiB, free: 434.3 MiB)
[2025-06-05T19:09:17.082+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:17 INFO ShuffleBlockFetcherIterator: Getting 1 (22.6 KiB) non-empty blocks including 1 (22.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T19:09:17.084+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 19 ms
[2025-06-05T19:09:17.101+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:17 INFO CodeGenerator: Code generated in 13.895145 ms
[2025-06-05T19:09:17.126+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:17 INFO CodeGenerator: Code generated in 9.220862 ms
[2025-06-05T19:09:17.894+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:17 INFO PythonRunner: Times: total = 740, boot = 609, init = 130, finish = 1
[2025-06-05T19:09:17.898+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:17 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 5219 bytes result sent to driver
[2025-06-05T19:09:17.901+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:17 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 921 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:17.901+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:17 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2025-06-05T19:09:17.903+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:17 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 52053
[2025-06-05T19:09:17.907+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:17 INFO DAGScheduler: ResultStage 4 (runJob at PythonRDD.scala:181) finished in 0.947 s
[2025-06-05T19:09:17.908+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:17 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T19:09:17.908+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2025-06-05T19:09:17.908+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:17 INFO DAGScheduler: Job 3 finished: runJob at PythonRDD.scala:181, took 0.973418 s
[2025-06-05T19:09:18.535+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:18 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.255.255.254:43723 in memory (size: 18.9 KiB, free: 434.4 MiB)
[2025-06-05T19:09:18.939+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:18 INFO FileSourceStrategy: Pushed Filters:
[2025-06-05T19:09:18.940+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:18 INFO FileSourceStrategy: Post-Scan Filters:
[2025-06-05T19:09:19.011+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO CodeGenerator: Code generated in 14.889216 ms
[2025-06-05T19:09:19.017+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 201.4 KiB, free 434.0 MiB)
[2025-06-05T19:09:19.028+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 433.9 MiB)
[2025-06-05T19:09:19.028+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.255.255.254:43723 (size: 35.0 KiB, free: 434.3 MiB)
[2025-06-05T19:09:19.029+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO SparkContext: Created broadcast 5 from first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93
[2025-06-05T19:09:19.031+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-06-05T19:09:19.072+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93
[2025-06-05T19:09:19.074+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Got job 4 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) with 1 output partitions
[2025-06-05T19:09:19.075+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Final stage: ResultStage 5 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93)
[2025-06-05T19:09:19.075+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T19:09:19.076+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:19.078+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[20] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93), which has no missing parents
[2025-06-05T19:09:19.082+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 19.3 KiB, free 433.9 MiB)
[2025-06-05T19:09:19.083+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 433.9 MiB)
[2025-06-05T19:09:19.084+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.255.255.254:43723 (size: 8.7 KiB, free: 434.3 MiB)
[2025-06-05T19:09:19.084+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:19.085+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[20] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:19.085+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2025-06-05T19:09:19.087+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9612 bytes)
[2025-06-05T19:09:19.087+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)
[2025-06-05T19:09:19.109+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO CodeGenerator: Code generated in 12.900762 ms
[2025-06-05T19:09:19.113+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T19:09:19.133+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 10024 bytes result sent to driver
[2025-06-05T19:09:19.135+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 49 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:19.135+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2025-06-05T19:09:19.136+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: ResultStage 5 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) finished in 0.057 s
[2025-06-05T19:09:19.136+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T19:09:19.137+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2025-06-05T19:09:19.137+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Job 4 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93, took 0.064351 s
[2025-06-05T19:09:19.141+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Registering RDD 21 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) as input to shuffle 1
[2025-06-05T19:09:19.143+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Got map stage job 5 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) with 1 output partitions
[2025-06-05T19:09:19.143+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Final stage: ShuffleMapStage 6 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93)
[2025-06-05T19:09:19.143+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T19:09:19.144+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:19.145+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[21] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93), which has no missing parents
[2025-06-05T19:09:19.151+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 27.6 KiB, free 433.9 MiB)
[2025-06-05T19:09:19.153+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 433.9 MiB)
[2025-06-05T19:09:19.153+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.255.255.254:43723 (size: 10.7 KiB, free: 434.3 MiB)
[2025-06-05T19:09:19.154+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:19.155+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[21] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:19.155+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2025-06-05T19:09:19.157+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9601 bytes)
[2025-06-05T19:09:19.158+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO Executor: Running task 0.0 in stage 6.0 (TID 5)
[2025-06-05T19:09:19.169+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T19:09:19.230+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO Executor: Finished task 0.0 in stage 6.0 (TID 5). 2199 bytes result sent to driver
[2025-06-05T19:09:19.232+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 75 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:19.232+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2025-06-05T19:09:19.233+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: ShuffleMapStage 6 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) finished in 0.087 s
[2025-06-05T19:09:19.233+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T19:09:19.234+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: running: Set()
[2025-06-05T19:09:19.234+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: waiting: Set()
[2025-06-05T19:09:19.234+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: failed: Set()
[2025-06-05T19:09:19.240+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-06-05T19:09:19.267+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO CodeGenerator: Code generated in 13.571303 ms
[2025-06-05T19:09:19.279+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Registering RDD 25 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) as input to shuffle 2
[2025-06-05T19:09:19.280+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Got map stage job 6 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) with 1 output partitions
[2025-06-05T19:09:19.280+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Final stage: ShuffleMapStage 8 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93)
[2025-06-05T19:09:19.281+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
[2025-06-05T19:09:19.282+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:19.283+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[25] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93), which has no missing parents
[2025-06-05T19:09:19.309+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 35.4 KiB, free 433.8 MiB)
[2025-06-05T19:09:19.334+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 17.1 KiB, free 433.8 MiB)
[2025-06-05T19:09:19.338+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 10.255.255.254:43723 in memory (size: 10.7 KiB, free: 434.3 MiB)
[2025-06-05T19:09:19.342+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.255.255.254:43723 (size: 17.1 KiB, free: 434.3 MiB)
[2025-06-05T19:09:19.345+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:19.351+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[25] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:19.352+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2025-06-05T19:09:19.353+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8988 bytes)
[2025-06-05T19:09:19.353+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 10.255.255.254:43723 in memory (size: 8.7 KiB, free: 434.3 MiB)
[2025-06-05T19:09:19.354+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO Executor: Running task 0.0 in stage 8.0 (TID 6)
[2025-06-05T19:09:19.369+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO ShuffleBlockFetcherIterator: Getting 1 (13.5 KiB) non-empty blocks including 1 (13.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T19:09:19.369+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-06-05T19:09:19.404+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO CodeGenerator: Code generated in 34.513052 ms
[2025-06-05T19:09:19.443+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO CodeGenerator: Code generated in 5.900357 ms
[2025-06-05T19:09:19.456+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO CodeGenerator: Code generated in 5.600061 ms
[2025-06-05T19:09:19.465+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO CodeGenerator: Code generated in 5.104079 ms
[2025-06-05T19:09:19.492+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO Executor: Finished task 0.0 in stage 8.0 (TID 6). 5170 bytes result sent to driver
[2025-06-05T19:09:19.493+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 141 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:19.494+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2025-06-05T19:09:19.495+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: ShuffleMapStage 8 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) finished in 0.205 s
[2025-06-05T19:09:19.495+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T19:09:19.495+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: running: Set()
[2025-06-05T19:09:19.495+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: waiting: Set()
[2025-06-05T19:09:19.495+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: failed: Set()
[2025-06-05T19:09:19.530+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93
[2025-06-05T19:09:19.531+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Got job 7 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) with 1 output partitions
[2025-06-05T19:09:19.532+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Final stage: ResultStage 11 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93)
[2025-06-05T19:09:19.532+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
[2025-06-05T19:09:19.533+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:19.534+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[28] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93), which has no missing parents
[2025-06-05T19:09:19.540+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 34.9 KiB, free 433.9 MiB)
[2025-06-05T19:09:19.541+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 16.6 KiB, free 433.8 MiB)
[2025-06-05T19:09:19.541+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.255.255.254:43723 (size: 16.6 KiB, free: 434.3 MiB)
[2025-06-05T19:09:19.542+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:19.543+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[28] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:19.543+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2025-06-05T19:09:19.544+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 7) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8999 bytes)
[2025-06-05T19:09:19.545+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO Executor: Running task 0.0 in stage 11.0 (TID 7)
[2025-06-05T19:09:19.562+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO ShuffleBlockFetcherIterator: Getting 1 (2.2 KiB) non-empty blocks including 1 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T19:09:19.563+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T19:09:19.577+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO CodeGenerator: Code generated in 5.314241 ms
[2025-06-05T19:09:19.598+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 10.255.255.254:43723 in memory (size: 17.1 KiB, free: 434.3 MiB)
[2025-06-05T19:09:19.602+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO Executor: Finished task 0.0 in stage 11.0 (TID 7). 6523 bytes result sent to driver
[2025-06-05T19:09:19.605+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 7) in 61 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:19.606+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2025-06-05T19:09:19.607+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: ResultStage 11 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) finished in 0.070 s
[2025-06-05T19:09:19.607+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T19:09:19.607+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
[2025-06-05T19:09:19.608+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Job 7 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93, took 0.077604 s
[2025-06-05T19:09:19.783+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO FileSourceStrategy: Pushed Filters:
[2025-06-05T19:09:19.783+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO FileSourceStrategy: Post-Scan Filters:
[2025-06-05T19:09:19.815+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.4 KiB, free 433.7 MiB)
[2025-06-05T19:09:19.824+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 433.7 MiB)
[2025-06-05T19:09:19.825+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.255.255.254:43723 (size: 35.0 KiB, free: 434.3 MiB)
[2025-06-05T19:09:19.826+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO SparkContext: Created broadcast 10 from first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94
[2025-06-05T19:09:19.828+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-06-05T19:09:19.850+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94
[2025-06-05T19:09:19.851+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Got job 8 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) with 1 output partitions
[2025-06-05T19:09:19.851+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Final stage: ResultStage 12 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94)
[2025-06-05T19:09:19.851+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T19:09:19.851+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:19.852+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[34] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94), which has no missing parents
[2025-06-05T19:09:19.854+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 19.3 KiB, free 433.6 MiB)
[2025-06-05T19:09:19.857+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 433.6 MiB)
[2025-06-05T19:09:19.858+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.255.255.254:43723 (size: 8.6 KiB, free: 434.3 MiB)
[2025-06-05T19:09:19.859+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:19.860+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[34] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:19.860+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
[2025-06-05T19:09:19.862+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 8) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9612 bytes)
[2025-06-05T19:09:19.862+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO Executor: Running task 0.0 in stage 12.0 (TID 8)
[2025-06-05T19:09:19.868+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T19:09:19.881+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO Executor: Finished task 0.0 in stage 12.0 (TID 8). 10024 bytes result sent to driver
[2025-06-05T19:09:19.883+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 8) in 22 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:19.883+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2025-06-05T19:09:19.884+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: ResultStage 12 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) finished in 0.032 s
[2025-06-05T19:09:19.884+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T19:09:19.884+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
[2025-06-05T19:09:19.885+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Job 8 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94, took 0.034918 s
[2025-06-05T19:09:19.888+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Registering RDD 35 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) as input to shuffle 3
[2025-06-05T19:09:19.888+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Got map stage job 9 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) with 1 output partitions
[2025-06-05T19:09:19.888+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Final stage: ShuffleMapStage 13 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94)
[2025-06-05T19:09:19.888+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T19:09:19.889+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:19.890+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[35] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94), which has no missing parents
[2025-06-05T19:09:19.895+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 27.6 KiB, free 433.6 MiB)
[2025-06-05T19:09:19.904+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 433.6 MiB)
[2025-06-05T19:09:19.904+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.255.255.254:43723 (size: 10.7 KiB, free: 434.3 MiB)
[2025-06-05T19:09:19.905+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 10.255.255.254:43723 in memory (size: 8.6 KiB, free: 434.3 MiB)
[2025-06-05T19:09:19.905+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:19.907+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[35] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:19.908+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2025-06-05T19:09:19.911+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 9) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9601 bytes)
[2025-06-05T19:09:19.912+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO Executor: Running task 0.0 in stage 13.0 (TID 9)
[2025-06-05T19:09:19.914+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 10.255.255.254:43723 in memory (size: 16.6 KiB, free: 434.3 MiB)
[2025-06-05T19:09:19.920+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T19:09:19.989+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO Executor: Finished task 0.0 in stage 13.0 (TID 9). 2199 bytes result sent to driver
[2025-06-05T19:09:19.990+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 9) in 80 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:19.991+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2025-06-05T19:09:19.992+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: ShuffleMapStage 13 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) finished in 0.100 s
[2025-06-05T19:09:19.992+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T19:09:19.993+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: running: Set()
[2025-06-05T19:09:19.993+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: waiting: Set()
[2025-06-05T19:09:19.993+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:19 INFO DAGScheduler: failed: Set()
[2025-06-05T19:09:20.001+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-06-05T19:09:20.056+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO CodeGenerator: Code generated in 18.826361 ms
[2025-06-05T19:09:20.081+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Registering RDD 38 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) as input to shuffle 4
[2025-06-05T19:09:20.081+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Got map stage job 10 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) with 1 output partitions
[2025-06-05T19:09:20.081+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Final stage: ShuffleMapStage 15 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94)
[2025-06-05T19:09:20.082+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)
[2025-06-05T19:09:20.082+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:20.083+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[38] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94), which has no missing parents
[2025-06-05T19:09:20.090+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 45.0 KiB, free 433.6 MiB)
[2025-06-05T19:09:20.091+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 433.6 MiB)
[2025-06-05T19:09:20.091+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.255.255.254:43723 (size: 20.4 KiB, free: 434.3 MiB)
[2025-06-05T19:09:20.092+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:20.094+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[38] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:20.095+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
[2025-06-05T19:09:20.097+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 10) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8988 bytes)
[2025-06-05T19:09:20.098+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO Executor: Running task 0.0 in stage 15.0 (TID 10)
[2025-06-05T19:09:20.117+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO ShuffleBlockFetcherIterator: Getting 1 (13.5 KiB) non-empty blocks including 1 (13.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T19:09:20.117+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-06-05T19:09:20.148+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO CodeGenerator: Code generated in 30.022608 ms
[2025-06-05T19:09:20.169+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO Executor: Finished task 0.0 in stage 15.0 (TID 10). 5314 bytes result sent to driver
[2025-06-05T19:09:20.171+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 10) in 73 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:20.171+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
[2025-06-05T19:09:20.172+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: ShuffleMapStage 15 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) finished in 0.085 s
[2025-06-05T19:09:20.172+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T19:09:20.173+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: running: Set()
[2025-06-05T19:09:20.173+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: waiting: Set()
[2025-06-05T19:09:20.173+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: failed: Set()
[2025-06-05T19:09:20.223+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO CodeGenerator: Code generated in 22.068146 ms
[2025-06-05T19:09:20.245+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94
[2025-06-05T19:09:20.247+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Got job 11 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) with 1 output partitions
[2025-06-05T19:09:20.248+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Final stage: ResultStage 18 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94)
[2025-06-05T19:09:20.248+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)
[2025-06-05T19:09:20.248+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:20.249+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[41] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94), which has no missing parents
[2025-06-05T19:09:20.252+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 16.9 KiB, free 433.6 MiB)
[2025-06-05T19:09:20.274+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 433.6 MiB)
[2025-06-05T19:09:20.275+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.255.255.254:43723 (size: 7.2 KiB, free: 434.3 MiB)
[2025-06-05T19:09:20.275+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:20.276+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 10.255.255.254:43723 in memory (size: 10.7 KiB, free: 434.3 MiB)
[2025-06-05T19:09:20.277+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[41] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:20.278+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
[2025-06-05T19:09:20.280+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 11) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8999 bytes)
[2025-06-05T19:09:20.282+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 10.255.255.254:43723 in memory (size: 20.4 KiB, free: 434.3 MiB)
[2025-06-05T19:09:20.283+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO Executor: Running task 0.0 in stage 18.0 (TID 11)
[2025-06-05T19:09:20.291+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T19:09:20.291+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-06-05T19:09:20.315+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO CodeGenerator: Code generated in 23.142446 ms
[2025-06-05T19:09:20.338+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO Executor: Finished task 0.0 in stage 18.0 (TID 11). 3995 bytes result sent to driver
[2025-06-05T19:09:20.340+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 11) in 60 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:20.340+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
[2025-06-05T19:09:20.341+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: ResultStage 18 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) finished in 0.091 s
[2025-06-05T19:09:20.341+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T19:09:20.342+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
[2025-06-05T19:09:20.342+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Job 11 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94, took 0.096745 s
[2025-06-05T19:09:20.470+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO FileSourceStrategy: Pushed Filters:
[2025-06-05T19:09:20.470+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO FileSourceStrategy: Post-Scan Filters:
[2025-06-05T19:09:20.499+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 201.4 KiB, free 433.5 MiB)
[2025-06-05T19:09:20.521+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 10.255.255.254:43723 in memory (size: 7.2 KiB, free: 434.3 MiB)
[2025-06-05T19:09:20.526+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 433.5 MiB)
[2025-06-05T19:09:20.527+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.255.255.254:43723 (size: 35.0 KiB, free: 434.3 MiB)
[2025-06-05T19:09:20.529+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO SparkContext: Created broadcast 15 from first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93
[2025-06-05T19:09:20.531+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-06-05T19:09:20.571+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93
[2025-06-05T19:09:20.574+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Got job 12 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) with 1 output partitions
[2025-06-05T19:09:20.574+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Final stage: ResultStage 19 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93)
[2025-06-05T19:09:20.574+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T19:09:20.575+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:20.576+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[47] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93), which has no missing parents
[2025-06-05T19:09:20.578+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 19.3 KiB, free 433.5 MiB)
[2025-06-05T19:09:20.582+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 433.4 MiB)
[2025-06-05T19:09:20.582+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.255.255.254:43723 (size: 8.7 KiB, free: 434.3 MiB)
[2025-06-05T19:09:20.583+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:20.584+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[47] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:20.584+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
[2025-06-05T19:09:20.586+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 12) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9612 bytes)
[2025-06-05T19:09:20.587+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO Executor: Running task 0.0 in stage 19.0 (TID 12)
[2025-06-05T19:09:20.595+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T19:09:20.608+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO Executor: Finished task 0.0 in stage 19.0 (TID 12). 10024 bytes result sent to driver
[2025-06-05T19:09:20.609+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 12) in 24 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:20.610+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
[2025-06-05T19:09:20.610+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: ResultStage 19 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) finished in 0.034 s
[2025-06-05T19:09:20.611+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T19:09:20.611+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished
[2025-06-05T19:09:20.611+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Job 12 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93, took 0.039487 s
[2025-06-05T19:09:20.614+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Registering RDD 48 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) as input to shuffle 5
[2025-06-05T19:09:20.615+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Got map stage job 13 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) with 1 output partitions
[2025-06-05T19:09:20.615+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Final stage: ShuffleMapStage 20 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93)
[2025-06-05T19:09:20.615+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T19:09:20.615+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:20.616+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[48] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93), which has no missing parents
[2025-06-05T19:09:20.620+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 27.6 KiB, free 433.4 MiB)
[2025-06-05T19:09:20.630+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 433.4 MiB)
[2025-06-05T19:09:20.631+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.255.255.254:43723 (size: 10.7 KiB, free: 434.2 MiB)
[2025-06-05T19:09:20.632+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 10.255.255.254:43723 in memory (size: 8.7 KiB, free: 434.3 MiB)
[2025-06-05T19:09:20.634+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:20.636+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[48] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:20.636+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
[2025-06-05T19:09:20.636+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 13) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9601 bytes)
[2025-06-05T19:09:20.636+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO Executor: Running task 0.0 in stage 20.0 (TID 13)
[2025-06-05T19:09:20.647+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T19:09:20.725+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO Executor: Finished task 0.0 in stage 20.0 (TID 13). 2199 bytes result sent to driver
[2025-06-05T19:09:20.726+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 13) in 92 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:20.726+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool
[2025-06-05T19:09:20.727+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: ShuffleMapStage 20 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) finished in 0.111 s
[2025-06-05T19:09:20.727+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T19:09:20.728+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: running: Set()
[2025-06-05T19:09:20.728+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: waiting: Set()
[2025-06-05T19:09:20.728+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: failed: Set()
[2025-06-05T19:09:20.735+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-06-05T19:09:20.761+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Registering RDD 52 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) as input to shuffle 6
[2025-06-05T19:09:20.761+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Got map stage job 14 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) with 1 output partitions
[2025-06-05T19:09:20.762+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Final stage: ShuffleMapStage 22 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93)
[2025-06-05T19:09:20.762+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)
[2025-06-05T19:09:20.763+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:20.763+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[52] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93), which has no missing parents
[2025-06-05T19:09:20.773+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 35.4 KiB, free 433.4 MiB)
[2025-06-05T19:09:20.775+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 17.1 KiB, free 433.4 MiB)
[2025-06-05T19:09:20.775+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.255.255.254:43723 (size: 17.1 KiB, free: 434.2 MiB)
[2025-06-05T19:09:20.776+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:20.777+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[52] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:20.777+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
[2025-06-05T19:09:20.779+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 14) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8988 bytes)
[2025-06-05T19:09:20.780+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO Executor: Running task 0.0 in stage 22.0 (TID 14)
[2025-06-05T19:09:20.791+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO ShuffleBlockFetcherIterator: Getting 1 (13.5 KiB) non-empty blocks including 1 (13.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T19:09:20.791+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-06-05T19:09:20.821+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO Executor: Finished task 0.0 in stage 22.0 (TID 14). 5256 bytes result sent to driver
[2025-06-05T19:09:20.823+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 14) in 44 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:20.823+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool
[2025-06-05T19:09:20.824+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 10.255.255.254:43723 in memory (size: 10.7 KiB, free: 434.2 MiB)
[2025-06-05T19:09:20.824+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: ShuffleMapStage 22 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) finished in 0.057 s
[2025-06-05T19:09:20.825+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T19:09:20.828+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: running: Set()
[2025-06-05T19:09:20.828+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: waiting: Set()
[2025-06-05T19:09:20.829+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: failed: Set()
[2025-06-05T19:09:20.848+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93
[2025-06-05T19:09:20.849+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Got job 15 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) with 1 output partitions
[2025-06-05T19:09:20.850+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Final stage: ResultStage 25 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93)
[2025-06-05T19:09:20.850+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 24)
[2025-06-05T19:09:20.850+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:20.851+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[55] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93), which has no missing parents
[2025-06-05T19:09:20.855+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 35.0 KiB, free 433.4 MiB)
[2025-06-05T19:09:20.856+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 16.6 KiB, free 433.4 MiB)
[2025-06-05T19:09:20.857+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.255.255.254:43723 (size: 16.6 KiB, free: 434.2 MiB)
[2025-06-05T19:09:20.857+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:20.869+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[55] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:20.870+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
[2025-06-05T19:09:20.872+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 15) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8999 bytes)
[2025-06-05T19:09:20.873+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO Executor: Running task 0.0 in stage 25.0 (TID 15)
[2025-06-05T19:09:20.881+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO ShuffleBlockFetcherIterator: Getting 1 (717.0 B) non-empty blocks including 1 (717.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T19:09:20.882+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T19:09:20.891+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO Executor: Finished task 0.0 in stage 25.0 (TID 15). 6431 bytes result sent to driver
[2025-06-05T19:09:20.893+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 15) in 21 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:20.893+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool
[2025-06-05T19:09:20.895+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: ResultStage 25 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) finished in 0.043 s
[2025-06-05T19:09:20.895+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T19:09:20.895+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished
[2025-06-05T19:09:20.896+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Job 15 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93, took 0.047873 s
[2025-06-05T19:09:20.972+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO FileSourceStrategy: Pushed Filters:
[2025-06-05T19:09:20.972+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO FileSourceStrategy: Post-Scan Filters:
[2025-06-05T19:09:20.989+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 201.4 KiB, free 433.2 MiB)
[2025-06-05T19:09:21.004+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 10.255.255.254:43723 in memory (size: 16.6 KiB, free: 434.2 MiB)
[2025-06-05T19:09:21.008+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 433.2 MiB)
[2025-06-05T19:09:21.008+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 10.255.255.254:43723 in memory (size: 17.1 KiB, free: 434.3 MiB)
[2025-06-05T19:09:21.009+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.255.255.254:43723 (size: 35.0 KiB, free: 434.2 MiB)
[2025-06-05T19:09:21.010+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO SparkContext: Created broadcast 20 from first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94
[2025-06-05T19:09:21.011+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-06-05T19:09:21.023+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94
[2025-06-05T19:09:21.024+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Got job 16 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) with 1 output partitions
[2025-06-05T19:09:21.024+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Final stage: ResultStage 26 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94)
[2025-06-05T19:09:21.025+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T19:09:21.025+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:21.025+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[61] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94), which has no missing parents
[2025-06-05T19:09:21.028+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 19.3 KiB, free 433.2 MiB)
[2025-06-05T19:09:21.029+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 433.2 MiB)
[2025-06-05T19:09:21.029+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.255.255.254:43723 (size: 8.7 KiB, free: 434.2 MiB)
[2025-06-05T19:09:21.030+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:21.031+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[61] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:21.031+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
[2025-06-05T19:09:21.040+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 16) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9612 bytes)
[2025-06-05T19:09:21.041+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO Executor: Running task 0.0 in stage 26.0 (TID 16)
[2025-06-05T19:09:21.047+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T19:09:21.055+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO Executor: Finished task 0.0 in stage 26.0 (TID 16). 10024 bytes result sent to driver
[2025-06-05T19:09:21.056+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 16) in 24 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:21.057+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool
[2025-06-05T19:09:21.057+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: ResultStage 26 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) finished in 0.031 s
[2025-06-05T19:09:21.057+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T19:09:21.057+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
[2025-06-05T19:09:21.058+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Job 16 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94, took 0.034707 s
[2025-06-05T19:09:21.061+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Registering RDD 62 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) as input to shuffle 7
[2025-06-05T19:09:21.061+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Got map stage job 17 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) with 1 output partitions
[2025-06-05T19:09:21.061+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Final stage: ShuffleMapStage 27 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94)
[2025-06-05T19:09:21.061+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T19:09:21.061+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:21.062+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[62] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94), which has no missing parents
[2025-06-05T19:09:21.065+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 27.6 KiB, free 433.2 MiB)
[2025-06-05T19:09:21.072+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 433.2 MiB)
[2025-06-05T19:09:21.073+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.255.255.254:43723 (size: 10.7 KiB, free: 434.2 MiB)
[2025-06-05T19:09:21.074+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:21.075+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[62] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:21.075+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 10.255.255.254:43723 in memory (size: 8.7 KiB, free: 434.2 MiB)
[2025-06-05T19:09:21.075+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
[2025-06-05T19:09:21.077+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 17) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9601 bytes)
[2025-06-05T19:09:21.077+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO Executor: Running task 0.0 in stage 27.0 (TID 17)
[2025-06-05T19:09:21.086+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T19:09:21.135+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO Executor: Finished task 0.0 in stage 27.0 (TID 17). 2199 bytes result sent to driver
[2025-06-05T19:09:20.549+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 17) in -528 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:20.549+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool
[2025-06-05T19:09:20.549+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: ShuffleMapStage 27 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) finished in -0.513 s
[2025-06-05T19:09:20.550+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T19:09:20.550+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: running: Set()
[2025-06-05T19:09:20.550+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: waiting: Set()
[2025-06-05T19:09:20.550+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: failed: Set()
[2025-06-05T19:09:20.556+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO ShufflePartitionsUtil: For shuffle(7), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-06-05T19:09:20.575+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Registering RDD 65 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) as input to shuffle 8
[2025-06-05T19:09:20.576+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Got map stage job 18 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) with 1 output partitions
[2025-06-05T19:09:20.576+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Final stage: ShuffleMapStage 29 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94)
[2025-06-05T19:09:20.576+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 28)
[2025-06-05T19:09:20.576+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:20.576+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[65] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94), which has no missing parents
[2025-06-05T19:09:20.580+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 45.0 KiB, free 433.2 MiB)
[2025-06-05T19:09:20.594+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 20.5 KiB, free 433.1 MiB)
[2025-06-05T19:09:20.595+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.255.255.254:43723 (size: 20.5 KiB, free: 434.2 MiB)
[2025-06-05T19:09:20.595+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:20.598+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[65] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:20.599+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0
[2025-06-05T19:09:20.599+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 10.255.255.254:43723 in memory (size: 10.7 KiB, free: 434.2 MiB)
[2025-06-05T19:09:20.600+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 18) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8988 bytes)
[2025-06-05T19:09:20.602+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO Executor: Running task 0.0 in stage 29.0 (TID 18)
[2025-06-05T19:09:20.612+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO ShuffleBlockFetcherIterator: Getting 1 (13.5 KiB) non-empty blocks including 1 (13.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T19:09:20.612+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-06-05T19:09:20.622+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO Executor: Finished task 0.0 in stage 29.0 (TID 18). 5314 bytes result sent to driver
[2025-06-05T19:09:20.623+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 18) in 23 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:20.623+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool
[2025-06-05T19:09:20.624+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: ShuffleMapStage 29 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) finished in 0.046 s
[2025-06-05T19:09:20.624+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T19:09:20.624+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: running: Set()
[2025-06-05T19:09:20.625+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: waiting: Set()
[2025-06-05T19:09:20.625+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: failed: Set()
[2025-06-05T19:09:20.659+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94
[2025-06-05T19:09:20.660+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Got job 19 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) with 1 output partitions
[2025-06-05T19:09:20.660+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Final stage: ResultStage 32 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94)
[2025-06-05T19:09:20.661+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 31)
[2025-06-05T19:09:20.661+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:20.661+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[68] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94), which has no missing parents
[2025-06-05T19:09:20.663+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 16.9 KiB, free 433.2 MiB)
[2025-06-05T19:09:20.672+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 433.2 MiB)
[2025-06-05T19:09:20.673+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.255.255.254:43723 (size: 7.2 KiB, free: 434.2 MiB)
[2025-06-05T19:09:20.674+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 10.255.255.254:43723 in memory (size: 20.5 KiB, free: 434.2 MiB)
[2025-06-05T19:09:20.674+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:20.676+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[68] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:20.677+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0
[2025-06-05T19:09:20.678+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 19) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8999 bytes)
[2025-06-05T19:09:20.679+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO Executor: Running task 0.0 in stage 32.0 (TID 19)
[2025-06-05T19:09:20.684+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T19:09:20.684+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T19:09:20.687+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO Executor: Finished task 0.0 in stage 32.0 (TID 19). 3995 bytes result sent to driver
[2025-06-05T19:09:20.690+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 19) in 11 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:20.690+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool
[2025-06-05T19:09:20.690+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: ResultStage 32 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) finished in 0.028 s
[2025-06-05T19:09:20.691+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T19:09:20.691+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished
[2025-06-05T19:09:20.691+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Job 19 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94, took 0.031923 s
[2025-06-05T19:09:20.809+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO FileSourceStrategy: Pushed Filters:
[2025-06-05T19:09:20.809+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO FileSourceStrategy: Post-Scan Filters:
[2025-06-05T19:09:20.828+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 201.4 KiB, free 433.0 MiB)
[2025-06-05T19:09:20.839+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 433.0 MiB)
[2025-06-05T19:09:20.840+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.255.255.254:43723 (size: 35.0 KiB, free: 434.2 MiB)
[2025-06-05T19:09:20.841+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO SparkContext: Created broadcast 25 from first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93
[2025-06-05T19:09:20.842+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-06-05T19:09:20.861+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93
[2025-06-05T19:09:20.862+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Got job 20 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) with 1 output partitions
[2025-06-05T19:09:20.862+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Final stage: ResultStage 33 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93)
[2025-06-05T19:09:20.862+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T19:09:20.862+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:20.863+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[74] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93), which has no missing parents
[2025-06-05T19:09:20.865+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 19.3 KiB, free 433.0 MiB)
[2025-06-05T19:09:20.866+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 433.0 MiB)
[2025-06-05T19:09:20.866+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.255.255.254:43723 (size: 8.7 KiB, free: 434.2 MiB)
[2025-06-05T19:09:20.867+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:20.868+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[74] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:20.868+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0
[2025-06-05T19:09:20.869+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 20) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9612 bytes)
[2025-06-05T19:09:20.870+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO Executor: Running task 0.0 in stage 33.0 (TID 20)
[2025-06-05T19:09:20.876+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T19:09:20.886+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO Executor: Finished task 0.0 in stage 33.0 (TID 20). 10024 bytes result sent to driver
[2025-06-05T19:09:20.888+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 20) in 19 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:20.888+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool
[2025-06-05T19:09:20.889+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: ResultStage 33 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) finished in 0.025 s
[2025-06-05T19:09:20.889+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T19:09:20.889+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished
[2025-06-05T19:09:20.889+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Job 20 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93, took 0.028731 s
[2025-06-05T19:09:20.893+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Registering RDD 75 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) as input to shuffle 9
[2025-06-05T19:09:20.894+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Got map stage job 21 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) with 1 output partitions
[2025-06-05T19:09:20.894+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Final stage: ShuffleMapStage 34 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93)
[2025-06-05T19:09:20.894+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T19:09:20.894+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:20.895+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Submitting ShuffleMapStage 34 (MapPartitionsRDD[75] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93), which has no missing parents
[2025-06-05T19:09:20.899+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 27.6 KiB, free 432.9 MiB)
[2025-06-05T19:09:20.907+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 432.9 MiB)
[2025-06-05T19:09:20.907+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.255.255.254:43723 (size: 10.7 KiB, free: 434.2 MiB)
[2025-06-05T19:09:20.910+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:20.910+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 10.255.255.254:43723 in memory (size: 8.7 KiB, free: 434.2 MiB)
[2025-06-05T19:09:20.910+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 34 (MapPartitionsRDD[75] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:20.910+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks resource profile 0
[2025-06-05T19:09:20.912+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 21) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9601 bytes)
[2025-06-05T19:09:20.913+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO Executor: Running task 0.0 in stage 34.0 (TID 21)
[2025-06-05T19:09:20.914+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 10.255.255.254:43723 in memory (size: 7.2 KiB, free: 434.2 MiB)
[2025-06-05T19:09:20.921+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T19:09:20.966+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO Executor: Finished task 0.0 in stage 34.0 (TID 21). 2199 bytes result sent to driver
[2025-06-05T19:09:20.967+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 21) in 56 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:20.967+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool
[2025-06-05T19:09:20.968+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: ShuffleMapStage 34 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) finished in 0.071 s
[2025-06-05T19:09:20.968+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T19:09:20.968+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: running: Set()
[2025-06-05T19:09:20.968+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: waiting: Set()
[2025-06-05T19:09:20.968+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: failed: Set()
[2025-06-05T19:09:20.972+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO ShufflePartitionsUtil: For shuffle(9), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-06-05T19:09:20.989+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Registering RDD 79 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) as input to shuffle 10
[2025-06-05T19:09:20.990+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Got map stage job 22 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) with 1 output partitions
[2025-06-05T19:09:20.990+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Final stage: ShuffleMapStage 36 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93)
[2025-06-05T19:09:20.991+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 35)
[2025-06-05T19:09:20.991+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:20.991+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[79] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93), which has no missing parents
[2025-06-05T19:09:20.998+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 35.4 KiB, free 432.9 MiB)
[2025-06-05T19:09:20.999+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 17.1 KiB, free 432.9 MiB)
[2025-06-05T19:09:20.999+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:20 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.255.255.254:43723 (size: 17.1 KiB, free: 434.2 MiB)
[2025-06-05T19:09:21.000+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:21.001+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[79] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:21.001+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks resource profile 0
[2025-06-05T19:09:21.002+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 22) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8988 bytes)
[2025-06-05T19:09:21.003+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO Executor: Running task 0.0 in stage 36.0 (TID 22)
[2025-06-05T19:09:21.011+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO ShuffleBlockFetcherIterator: Getting 1 (13.5 KiB) non-empty blocks including 1 (13.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T19:09:21.011+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T19:09:21.025+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO Executor: Finished task 0.0 in stage 36.0 (TID 22). 5170 bytes result sent to driver
[2025-06-05T19:09:21.027+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 22) in 25 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:21.027+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool
[2025-06-05T19:09:21.028+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: ShuffleMapStage 36 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) finished in 0.034 s
[2025-06-05T19:09:21.028+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T19:09:21.028+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: running: Set()
[2025-06-05T19:09:21.028+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: waiting: Set()
[2025-06-05T19:09:21.028+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: failed: Set()
[2025-06-05T19:09:21.043+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93
[2025-06-05T19:09:21.044+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Got job 23 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) with 1 output partitions
[2025-06-05T19:09:21.045+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Final stage: ResultStage 39 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93)
[2025-06-05T19:09:21.045+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 38)
[2025-06-05T19:09:21.045+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:21.046+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[82] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93), which has no missing parents
[2025-06-05T19:09:21.049+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 35.0 KiB, free 432.9 MiB)
[2025-06-05T19:09:21.050+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 16.6 KiB, free 432.9 MiB)
[2025-06-05T19:09:21.051+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.255.255.254:43723 (size: 16.6 KiB, free: 434.2 MiB)
[2025-06-05T19:09:21.052+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:21.052+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[82] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:21.052+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks resource profile 0
[2025-06-05T19:09:21.053+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 23) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8999 bytes)
[2025-06-05T19:09:21.054+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO Executor: Running task 0.0 in stage 39.0 (TID 23)
[2025-06-05T19:09:21.059+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO ShuffleBlockFetcherIterator: Getting 1 (1399.0 B) non-empty blocks including 1 (1399.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T19:09:21.060+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T19:09:21.066+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO Executor: Finished task 0.0 in stage 39.0 (TID 23). 6437 bytes result sent to driver
[2025-06-05T19:09:21.067+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 23) in 13 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:21.067+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool
[2025-06-05T19:09:21.067+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: ResultStage 39 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) finished in 0.021 s
[2025-06-05T19:09:21.068+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T19:09:21.068+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished
[2025-06-05T19:09:21.068+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Job 23 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93, took 0.024724 s
[2025-06-05T19:09:21.148+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO FileSourceStrategy: Pushed Filters:
[2025-06-05T19:09:21.148+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO FileSourceStrategy: Post-Scan Filters:
[2025-06-05T19:09:21.166+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 201.4 KiB, free 432.7 MiB)
[2025-06-05T19:09:21.177+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 10.255.255.254:43723 in memory (size: 10.7 KiB, free: 434.2 MiB)
[2025-06-05T19:09:21.183+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 432.7 MiB)
[2025-06-05T19:09:21.183+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 10.255.255.254:43723 in memory (size: 16.6 KiB, free: 434.2 MiB)
[2025-06-05T19:09:21.184+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.255.255.254:43723 (size: 35.0 KiB, free: 434.1 MiB)
[2025-06-05T19:09:21.185+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO SparkContext: Created broadcast 30 from first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94
[2025-06-05T19:09:21.186+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-06-05T19:09:21.190+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 10.255.255.254:43723 in memory (size: 17.1 KiB, free: 434.2 MiB)
[2025-06-05T19:09:21.203+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94
[2025-06-05T19:09:21.204+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Got job 24 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) with 1 output partitions
[2025-06-05T19:09:21.204+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Final stage: ResultStage 40 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94)
[2025-06-05T19:09:21.204+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T19:09:21.204+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:21.205+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[88] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94), which has no missing parents
[2025-06-05T19:09:21.207+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 19.3 KiB, free 432.8 MiB)
[2025-06-05T19:09:21.208+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 432.8 MiB)
[2025-06-05T19:09:21.209+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.255.255.254:43723 (size: 8.7 KiB, free: 434.2 MiB)
[2025-06-05T19:09:21.209+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:21.219+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[88] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:21.219+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks resource profile 0
[2025-06-05T19:09:21.221+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 24) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9612 bytes)
[2025-06-05T19:09:21.221+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO Executor: Running task 0.0 in stage 40.0 (TID 24)
[2025-06-05T19:09:21.228+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T19:09:21.239+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO Executor: Finished task 0.0 in stage 40.0 (TID 24). 10024 bytes result sent to driver
[2025-06-05T19:09:21.240+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 24) in 19 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:21.240+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool
[2025-06-05T19:09:21.240+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: ResultStage 40 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) finished in 0.035 s
[2025-06-05T19:09:21.240+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T19:09:21.241+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 40: Stage finished
[2025-06-05T19:09:21.241+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Job 24 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94, took 0.038026 s
[2025-06-05T19:09:21.243+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Registering RDD 89 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) as input to shuffle 11
[2025-06-05T19:09:21.243+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Got map stage job 25 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) with 1 output partitions
[2025-06-05T19:09:21.243+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Final stage: ShuffleMapStage 41 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94)
[2025-06-05T19:09:21.243+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T19:09:21.243+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:21.245+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Submitting ShuffleMapStage 41 (MapPartitionsRDD[89] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94), which has no missing parents
[2025-06-05T19:09:21.248+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 27.6 KiB, free 432.7 MiB)
[2025-06-05T19:09:21.249+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 432.7 MiB)
[2025-06-05T19:09:21.249+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.255.255.254:43723 (size: 10.7 KiB, free: 434.1 MiB)
[2025-06-05T19:09:21.250+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:21.250+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 41 (MapPartitionsRDD[89] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:21.250+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Adding task set 41.0 with 1 tasks resource profile 0
[2025-06-05T19:09:21.252+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 25) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9601 bytes)
[2025-06-05T19:09:21.253+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO Executor: Running task 0.0 in stage 41.0 (TID 25)
[2025-06-05T19:09:21.259+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T19:09:21.290+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO Executor: Finished task 0.0 in stage 41.0 (TID 25). 2199 bytes result sent to driver
[2025-06-05T19:09:21.291+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 25) in 40 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:21.292+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool
[2025-06-05T19:09:21.292+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: ShuffleMapStage 41 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) finished in 0.046 s
[2025-06-05T19:09:21.293+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T19:09:21.293+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: running: Set()
[2025-06-05T19:09:21.293+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: waiting: Set()
[2025-06-05T19:09:21.293+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: failed: Set()
[2025-06-05T19:09:21.298+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO ShufflePartitionsUtil: For shuffle(11), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-06-05T19:09:21.320+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Registering RDD 92 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) as input to shuffle 12
[2025-06-05T19:09:21.320+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Got map stage job 26 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) with 1 output partitions
[2025-06-05T19:09:21.320+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Final stage: ShuffleMapStage 43 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94)
[2025-06-05T19:09:21.321+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 42)
[2025-06-05T19:09:21.321+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:21.321+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Submitting ShuffleMapStage 43 (MapPartitionsRDD[92] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94), which has no missing parents
[2025-06-05T19:09:21.324+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 45.1 KiB, free 432.7 MiB)
[2025-06-05T19:09:21.325+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 20.5 KiB, free 432.7 MiB)
[2025-06-05T19:09:21.325+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.255.255.254:43723 (size: 20.5 KiB, free: 434.1 MiB)
[2025-06-05T19:09:21.326+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:21.326+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 43 (MapPartitionsRDD[92] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:21.326+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Adding task set 43.0 with 1 tasks resource profile 0
[2025-06-05T19:09:21.328+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 26) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8988 bytes)
[2025-06-05T19:09:21.328+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO Executor: Running task 0.0 in stage 43.0 (TID 26)
[2025-06-05T19:09:21.334+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO ShuffleBlockFetcherIterator: Getting 1 (13.5 KiB) non-empty blocks including 1 (13.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T19:09:21.335+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T19:09:21.351+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO Executor: Finished task 0.0 in stage 43.0 (TID 26). 5400 bytes result sent to driver
[2025-06-05T19:09:21.354+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 10.255.255.254:43723 in memory (size: 8.7 KiB, free: 434.1 MiB)
[2025-06-05T19:09:21.354+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 26) in 27 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:21.355+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool
[2025-06-05T19:09:21.355+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: ShuffleMapStage 43 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) finished in 0.033 s
[2025-06-05T19:09:21.356+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T19:09:21.356+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: running: Set()
[2025-06-05T19:09:21.356+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: waiting: Set()
[2025-06-05T19:09:21.356+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: failed: Set()
[2025-06-05T19:09:21.360+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 10.255.255.254:43723 in memory (size: 10.7 KiB, free: 434.1 MiB)
[2025-06-05T19:09:21.379+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94
[2025-06-05T19:09:21.381+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Got job 27 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) with 1 output partitions
[2025-06-05T19:09:21.381+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Final stage: ResultStage 46 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94)
[2025-06-05T19:09:21.381+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 45)
[2025-06-05T19:09:21.381+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:21.381+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Submitting ResultStage 46 (MapPartitionsRDD[95] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94), which has no missing parents
[2025-06-05T19:09:21.393+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 16.9 KiB, free 432.7 MiB)
[2025-06-05T19:09:21.394+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 432.7 MiB)
[2025-06-05T19:09:21.394+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.255.255.254:43723 (size: 7.2 KiB, free: 434.1 MiB)
[2025-06-05T19:09:21.395+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:21.396+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (MapPartitionsRDD[95] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:21.396+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Adding task set 46.0 with 1 tasks resource profile 0
[2025-06-05T19:09:21.397+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSetManager: Starting task 0.0 in stage 46.0 (TID 27) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8999 bytes)
[2025-06-05T19:09:21.398+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO Executor: Running task 0.0 in stage 46.0 (TID 27)
[2025-06-05T19:09:21.401+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T19:09:21.401+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T19:09:21.405+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO Executor: Finished task 0.0 in stage 46.0 (TID 27). 3995 bytes result sent to driver
[2025-06-05T19:09:21.406+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSetManager: Finished task 0.0 in stage 46.0 (TID 27) in 9 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:21.406+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Removed TaskSet 46.0, whose tasks have all completed, from pool
[2025-06-05T19:09:21.406+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: ResultStage 46 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) finished in 0.015 s
[2025-06-05T19:09:21.407+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T19:09:21.407+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 46: Stage finished
[2025-06-05T19:09:21.407+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Job 27 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94, took 0.027462 s
[2025-06-05T19:09:21.469+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO FileSourceStrategy: Pushed Filters:
[2025-06-05T19:09:21.469+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO FileSourceStrategy: Post-Scan Filters:
[2025-06-05T19:09:21.491+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 201.4 KiB, free 432.5 MiB)
[2025-06-05T19:09:21.497+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 432.5 MiB)
[2025-06-05T19:09:21.498+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.255.255.254:43723 (size: 35.1 KiB, free: 434.1 MiB)
[2025-06-05T19:09:21.499+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO SparkContext: Created broadcast 35 from first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93
[2025-06-05T19:09:21.500+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-06-05T19:09:21.511+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93
[2025-06-05T19:09:21.512+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Got job 28 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) with 1 output partitions
[2025-06-05T19:09:21.512+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Final stage: ResultStage 47 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93)
[2025-06-05T19:09:21.512+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T19:09:21.513+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:21.513+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Submitting ResultStage 47 (MapPartitionsRDD[101] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93), which has no missing parents
[2025-06-05T19:09:21.514+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 19.4 KiB, free 432.4 MiB)
[2025-06-05T19:09:21.515+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 432.4 MiB)
[2025-06-05T19:09:21.515+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.255.255.254:43723 (size: 8.7 KiB, free: 434.1 MiB)
[2025-06-05T19:09:21.516+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:21.516+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 47 (MapPartitionsRDD[101] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:21.516+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Adding task set 47.0 with 1 tasks resource profile 0
[2025-06-05T19:09:21.517+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSetManager: Starting task 0.0 in stage 47.0 (TID 28) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9612 bytes)
[2025-06-05T19:09:21.518+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO Executor: Running task 0.0 in stage 47.0 (TID 28)
[2025-06-05T19:09:21.524+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T19:09:21.533+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO Executor: Finished task 0.0 in stage 47.0 (TID 28). 10024 bytes result sent to driver
[2025-06-05T19:09:21.534+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSetManager: Finished task 0.0 in stage 47.0 (TID 28) in 17 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:21.534+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool
[2025-06-05T19:09:21.535+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: ResultStage 47 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) finished in 0.022 s
[2025-06-05T19:09:21.535+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T19:09:21.535+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 47: Stage finished
[2025-06-05T19:09:21.535+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Job 28 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93, took 0.024262 s
[2025-06-05T19:09:21.537+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Registering RDD 102 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) as input to shuffle 13
[2025-06-05T19:09:21.538+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Got map stage job 29 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) with 1 output partitions
[2025-06-05T19:09:21.538+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Final stage: ShuffleMapStage 48 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93)
[2025-06-05T19:09:21.538+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T19:09:21.538+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:21.539+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Submitting ShuffleMapStage 48 (MapPartitionsRDD[102] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93), which has no missing parents
[2025-06-05T19:09:21.542+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 27.6 KiB, free 432.4 MiB)
[2025-06-05T19:09:21.553+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 432.4 MiB)
[2025-06-05T19:09:21.553+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.255.255.254:43723 (size: 10.7 KiB, free: 434.1 MiB)
[2025-06-05T19:09:21.554+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:21.554+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 10.255.255.254:43723 in memory (size: 7.2 KiB, free: 434.1 MiB)
[2025-06-05T19:09:21.555+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 48 (MapPartitionsRDD[102] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:21.555+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Adding task set 48.0 with 1 tasks resource profile 0
[2025-06-05T19:09:21.557+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSetManager: Starting task 0.0 in stage 48.0 (TID 29) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9601 bytes)
[2025-06-05T19:09:21.557+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO Executor: Running task 0.0 in stage 48.0 (TID 29)
[2025-06-05T19:09:21.560+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 10.255.255.254:43723 in memory (size: 20.5 KiB, free: 434.1 MiB)
[2025-06-05T19:09:21.566+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 10.255.255.254:43723 in memory (size: 8.7 KiB, free: 434.1 MiB)
[2025-06-05T19:09:21.568+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T19:09:21.649+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO Executor: Finished task 0.0 in stage 48.0 (TID 29). 2199 bytes result sent to driver
[2025-06-05T19:09:21.650+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSetManager: Finished task 0.0 in stage 48.0 (TID 29) in 94 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:21.651+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Removed TaskSet 48.0, whose tasks have all completed, from pool
[2025-06-05T19:09:21.651+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: ShuffleMapStage 48 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) finished in 0.112 s
[2025-06-05T19:09:21.651+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T19:09:21.652+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: running: Set()
[2025-06-05T19:09:21.652+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: waiting: Set()
[2025-06-05T19:09:21.652+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: failed: Set()
[2025-06-05T19:09:21.657+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO ShufflePartitionsUtil: For shuffle(13), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-06-05T19:09:21.682+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Registering RDD 106 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) as input to shuffle 14
[2025-06-05T19:09:21.682+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Got map stage job 30 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) with 1 output partitions
[2025-06-05T19:09:21.682+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Final stage: ShuffleMapStage 50 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93)
[2025-06-05T19:09:21.683+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 49)
[2025-06-05T19:09:21.683+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:21.683+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Submitting ShuffleMapStage 50 (MapPartitionsRDD[106] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93), which has no missing parents
[2025-06-05T19:09:21.693+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 35.5 KiB, free 432.5 MiB)
[2025-06-05T19:09:21.694+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 17.1 KiB, free 432.5 MiB)
[2025-06-05T19:09:21.695+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 10.255.255.254:43723 (size: 17.1 KiB, free: 434.1 MiB)
[2025-06-05T19:09:21.695+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:21.696+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 50 (MapPartitionsRDD[106] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:21.696+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Adding task set 50.0 with 1 tasks resource profile 0
[2025-06-05T19:09:21.698+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSetManager: Starting task 0.0 in stage 50.0 (TID 30) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8988 bytes)
[2025-06-05T19:09:21.699+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO Executor: Running task 0.0 in stage 50.0 (TID 30)
[2025-06-05T19:09:21.721+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO ShuffleBlockFetcherIterator: Getting 1 (13.5 KiB) non-empty blocks including 1 (13.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T19:09:21.721+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T19:09:21.738+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO Executor: Finished task 0.0 in stage 50.0 (TID 30). 5170 bytes result sent to driver
[2025-06-05T19:09:21.739+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSetManager: Finished task 0.0 in stage 50.0 (TID 30) in 42 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:21.739+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Removed TaskSet 50.0, whose tasks have all completed, from pool
[2025-06-05T19:09:21.740+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: ShuffleMapStage 50 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) finished in 0.052 s
[2025-06-05T19:09:21.740+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T19:09:21.740+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: running: Set()
[2025-06-05T19:09:21.740+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: waiting: Set()
[2025-06-05T19:09:21.741+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: failed: Set()
[2025-06-05T19:09:21.784+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93
[2025-06-05T19:09:21.785+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Got job 31 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) with 1 output partitions
[2025-06-05T19:09:21.785+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Final stage: ResultStage 53 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93)
[2025-06-05T19:09:21.785+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 52)
[2025-06-05T19:09:21.786+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:21.786+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Submitting ResultStage 53 (MapPartitionsRDD[109] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93), which has no missing parents
[2025-06-05T19:09:21.788+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 35.0 KiB, free 432.4 MiB)
[2025-06-05T19:09:21.789+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 16.6 KiB, free 432.4 MiB)
[2025-06-05T19:09:21.790+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 10.255.255.254:43723 (size: 16.6 KiB, free: 434.1 MiB)
[2025-06-05T19:09:21.790+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:21.791+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 53 (MapPartitionsRDD[109] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:21.791+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Adding task set 53.0 with 1 tasks resource profile 0
[2025-06-05T19:09:21.793+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSetManager: Starting task 0.0 in stage 53.0 (TID 31) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8999 bytes)
[2025-06-05T19:09:21.795+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO Executor: Running task 0.0 in stage 53.0 (TID 31)
[2025-06-05T19:09:21.802+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO ShuffleBlockFetcherIterator: Getting 1 (1538.0 B) non-empty blocks including 1 (1538.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T19:09:21.802+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T19:09:21.807+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO Executor: Finished task 0.0 in stage 53.0 (TID 31). 6437 bytes result sent to driver
[2025-06-05T19:09:21.808+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSetManager: Finished task 0.0 in stage 53.0 (TID 31) in 16 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:21.809+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Removed TaskSet 53.0, whose tasks have all completed, from pool
[2025-06-05T19:09:21.809+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: ResultStage 53 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) finished in 0.023 s
[2025-06-05T19:09:21.810+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T19:09:21.810+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 53: Stage finished
[2025-06-05T19:09:21.810+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Job 31 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93, took 0.026330 s
[2025-06-05T19:09:21.884+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO FileSourceStrategy: Pushed Filters:
[2025-06-05T19:09:21.885+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO FileSourceStrategy: Post-Scan Filters:
[2025-06-05T19:09:21.901+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 201.4 KiB, free 432.2 MiB)
[2025-06-05T19:09:21.908+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 432.2 MiB)
[2025-06-05T19:09:21.908+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 10.255.255.254:43723 (size: 35.1 KiB, free: 434.0 MiB)
[2025-06-05T19:09:21.909+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO SparkContext: Created broadcast 40 from first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94
[2025-06-05T19:09:21.910+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-06-05T19:09:21.922+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94
[2025-06-05T19:09:21.923+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Got job 32 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) with 1 output partitions
[2025-06-05T19:09:21.923+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Final stage: ResultStage 54 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94)
[2025-06-05T19:09:21.923+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T19:09:21.923+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:21.923+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Submitting ResultStage 54 (MapPartitionsRDD[115] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94), which has no missing parents
[2025-06-05T19:09:21.926+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 19.4 KiB, free 432.2 MiB)
[2025-06-05T19:09:21.927+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 432.2 MiB)
[2025-06-05T19:09:21.927+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 10.255.255.254:43723 (size: 8.7 KiB, free: 434.0 MiB)
[2025-06-05T19:09:21.928+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:21.928+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 54 (MapPartitionsRDD[115] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:21.929+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Adding task set 54.0 with 1 tasks resource profile 0
[2025-06-05T19:09:21.929+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSetManager: Starting task 0.0 in stage 54.0 (TID 32) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9612 bytes)
[2025-06-05T19:09:21.930+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO Executor: Running task 0.0 in stage 54.0 (TID 32)
[2025-06-05T19:09:21.936+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T19:09:21.944+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO Executor: Finished task 0.0 in stage 54.0 (TID 32). 10024 bytes result sent to driver
[2025-06-05T19:09:21.945+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSetManager: Finished task 0.0 in stage 54.0 (TID 32) in 16 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:21.945+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Removed TaskSet 54.0, whose tasks have all completed, from pool
[2025-06-05T19:09:21.946+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: ResultStage 54 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) finished in 0.021 s
[2025-06-05T19:09:21.946+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T19:09:21.946+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 54: Stage finished
[2025-06-05T19:09:21.946+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Job 32 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94, took 0.024278 s
[2025-06-05T19:09:21.949+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Registering RDD 116 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) as input to shuffle 15
[2025-06-05T19:09:21.950+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Got map stage job 33 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) with 1 output partitions
[2025-06-05T19:09:21.950+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Final stage: ShuffleMapStage 55 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94)
[2025-06-05T19:09:21.950+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T19:09:21.950+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:21.950+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Submitting ShuffleMapStage 55 (MapPartitionsRDD[116] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94), which has no missing parents
[2025-06-05T19:09:21.953+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 27.6 KiB, free 432.1 MiB)
[2025-06-05T19:09:21.963+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 432.1 MiB)
[2025-06-05T19:09:21.964+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 10.255.255.254:43723 (size: 10.7 KiB, free: 434.0 MiB)
[2025-06-05T19:09:21.965+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:21.966+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 10.255.255.254:43723 in memory (size: 17.1 KiB, free: 434.0 MiB)
[2025-06-05T19:09:21.966+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 55 (MapPartitionsRDD[116] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:21.966+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSchedulerImpl: Adding task set 55.0 with 1 tasks resource profile 0
[2025-06-05T19:09:21.968+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO TaskSetManager: Starting task 0.0 in stage 55.0 (TID 33) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9601 bytes)
[2025-06-05T19:09:21.969+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO Executor: Running task 0.0 in stage 55.0 (TID 33)
[2025-06-05T19:09:21.977+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 10.255.255.254:43723 in memory (size: 8.7 KiB, free: 434.1 MiB)
[2025-06-05T19:09:21.979+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T19:09:21.988+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:21 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 10.255.255.254:43723 in memory (size: 16.6 KiB, free: 434.1 MiB)
[2025-06-05T19:09:22.009+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 10.255.255.254:43723 in memory (size: 10.7 KiB, free: 434.1 MiB)
[2025-06-05T19:09:22.040+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO Executor: Finished task 0.0 in stage 55.0 (TID 33). 2199 bytes result sent to driver
[2025-06-05T19:09:22.042+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSetManager: Finished task 0.0 in stage 55.0 (TID 33) in 75 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:22.042+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSchedulerImpl: Removed TaskSet 55.0, whose tasks have all completed, from pool
[2025-06-05T19:09:22.043+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: ShuffleMapStage 55 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) finished in 0.092 s
[2025-06-05T19:09:22.043+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T19:09:22.043+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: running: Set()
[2025-06-05T19:09:22.043+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: waiting: Set()
[2025-06-05T19:09:22.044+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: failed: Set()
[2025-06-05T19:09:22.052+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO ShufflePartitionsUtil: For shuffle(15), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-06-05T19:09:22.068+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Registering RDD 119 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) as input to shuffle 16
[2025-06-05T19:09:22.069+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Got map stage job 34 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) with 1 output partitions
[2025-06-05T19:09:22.069+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Final stage: ShuffleMapStage 57 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94)
[2025-06-05T19:09:22.069+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 56)
[2025-06-05T19:09:22.069+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:22.070+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Submitting ShuffleMapStage 57 (MapPartitionsRDD[119] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94), which has no missing parents
[2025-06-05T19:09:22.073+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 45.1 KiB, free 432.2 MiB)
[2025-06-05T19:09:22.073+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 20.5 KiB, free 432.2 MiB)
[2025-06-05T19:09:22.074+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 10.255.255.254:43723 (size: 20.5 KiB, free: 434.1 MiB)
[2025-06-05T19:09:22.074+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:22.075+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 57 (MapPartitionsRDD[119] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:22.075+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSchedulerImpl: Adding task set 57.0 with 1 tasks resource profile 0
[2025-06-05T19:09:22.076+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSetManager: Starting task 0.0 in stage 57.0 (TID 34) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8988 bytes)
[2025-06-05T19:09:22.076+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO Executor: Running task 0.0 in stage 57.0 (TID 34)
[2025-06-05T19:09:22.084+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO ShuffleBlockFetcherIterator: Getting 1 (13.5 KiB) non-empty blocks including 1 (13.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T19:09:22.084+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T19:09:22.090+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO Executor: Finished task 0.0 in stage 57.0 (TID 34). 5314 bytes result sent to driver
[2025-06-05T19:09:22.091+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSetManager: Finished task 0.0 in stage 57.0 (TID 34) in 15 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:22.092+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSchedulerImpl: Removed TaskSet 57.0, whose tasks have all completed, from pool
[2025-06-05T19:09:22.092+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: ShuffleMapStage 57 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) finished in 0.021 s
[2025-06-05T19:09:22.092+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T19:09:22.092+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: running: Set()
[2025-06-05T19:09:22.092+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: waiting: Set()
[2025-06-05T19:09:22.093+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: failed: Set()
[2025-06-05T19:09:22.112+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94
[2025-06-05T19:09:22.113+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Got job 35 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) with 1 output partitions
[2025-06-05T19:09:22.113+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Final stage: ResultStage 60 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94)
[2025-06-05T19:09:22.113+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 59)
[2025-06-05T19:09:22.114+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:22.114+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Submitting ResultStage 60 (MapPartitionsRDD[122] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94), which has no missing parents
[2025-06-05T19:09:22.115+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 16.9 KiB, free 432.2 MiB)
[2025-06-05T19:09:22.116+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 432.2 MiB)
[2025-06-05T19:09:22.116+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 10.255.255.254:43723 (size: 7.2 KiB, free: 434.1 MiB)
[2025-06-05T19:09:22.117+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:22.117+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 60 (MapPartitionsRDD[122] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:22.117+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSchedulerImpl: Adding task set 60.0 with 1 tasks resource profile 0
[2025-06-05T19:09:22.118+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSetManager: Starting task 0.0 in stage 60.0 (TID 35) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8999 bytes)
[2025-06-05T19:09:22.119+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO Executor: Running task 0.0 in stage 60.0 (TID 35)
[2025-06-05T19:09:22.122+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T19:09:22.122+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T19:09:22.124+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO Executor: Finished task 0.0 in stage 60.0 (TID 35). 3995 bytes result sent to driver
[2025-06-05T19:09:22.125+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSetManager: Finished task 0.0 in stage 60.0 (TID 35) in 7 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:22.125+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSchedulerImpl: Removed TaskSet 60.0, whose tasks have all completed, from pool
[2025-06-05T19:09:22.126+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: ResultStage 60 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) finished in 0.011 s
[2025-06-05T19:09:22.126+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Job 35 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T19:09:22.126+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 60: Stage finished
[2025-06-05T19:09:22.126+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Job 35 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94, took 0.014136 s
[2025-06-05T19:09:22.200+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO FileSourceStrategy: Pushed Filters:
[2025-06-05T19:09:22.200+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO FileSourceStrategy: Post-Scan Filters:
[2025-06-05T19:09:22.213+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 201.4 KiB, free 432.0 MiB)
[2025-06-05T19:09:22.221+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 432.0 MiB)
[2025-06-05T19:09:22.221+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 10.255.255.254:43723 (size: 35.1 KiB, free: 434.0 MiB)
[2025-06-05T19:09:22.222+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO SparkContext: Created broadcast 45 from first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93
[2025-06-05T19:09:22.223+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-06-05T19:09:22.236+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93
[2025-06-05T19:09:22.237+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Got job 36 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) with 1 output partitions
[2025-06-05T19:09:22.237+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Final stage: ResultStage 61 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93)
[2025-06-05T19:09:22.237+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T19:09:22.237+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:22.237+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Submitting ResultStage 61 (MapPartitionsRDD[128] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93), which has no missing parents
[2025-06-05T19:09:22.239+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 19.4 KiB, free 431.9 MiB)
[2025-06-05T19:09:22.240+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 431.9 MiB)
[2025-06-05T19:09:22.240+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 10.255.255.254:43723 (size: 8.7 KiB, free: 434.0 MiB)
[2025-06-05T19:09:22.241+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:22.241+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 61 (MapPartitionsRDD[128] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:22.241+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSchedulerImpl: Adding task set 61.0 with 1 tasks resource profile 0
[2025-06-05T19:09:22.242+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSetManager: Starting task 0.0 in stage 61.0 (TID 36) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9612 bytes)
[2025-06-05T19:09:22.243+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO Executor: Running task 0.0 in stage 61.0 (TID 36)
[2025-06-05T19:09:22.247+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T19:09:22.254+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO Executor: Finished task 0.0 in stage 61.0 (TID 36). 10024 bytes result sent to driver
[2025-06-05T19:09:22.256+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSetManager: Finished task 0.0 in stage 61.0 (TID 36) in 13 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:22.256+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSchedulerImpl: Removed TaskSet 61.0, whose tasks have all completed, from pool
[2025-06-05T19:09:22.256+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: ResultStage 61 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) finished in 0.018 s
[2025-06-05T19:09:22.256+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T19:09:22.257+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 61: Stage finished
[2025-06-05T19:09:22.257+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Job 36 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93, took 0.020801 s
[2025-06-05T19:09:22.259+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Registering RDD 129 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) as input to shuffle 17
[2025-06-05T19:09:22.259+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Got map stage job 37 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) with 1 output partitions
[2025-06-05T19:09:22.259+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Final stage: ShuffleMapStage 62 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93)
[2025-06-05T19:09:22.259+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T19:09:22.259+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:22.259+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Submitting ShuffleMapStage 62 (MapPartitionsRDD[129] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93), which has no missing parents
[2025-06-05T19:09:22.262+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 27.6 KiB, free 431.9 MiB)
[2025-06-05T19:09:22.272+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 431.9 MiB)
[2025-06-05T19:09:22.273+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 10.255.255.254:43723 (size: 10.7 KiB, free: 434.0 MiB)
[2025-06-05T19:09:22.273+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:22.274+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 62 (MapPartitionsRDD[129] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:22.274+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSchedulerImpl: Adding task set 62.0 with 1 tasks resource profile 0
[2025-06-05T19:09:22.274+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO BlockManagerInfo: Removed broadcast_44_piece0 on 10.255.255.254:43723 in memory (size: 7.2 KiB, free: 434.0 MiB)
[2025-06-05T19:09:22.275+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSetManager: Starting task 0.0 in stage 62.0 (TID 37) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9601 bytes)
[2025-06-05T19:09:22.276+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO Executor: Running task 0.0 in stage 62.0 (TID 37)
[2025-06-05T19:09:22.278+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO BlockManagerInfo: Removed broadcast_43_piece0 on 10.255.255.254:43723 in memory (size: 20.5 KiB, free: 434.0 MiB)
[2025-06-05T19:09:22.284+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO BlockManagerInfo: Removed broadcast_46_piece0 on 10.255.255.254:43723 in memory (size: 8.7 KiB, free: 434.0 MiB)
[2025-06-05T19:09:22.285+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T19:09:22.289+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO BlockManagerInfo: Removed broadcast_42_piece0 on 10.255.255.254:43723 in memory (size: 10.7 KiB, free: 434.0 MiB)
[2025-06-05T19:09:22.339+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO Executor: Finished task 0.0 in stage 62.0 (TID 37). 2199 bytes result sent to driver
[2025-06-05T19:09:22.340+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSetManager: Finished task 0.0 in stage 62.0 (TID 37) in 65 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:22.340+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSchedulerImpl: Removed TaskSet 62.0, whose tasks have all completed, from pool
[2025-06-05T19:09:22.341+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: ShuffleMapStage 62 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) finished in 0.081 s
[2025-06-05T19:09:22.341+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T19:09:22.341+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: running: Set()
[2025-06-05T19:09:22.341+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: waiting: Set()
[2025-06-05T19:09:22.341+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: failed: Set()
[2025-06-05T19:09:22.346+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO ShufflePartitionsUtil: For shuffle(17), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-06-05T19:09:22.367+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Registering RDD 133 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) as input to shuffle 18
[2025-06-05T19:09:22.367+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Got map stage job 38 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) with 1 output partitions
[2025-06-05T19:09:22.367+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Final stage: ShuffleMapStage 64 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93)
[2025-06-05T19:09:22.367+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 63)
[2025-06-05T19:09:22.368+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:22.368+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Submitting ShuffleMapStage 64 (MapPartitionsRDD[133] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93), which has no missing parents
[2025-06-05T19:09:22.374+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 35.5 KiB, free 432.0 MiB)
[2025-06-05T19:09:22.375+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 17.1 KiB, free 432.0 MiB)
[2025-06-05T19:09:22.375+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 10.255.255.254:43723 (size: 17.1 KiB, free: 434.0 MiB)
[2025-06-05T19:09:22.375+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:22.378+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 64 (MapPartitionsRDD[133] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:22.378+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSchedulerImpl: Adding task set 64.0 with 1 tasks resource profile 0
[2025-06-05T19:09:22.379+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSetManager: Starting task 0.0 in stage 64.0 (TID 38) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8988 bytes)
[2025-06-05T19:09:22.380+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO Executor: Running task 0.0 in stage 64.0 (TID 38)
[2025-06-05T19:09:22.386+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO ShuffleBlockFetcherIterator: Getting 1 (13.5 KiB) non-empty blocks including 1 (13.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T19:09:22.387+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T19:09:22.395+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO Executor: Finished task 0.0 in stage 64.0 (TID 38). 5170 bytes result sent to driver
[2025-06-05T19:09:22.396+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSetManager: Finished task 0.0 in stage 64.0 (TID 38) in 17 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:22.396+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSchedulerImpl: Removed TaskSet 64.0, whose tasks have all completed, from pool
[2025-06-05T19:09:22.397+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: ShuffleMapStage 64 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) finished in 0.027 s
[2025-06-05T19:09:22.397+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T19:09:22.397+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: running: Set()
[2025-06-05T19:09:22.397+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: waiting: Set()
[2025-06-05T19:09:22.397+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: failed: Set()
[2025-06-05T19:09:22.411+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93
[2025-06-05T19:09:22.412+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Got job 39 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) with 1 output partitions
[2025-06-05T19:09:22.412+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Final stage: ResultStage 67 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93)
[2025-06-05T19:09:22.413+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 66)
[2025-06-05T19:09:22.413+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:22.413+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Submitting ResultStage 67 (MapPartitionsRDD[136] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93), which has no missing parents
[2025-06-05T19:09:22.415+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 35.1 KiB, free 432.0 MiB)
[2025-06-05T19:09:22.416+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 16.7 KiB, free 432.0 MiB)
[2025-06-05T19:09:22.416+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on 10.255.255.254:43723 (size: 16.7 KiB, free: 434.0 MiB)
[2025-06-05T19:09:22.417+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:22.420+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 67 (MapPartitionsRDD[136] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:22.420+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSchedulerImpl: Adding task set 67.0 with 1 tasks resource profile 0
[2025-06-05T19:09:22.421+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSetManager: Starting task 0.0 in stage 67.0 (TID 39) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8999 bytes)
[2025-06-05T19:09:22.422+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO Executor: Running task 0.0 in stage 67.0 (TID 39)
[2025-06-05T19:09:22.428+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO ShuffleBlockFetcherIterator: Getting 1 (1538.0 B) non-empty blocks including 1 (1538.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T19:09:22.429+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T19:09:22.435+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO Executor: Finished task 0.0 in stage 67.0 (TID 39). 6437 bytes result sent to driver
[2025-06-05T19:09:22.439+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSetManager: Finished task 0.0 in stage 67.0 (TID 39) in 17 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:22.439+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSchedulerImpl: Removed TaskSet 67.0, whose tasks have all completed, from pool
[2025-06-05T19:09:22.440+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: ResultStage 67 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) finished in 0.026 s
[2025-06-05T19:09:22.440+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Job 39 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T19:09:22.440+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 67: Stage finished
[2025-06-05T19:09:22.440+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Job 39 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93, took 0.029037 s
[2025-06-05T19:09:22.536+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO FileSourceStrategy: Pushed Filters:
[2025-06-05T19:09:22.537+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO FileSourceStrategy: Post-Scan Filters:
[2025-06-05T19:09:22.554+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 201.4 KiB, free 431.8 MiB)
[2025-06-05T19:09:22.561+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 431.7 MiB)
[2025-06-05T19:09:22.562+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on 10.255.255.254:43723 (size: 35.1 KiB, free: 434.0 MiB)
[2025-06-05T19:09:22.562+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO SparkContext: Created broadcast 50 from first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94
[2025-06-05T19:09:22.563+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-06-05T19:09:22.576+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94
[2025-06-05T19:09:22.577+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Got job 40 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) with 1 output partitions
[2025-06-05T19:09:22.577+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Final stage: ResultStage 68 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94)
[2025-06-05T19:09:22.577+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T19:09:22.577+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:22.577+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Submitting ResultStage 68 (MapPartitionsRDD[142] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94), which has no missing parents
[2025-06-05T19:09:22.579+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 19.4 KiB, free 431.7 MiB)
[2025-06-05T19:09:22.580+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 431.7 MiB)
[2025-06-05T19:09:22.580+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on 10.255.255.254:43723 (size: 8.7 KiB, free: 434.0 MiB)
[2025-06-05T19:09:22.581+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:22.581+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 68 (MapPartitionsRDD[142] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:22.581+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSchedulerImpl: Adding task set 68.0 with 1 tasks resource profile 0
[2025-06-05T19:09:22.582+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSetManager: Starting task 0.0 in stage 68.0 (TID 40) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9612 bytes)
[2025-06-05T19:09:22.583+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO Executor: Running task 0.0 in stage 68.0 (TID 40)
[2025-06-05T19:09:22.588+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T19:09:22.596+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO Executor: Finished task 0.0 in stage 68.0 (TID 40). 10024 bytes result sent to driver
[2025-06-05T19:09:22.597+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSetManager: Finished task 0.0 in stage 68.0 (TID 40) in 15 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:22.597+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSchedulerImpl: Removed TaskSet 68.0, whose tasks have all completed, from pool
[2025-06-05T19:09:22.598+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: ResultStage 68 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) finished in 0.020 s
[2025-06-05T19:09:22.598+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Job 40 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T19:09:22.598+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 68: Stage finished
[2025-06-05T19:09:22.598+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Job 40 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94, took 0.022602 s
[2025-06-05T19:09:22.601+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Registering RDD 143 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) as input to shuffle 19
[2025-06-05T19:09:22.601+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Got map stage job 41 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) with 1 output partitions
[2025-06-05T19:09:22.601+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Final stage: ShuffleMapStage 69 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94)
[2025-06-05T19:09:22.601+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T19:09:22.601+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:22.601+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Submitting ShuffleMapStage 69 (MapPartitionsRDD[143] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94), which has no missing parents
[2025-06-05T19:09:22.605+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 27.6 KiB, free 431.7 MiB)
[2025-06-05T19:09:22.616+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 431.7 MiB)
[2025-06-05T19:09:22.617+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on 10.255.255.254:43723 (size: 10.7 KiB, free: 434.0 MiB)
[2025-06-05T19:09:22.618+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO BlockManagerInfo: Removed broadcast_49_piece0 on 10.255.255.254:43723 in memory (size: 16.7 KiB, free: 434.0 MiB)
[2025-06-05T19:09:22.618+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:22.618+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 69 (MapPartitionsRDD[143] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:22.619+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSchedulerImpl: Adding task set 69.0 with 1 tasks resource profile 0
[2025-06-05T19:09:22.620+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSetManager: Starting task 0.0 in stage 69.0 (TID 41) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9601 bytes)
[2025-06-05T19:09:22.621+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO BlockManagerInfo: Removed broadcast_51_piece0 on 10.255.255.254:43723 in memory (size: 8.7 KiB, free: 434.0 MiB)
[2025-06-05T19:09:22.621+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO Executor: Running task 0.0 in stage 69.0 (TID 41)
[2025-06-05T19:09:22.628+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO BlockManagerInfo: Removed broadcast_48_piece0 on 10.255.255.254:43723 in memory (size: 17.1 KiB, free: 434.0 MiB)
[2025-06-05T19:09:22.631+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T19:09:22.632+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO BlockManagerInfo: Removed broadcast_47_piece0 on 10.255.255.254:43723 in memory (size: 10.7 KiB, free: 434.0 MiB)
[2025-06-05T19:09:22.701+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO Executor: Finished task 0.0 in stage 69.0 (TID 41). 2199 bytes result sent to driver
[2025-06-05T19:09:22.702+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSetManager: Finished task 0.0 in stage 69.0 (TID 41) in 82 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:22.702+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSchedulerImpl: Removed TaskSet 69.0, whose tasks have all completed, from pool
[2025-06-05T19:09:22.702+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: ShuffleMapStage 69 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) finished in 0.100 s
[2025-06-05T19:09:22.703+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T19:09:22.703+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: running: Set()
[2025-06-05T19:09:22.703+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: waiting: Set()
[2025-06-05T19:09:22.703+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: failed: Set()
[2025-06-05T19:09:22.708+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO ShufflePartitionsUtil: For shuffle(19), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-06-05T19:09:22.731+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Registering RDD 146 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) as input to shuffle 20
[2025-06-05T19:09:22.731+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Got map stage job 42 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) with 1 output partitions
[2025-06-05T19:09:22.731+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Final stage: ShuffleMapStage 71 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94)
[2025-06-05T19:09:22.731+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 70)
[2025-06-05T19:09:22.731+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:22.732+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Submitting ShuffleMapStage 71 (MapPartitionsRDD[146] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94), which has no missing parents
[2025-06-05T19:09:22.735+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 45.1 KiB, free 431.8 MiB)
[2025-06-05T19:09:22.735+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 20.5 KiB, free 431.8 MiB)
[2025-06-05T19:09:22.736+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on 10.255.255.254:43723 (size: 20.5 KiB, free: 434.0 MiB)
[2025-06-05T19:09:22.736+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:22.736+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 71 (MapPartitionsRDD[146] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:22.737+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSchedulerImpl: Adding task set 71.0 with 1 tasks resource profile 0
[2025-06-05T19:09:22.738+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSetManager: Starting task 0.0 in stage 71.0 (TID 42) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8988 bytes)
[2025-06-05T19:09:22.738+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO Executor: Running task 0.0 in stage 71.0 (TID 42)
[2025-06-05T19:09:22.747+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO ShuffleBlockFetcherIterator: Getting 1 (13.5 KiB) non-empty blocks including 1 (13.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T19:09:22.747+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T19:09:22.753+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO Executor: Finished task 0.0 in stage 71.0 (TID 42). 5314 bytes result sent to driver
[2025-06-05T19:09:22.754+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSetManager: Finished task 0.0 in stage 71.0 (TID 42) in 16 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:22.754+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSchedulerImpl: Removed TaskSet 71.0, whose tasks have all completed, from pool
[2025-06-05T19:09:22.754+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: ShuffleMapStage 71 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) finished in 0.021 s
[2025-06-05T19:09:22.755+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T19:09:22.755+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: running: Set()
[2025-06-05T19:09:22.755+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: waiting: Set()
[2025-06-05T19:09:22.755+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: failed: Set()
[2025-06-05T19:09:22.772+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94
[2025-06-05T19:09:22.773+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Got job 43 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) with 1 output partitions
[2025-06-05T19:09:22.773+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Final stage: ResultStage 74 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94)
[2025-06-05T19:09:22.773+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 73)
[2025-06-05T19:09:22.774+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:22.774+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Submitting ResultStage 74 (MapPartitionsRDD[149] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94), which has no missing parents
[2025-06-05T19:09:22.775+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 16.9 KiB, free 431.7 MiB)
[2025-06-05T19:09:22.776+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 431.7 MiB)
[2025-06-05T19:09:22.777+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on 10.255.255.254:43723 (size: 7.2 KiB, free: 434.0 MiB)
[2025-06-05T19:09:22.777+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:22.777+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 74 (MapPartitionsRDD[149] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:22.777+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSchedulerImpl: Adding task set 74.0 with 1 tasks resource profile 0
[2025-06-05T19:09:22.778+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSetManager: Starting task 0.0 in stage 74.0 (TID 43) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8999 bytes)
[2025-06-05T19:09:22.779+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO Executor: Running task 0.0 in stage 74.0 (TID 43)
[2025-06-05T19:09:22.782+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T19:09:22.782+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T19:09:22.783+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO Executor: Finished task 0.0 in stage 74.0 (TID 43). 3995 bytes result sent to driver
[2025-06-05T19:09:22.784+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSetManager: Finished task 0.0 in stage 74.0 (TID 43) in 6 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:22.784+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSchedulerImpl: Removed TaskSet 74.0, whose tasks have all completed, from pool
[2025-06-05T19:09:22.785+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: ResultStage 74 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) finished in 0.011 s
[2025-06-05T19:09:22.785+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Job 43 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T19:09:22.785+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 74: Stage finished
[2025-06-05T19:09:22.785+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Job 43 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94, took 0.012990 s
[2025-06-05T19:09:22.853+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO FileSourceStrategy: Pushed Filters:
[2025-06-05T19:09:22.853+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO FileSourceStrategy: Post-Scan Filters:
[2025-06-05T19:09:22.867+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 201.4 KiB, free 431.5 MiB)
[2025-06-05T19:09:22.873+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 431.5 MiB)
[2025-06-05T19:09:22.874+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on 10.255.255.254:43723 (size: 35.0 KiB, free: 434.0 MiB)
[2025-06-05T19:09:22.874+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO SparkContext: Created broadcast 55 from first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93
[2025-06-05T19:09:22.875+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-06-05T19:09:22.888+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93
[2025-06-05T19:09:22.889+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Got job 44 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) with 1 output partitions
[2025-06-05T19:09:22.889+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Final stage: ResultStage 75 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93)
[2025-06-05T19:09:22.889+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T19:09:22.889+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:22.889+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Submitting ResultStage 75 (MapPartitionsRDD[155] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93), which has no missing parents
[2025-06-05T19:09:22.891+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO MemoryStore: Block broadcast_56 stored as values in memory (estimated size 19.3 KiB, free 431.5 MiB)
[2025-06-05T19:09:22.892+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 431.5 MiB)
[2025-06-05T19:09:22.892+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on 10.255.255.254:43723 (size: 8.6 KiB, free: 433.9 MiB)
[2025-06-05T19:09:22.893+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO SparkContext: Created broadcast 56 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:22.893+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 75 (MapPartitionsRDD[155] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:22.893+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSchedulerImpl: Adding task set 75.0 with 1 tasks resource profile 0
[2025-06-05T19:09:22.895+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSetManager: Starting task 0.0 in stage 75.0 (TID 44) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9612 bytes)
[2025-06-05T19:09:22.895+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO Executor: Running task 0.0 in stage 75.0 (TID 44)
[2025-06-05T19:09:22.900+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T19:09:22.908+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO Executor: Finished task 0.0 in stage 75.0 (TID 44). 10024 bytes result sent to driver
[2025-06-05T19:09:22.909+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSetManager: Finished task 0.0 in stage 75.0 (TID 44) in 15 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:22.909+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSchedulerImpl: Removed TaskSet 75.0, whose tasks have all completed, from pool
[2025-06-05T19:09:22.909+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: ResultStage 75 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) finished in 0.019 s
[2025-06-05T19:09:22.910+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Job 44 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T19:09:22.910+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 75: Stage finished
[2025-06-05T19:09:22.910+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Job 44 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93, took 0.021936 s
[2025-06-05T19:09:22.912+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Registering RDD 156 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) as input to shuffle 21
[2025-06-05T19:09:22.913+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Got map stage job 45 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) with 1 output partitions
[2025-06-05T19:09:22.913+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Final stage: ShuffleMapStage 76 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93)
[2025-06-05T19:09:22.913+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T19:09:22.913+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:22.913+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Submitting ShuffleMapStage 76 (MapPartitionsRDD[156] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93), which has no missing parents
[2025-06-05T19:09:22.916+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO MemoryStore: Block broadcast_57 stored as values in memory (estimated size 27.6 KiB, free 431.4 MiB)
[2025-06-05T19:09:22.929+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 431.4 MiB)
[2025-06-05T19:09:22.929+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on 10.255.255.254:43723 (size: 10.7 KiB, free: 433.9 MiB)
[2025-06-05T19:09:22.932+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO SparkContext: Created broadcast 57 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:22.932+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO BlockManagerInfo: Removed broadcast_52_piece0 on 10.255.255.254:43723 in memory (size: 10.7 KiB, free: 433.9 MiB)
[2025-06-05T19:09:22.933+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 76 (MapPartitionsRDD[156] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:22.933+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSchedulerImpl: Adding task set 76.0 with 1 tasks resource profile 0
[2025-06-05T19:09:22.935+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO TaskSetManager: Starting task 0.0 in stage 76.0 (TID 45) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9601 bytes)
[2025-06-05T19:09:22.936+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO Executor: Running task 0.0 in stage 76.0 (TID 45)
[2025-06-05T19:09:22.936+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO BlockManagerInfo: Removed broadcast_53_piece0 on 10.255.255.254:43723 in memory (size: 20.5 KiB, free: 434.0 MiB)
[2025-06-05T19:09:22.941+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO BlockManagerInfo: Removed broadcast_56_piece0 on 10.255.255.254:43723 in memory (size: 8.6 KiB, free: 434.0 MiB)
[2025-06-05T19:09:22.947+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO BlockManagerInfo: Removed broadcast_54_piece0 on 10.255.255.254:43723 in memory (size: 7.2 KiB, free: 434.0 MiB)
[2025-06-05T19:09:22.948+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:22 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T19:09:23.248+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO Executor: Finished task 0.0 in stage 76.0 (TID 45). 2199 bytes result sent to driver
[2025-06-05T19:09:23.249+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO TaskSetManager: Finished task 0.0 in stage 76.0 (TID 45) in 315 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:23.249+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO TaskSchedulerImpl: Removed TaskSet 76.0, whose tasks have all completed, from pool
[2025-06-05T19:09:23.250+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: ShuffleMapStage 76 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) finished in 0.337 s
[2025-06-05T19:09:23.250+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T19:09:23.251+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: running: Set()
[2025-06-05T19:09:23.251+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: waiting: Set()
[2025-06-05T19:09:23.251+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: failed: Set()
[2025-06-05T19:09:23.256+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO ShufflePartitionsUtil: For shuffle(21), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-06-05T19:09:23.286+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Registering RDD 160 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) as input to shuffle 22
[2025-06-05T19:09:23.286+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Got map stage job 46 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) with 1 output partitions
[2025-06-05T19:09:23.287+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Final stage: ShuffleMapStage 78 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93)
[2025-06-05T19:09:23.287+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 77)
[2025-06-05T19:09:23.287+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:23.287+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Submitting ShuffleMapStage 78 (MapPartitionsRDD[160] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93), which has no missing parents
[2025-06-05T19:09:23.297+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO MemoryStore: Block broadcast_58 stored as values in memory (estimated size 35.4 KiB, free 431.6 MiB)
[2025-06-05T19:09:23.299+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 17.1 KiB, free 431.5 MiB)
[2025-06-05T19:09:23.299+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on 10.255.255.254:43723 (size: 17.1 KiB, free: 434.0 MiB)
[2025-06-05T19:09:23.300+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:23.301+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 78 (MapPartitionsRDD[160] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:23.301+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO TaskSchedulerImpl: Adding task set 78.0 with 1 tasks resource profile 0
[2025-06-05T19:09:23.304+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO TaskSetManager: Starting task 0.0 in stage 78.0 (TID 46) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8988 bytes)
[2025-06-05T19:09:23.304+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO Executor: Running task 0.0 in stage 78.0 (TID 46)
[2025-06-05T19:09:23.310+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO ShuffleBlockFetcherIterator: Getting 1 (13.5 KiB) non-empty blocks including 1 (13.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T19:09:23.311+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T19:09:23.330+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO Executor: Finished task 0.0 in stage 78.0 (TID 46). 5170 bytes result sent to driver
[2025-06-05T19:09:23.334+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO TaskSetManager: Finished task 0.0 in stage 78.0 (TID 46) in 32 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:23.335+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO TaskSchedulerImpl: Removed TaskSet 78.0, whose tasks have all completed, from pool
[2025-06-05T19:09:23.336+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: ShuffleMapStage 78 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) finished in 0.043 s
[2025-06-05T19:09:23.336+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T19:09:23.336+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: running: Set()
[2025-06-05T19:09:23.337+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: waiting: Set()
[2025-06-05T19:09:23.337+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: failed: Set()
[2025-06-05T19:09:23.389+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93
[2025-06-05T19:09:23.390+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Got job 47 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) with 1 output partitions
[2025-06-05T19:09:23.390+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Final stage: ResultStage 81 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93)
[2025-06-05T19:09:23.390+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 80)
[2025-06-05T19:09:23.391+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:23.391+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Submitting ResultStage 81 (MapPartitionsRDD[163] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93), which has no missing parents
[2025-06-05T19:09:23.393+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO MemoryStore: Block broadcast_59 stored as values in memory (estimated size 35.0 KiB, free 431.5 MiB)
[2025-06-05T19:09:23.394+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 16.6 KiB, free 431.5 MiB)
[2025-06-05T19:09:23.394+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on 10.255.255.254:43723 (size: 16.6 KiB, free: 433.9 MiB)
[2025-06-05T19:09:23.395+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:23.395+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 81 (MapPartitionsRDD[163] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:23.395+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO TaskSchedulerImpl: Adding task set 81.0 with 1 tasks resource profile 0
[2025-06-05T19:09:23.396+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO TaskSetManager: Starting task 0.0 in stage 81.0 (TID 47) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8999 bytes)
[2025-06-05T19:09:23.398+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO Executor: Running task 0.0 in stage 81.0 (TID 47)
[2025-06-05T19:09:23.407+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO ShuffleBlockFetcherIterator: Getting 1 (1051.0 B) non-empty blocks including 1 (1051.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T19:09:23.407+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T19:09:23.412+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO Executor: Finished task 0.0 in stage 81.0 (TID 47). 6437 bytes result sent to driver
[2025-06-05T19:09:23.414+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO TaskSetManager: Finished task 0.0 in stage 81.0 (TID 47) in 17 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:23.414+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO TaskSchedulerImpl: Removed TaskSet 81.0, whose tasks have all completed, from pool
[2025-06-05T19:09:23.414+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: ResultStage 81 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93) finished in 0.023 s
[2025-06-05T19:09:23.415+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Job 47 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T19:09:23.415+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 81: Stage finished
[2025-06-05T19:09:23.415+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Job 47 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:93, took 0.026007 s
[2025-06-05T19:09:23.477+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO FileSourceStrategy: Pushed Filters:
[2025-06-05T19:09:23.477+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO FileSourceStrategy: Post-Scan Filters:
[2025-06-05T19:09:23.490+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 201.4 KiB, free 431.3 MiB)
[2025-06-05T19:09:23.496+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 431.3 MiB)
[2025-06-05T19:09:23.496+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on 10.255.255.254:43723 (size: 35.0 KiB, free: 433.9 MiB)
[2025-06-05T19:09:23.497+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO SparkContext: Created broadcast 60 from first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94
[2025-06-05T19:09:23.498+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-06-05T19:09:23.511+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94
[2025-06-05T19:09:23.512+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Got job 48 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) with 1 output partitions
[2025-06-05T19:09:23.512+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Final stage: ResultStage 82 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94)
[2025-06-05T19:09:23.512+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T19:09:23.512+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:23.513+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Submitting ResultStage 82 (MapPartitionsRDD[169] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94), which has no missing parents
[2025-06-05T19:09:23.514+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO MemoryStore: Block broadcast_61 stored as values in memory (estimated size 19.3 KiB, free 431.2 MiB)
[2025-06-05T19:09:23.515+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 431.2 MiB)
[2025-06-05T19:09:23.515+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on 10.255.255.254:43723 (size: 8.6 KiB, free: 433.9 MiB)
[2025-06-05T19:09:23.515+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO SparkContext: Created broadcast 61 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:23.516+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 82 (MapPartitionsRDD[169] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:23.516+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO TaskSchedulerImpl: Adding task set 82.0 with 1 tasks resource profile 0
[2025-06-05T19:09:23.517+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO TaskSetManager: Starting task 0.0 in stage 82.0 (TID 48) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9612 bytes)
[2025-06-05T19:09:23.517+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO Executor: Running task 0.0 in stage 82.0 (TID 48)
[2025-06-05T19:09:23.523+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T19:09:23.530+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO Executor: Finished task 0.0 in stage 82.0 (TID 48). 10024 bytes result sent to driver
[2025-06-05T19:09:23.531+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO TaskSetManager: Finished task 0.0 in stage 82.0 (TID 48) in 15 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:23.531+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO TaskSchedulerImpl: Removed TaskSet 82.0, whose tasks have all completed, from pool
[2025-06-05T19:09:23.532+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: ResultStage 82 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) finished in 0.018 s
[2025-06-05T19:09:23.532+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Job 48 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T19:09:23.532+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 82: Stage finished
[2025-06-05T19:09:23.532+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Job 48 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94, took 0.021062 s
[2025-06-05T19:09:23.535+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Registering RDD 170 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) as input to shuffle 23
[2025-06-05T19:09:23.535+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Got map stage job 49 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) with 1 output partitions
[2025-06-05T19:09:23.535+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Final stage: ShuffleMapStage 83 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94)
[2025-06-05T19:09:23.535+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T19:09:23.536+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:23.536+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Submitting ShuffleMapStage 83 (MapPartitionsRDD[170] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94), which has no missing parents
[2025-06-05T19:09:23.540+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO MemoryStore: Block broadcast_62 stored as values in memory (estimated size 27.6 KiB, free 431.2 MiB)
[2025-06-05T19:09:23.541+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 431.2 MiB)
[2025-06-05T19:09:23.541+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on 10.255.255.254:43723 (size: 10.7 KiB, free: 433.9 MiB)
[2025-06-05T19:09:23.541+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:23.542+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 83 (MapPartitionsRDD[170] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:23.542+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO TaskSchedulerImpl: Adding task set 83.0 with 1 tasks resource profile 0
[2025-06-05T19:09:23.543+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO TaskSetManager: Starting task 0.0 in stage 83.0 (TID 49) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9601 bytes)
[2025-06-05T19:09:23.543+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO Executor: Running task 0.0 in stage 83.0 (TID 49)
[2025-06-05T19:09:23.549+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T19:09:23.595+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO Executor: Finished task 0.0 in stage 83.0 (TID 49). 2199 bytes result sent to driver
[2025-06-05T19:09:23.597+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO TaskSetManager: Finished task 0.0 in stage 83.0 (TID 49) in 54 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:23.597+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO TaskSchedulerImpl: Removed TaskSet 83.0, whose tasks have all completed, from pool
[2025-06-05T19:09:23.597+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: ShuffleMapStage 83 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) finished in 0.061 s
[2025-06-05T19:09:23.597+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T19:09:23.597+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: running: Set()
[2025-06-05T19:09:23.597+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: waiting: Set()
[2025-06-05T19:09:23.598+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: failed: Set()
[2025-06-05T19:09:23.601+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO ShufflePartitionsUtil: For shuffle(23), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-06-05T19:09:23.619+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Registering RDD 173 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) as input to shuffle 24
[2025-06-05T19:09:23.620+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Got map stage job 50 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) with 1 output partitions
[2025-06-05T19:09:23.620+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Final stage: ShuffleMapStage 85 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94)
[2025-06-05T19:09:23.620+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 84)
[2025-06-05T19:09:23.620+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:23.620+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Submitting ShuffleMapStage 85 (MapPartitionsRDD[173] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94), which has no missing parents
[2025-06-05T19:09:23.623+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO MemoryStore: Block broadcast_63 stored as values in memory (estimated size 45.0 KiB, free 431.1 MiB)
[2025-06-05T19:09:23.624+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 20.5 KiB, free 431.1 MiB)
[2025-06-05T19:09:23.625+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on 10.255.255.254:43723 (size: 20.5 KiB, free: 433.9 MiB)
[2025-06-05T19:09:23.625+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:23.626+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 85 (MapPartitionsRDD[173] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:23.626+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO TaskSchedulerImpl: Adding task set 85.0 with 1 tasks resource profile 0
[2025-06-05T19:09:23.627+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO TaskSetManager: Starting task 0.0 in stage 85.0 (TID 50) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8988 bytes)
[2025-06-05T19:09:23.628+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO Executor: Running task 0.0 in stage 85.0 (TID 50)
[2025-06-05T19:09:23.633+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO ShuffleBlockFetcherIterator: Getting 1 (13.5 KiB) non-empty blocks including 1 (13.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T19:09:23.633+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T19:09:23.638+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO Executor: Finished task 0.0 in stage 85.0 (TID 50). 5314 bytes result sent to driver
[2025-06-05T19:09:23.640+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO TaskSetManager: Finished task 0.0 in stage 85.0 (TID 50) in 12 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:23.640+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO TaskSchedulerImpl: Removed TaskSet 85.0, whose tasks have all completed, from pool
[2025-06-05T19:09:23.640+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: ShuffleMapStage 85 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) finished in 0.019 s
[2025-06-05T19:09:23.641+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T19:09:23.641+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: running: Set()
[2025-06-05T19:09:23.641+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: waiting: Set()
[2025-06-05T19:09:23.641+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: failed: Set()
[2025-06-05T19:09:23.657+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94
[2025-06-05T19:09:23.658+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Got job 51 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) with 1 output partitions
[2025-06-05T19:09:23.659+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Final stage: ResultStage 88 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94)
[2025-06-05T19:09:23.659+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 87)
[2025-06-05T19:09:23.659+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:23.659+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Submitting ResultStage 88 (MapPartitionsRDD[176] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94), which has no missing parents
[2025-06-05T19:09:23.661+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO MemoryStore: Block broadcast_64 stored as values in memory (estimated size 16.9 KiB, free 431.1 MiB)
[2025-06-05T19:09:23.685+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 431.1 MiB)
[2025-06-05T19:09:23.686+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO BlockManagerInfo: Removed broadcast_59_piece0 on 10.255.255.254:43723 in memory (size: 16.6 KiB, free: 433.9 MiB)
[2025-06-05T19:09:23.687+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on 10.255.255.254:43723 (size: 7.2 KiB, free: 433.9 MiB)
[2025-06-05T19:09:23.687+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:23.695+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 88 (MapPartitionsRDD[176] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:23.695+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO TaskSchedulerImpl: Adding task set 88.0 with 1 tasks resource profile 0
[2025-06-05T19:09:23.695+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO TaskSetManager: Starting task 0.0 in stage 88.0 (TID 51) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8999 bytes)
[2025-06-05T19:09:23.696+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO BlockManagerInfo: Removed broadcast_57_piece0 on 10.255.255.254:43723 in memory (size: 10.7 KiB, free: 433.9 MiB)
[2025-06-05T19:09:23.696+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO Executor: Running task 0.0 in stage 88.0 (TID 51)
[2025-06-05T19:09:23.697+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T19:09:23.697+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T19:09:23.698+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO BlockManagerInfo: Removed broadcast_61_piece0 on 10.255.255.254:43723 in memory (size: 8.6 KiB, free: 433.9 MiB)
[2025-06-05T19:09:23.699+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO Executor: Finished task 0.0 in stage 88.0 (TID 51). 3995 bytes result sent to driver
[2025-06-05T19:09:23.701+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO TaskSetManager: Finished task 0.0 in stage 88.0 (TID 51) in 11 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:23.701+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO TaskSchedulerImpl: Removed TaskSet 88.0, whose tasks have all completed, from pool
[2025-06-05T19:09:23.702+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: ResultStage 88 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94) finished in 0.042 s
[2025-06-05T19:09:23.702+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Job 51 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T19:09:23.702+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 88: Stage finished
[2025-06-05T19:09:23.703+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO BlockManagerInfo: Removed broadcast_62_piece0 on 10.255.255.254:43723 in memory (size: 10.7 KiB, free: 433.9 MiB)
[2025-06-05T19:09:23.704+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO DAGScheduler: Job 51 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:94, took 0.046288 s
[2025-06-05T19:09:23.714+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO BlockManagerInfo: Removed broadcast_58_piece0 on 10.255.255.254:43723 in memory (size: 17.1 KiB, free: 433.9 MiB)
[2025-06-05T19:09:23.719+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:23 INFO BlockManagerInfo: Removed broadcast_63_piece0 on 10.255.255.254:43723 in memory (size: 20.5 KiB, free: 433.9 MiB)
[2025-06-05T19:09:24.303+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:24 INFO BlockManagerInfo: Removed broadcast_64_piece0 on 10.255.255.254:43723 in memory (size: 7.2 KiB, free: 434.0 MiB)
[2025-06-05T19:09:25.238+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO FileSourceStrategy: Pushed Filters:
[2025-06-05T19:09:25.239+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO FileSourceStrategy: Post-Scan Filters:
[2025-06-05T19:09:25.310+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.353+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.355+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.403+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.404+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.405+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.406+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.407+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.425+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.426+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.426+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.426+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.427+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.427+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.428+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.466+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[2025-06-05T19:09:25.522+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO MemoryStore: Block broadcast_65 stored as values in memory (estimated size 202.1 KiB, free 431.2 MiB)
[2025-06-05T19:09:25.531+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 35.3 KiB, free 431.2 MiB)
[2025-06-05T19:09:25.532+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on 10.255.255.254:43723 (size: 35.3 KiB, free: 433.9 MiB)
[2025-06-05T19:09:25.533+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO SparkContext: Created broadcast 65 from parquet at NativeMethodAccessorImpl.java:0
[2025-06-05T19:09:25.534+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-06-05T19:09:25.559+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-06-05T19:09:25.560+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: Got job 52 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-06-05T19:09:25.561+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: Final stage: ResultStage 89 (parquet at NativeMethodAccessorImpl.java:0)
[2025-06-05T19:09:25.561+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T19:09:25.561+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:25.561+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: Submitting ResultStage 89 (MapPartitionsRDD[182] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-06-05T19:09:25.563+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO MemoryStore: Block broadcast_66 stored as values in memory (estimated size 22.7 KiB, free 431.1 MiB)
[2025-06-05T19:09:25.564+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 431.1 MiB)
[2025-06-05T19:09:25.564+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on 10.255.255.254:43723 (size: 9.2 KiB, free: 433.9 MiB)
[2025-06-05T19:09:25.565+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:25.565+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 89 (MapPartitionsRDD[182] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:25.565+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO TaskSchedulerImpl: Adding task set 89.0 with 1 tasks resource profile 0
[2025-06-05T19:09:25.567+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO TaskSetManager: Starting task 0.0 in stage 89.0 (TID 52) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9612 bytes)
[2025-06-05T19:09:25.568+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO Executor: Running task 0.0 in stage 89.0 (TID 52)
[2025-06-05T19:09:25.575+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T19:09:25.584+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO Executor: Finished task 0.0 in stage 89.0 (TID 52). 10024 bytes result sent to driver
[2025-06-05T19:09:25.585+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO TaskSetManager: Finished task 0.0 in stage 89.0 (TID 52) in 19 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:25.586+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO TaskSchedulerImpl: Removed TaskSet 89.0, whose tasks have all completed, from pool
[2025-06-05T19:09:25.586+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: ResultStage 89 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.025 s
[2025-06-05T19:09:25.587+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: Job 52 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T19:09:25.587+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 89: Stage finished
[2025-06-05T19:09:25.587+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: Job 52 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.027543 s
[2025-06-05T19:09:25.590+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: Registering RDD 183 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 25
[2025-06-05T19:09:25.590+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: Got map stage job 53 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-06-05T19:09:25.591+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: Final stage: ShuffleMapStage 90 (parquet at NativeMethodAccessorImpl.java:0)
[2025-06-05T19:09:25.591+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T19:09:25.591+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:25.591+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: Submitting ShuffleMapStage 90 (MapPartitionsRDD[183] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-06-05T19:09:25.595+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO MemoryStore: Block broadcast_67 stored as values in memory (estimated size 30.9 KiB, free 431.1 MiB)
[2025-06-05T19:09:25.596+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 11.2 KiB, free 431.1 MiB)
[2025-06-05T19:09:25.596+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on 10.255.255.254:43723 (size: 11.2 KiB, free: 433.9 MiB)
[2025-06-05T19:09:25.597+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO SparkContext: Created broadcast 67 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:25.597+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 90 (MapPartitionsRDD[183] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:25.597+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO TaskSchedulerImpl: Adding task set 90.0 with 1 tasks resource profile 0
[2025-06-05T19:09:25.598+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO TaskSetManager: Starting task 0.0 in stage 90.0 (TID 53) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9601 bytes)
[2025-06-05T19:09:25.599+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO Executor: Running task 0.0 in stage 90.0 (TID 53)
[2025-06-05T19:09:25.605+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T19:09:25.647+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO Executor: Finished task 0.0 in stage 90.0 (TID 53). 2199 bytes result sent to driver
[2025-06-05T19:09:25.648+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO TaskSetManager: Finished task 0.0 in stage 90.0 (TID 53) in 49 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:25.648+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO TaskSchedulerImpl: Removed TaskSet 90.0, whose tasks have all completed, from pool
[2025-06-05T19:09:25.648+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: ShuffleMapStage 90 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.057 s
[2025-06-05T19:09:25.649+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T19:09:25.649+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: running: Set()
[2025-06-05T19:09:25.649+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: waiting: Set()
[2025-06-05T19:09:25.649+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: failed: Set()
[2025-06-05T19:09:25.690+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.690+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.691+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.691+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.692+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.692+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.693+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.698+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.698+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.698+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.699+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.699+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.699+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.699+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.704+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO ShufflePartitionsUtil: For shuffle(25), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-06-05T19:09:25.740+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO BlockManagerInfo: Removed broadcast_66_piece0 on 10.255.255.254:43723 in memory (size: 9.2 KiB, free: 433.9 MiB)
[2025-06-05T19:09:25.743+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO BlockManagerInfo: Removed broadcast_67_piece0 on 10.255.255.254:43723 in memory (size: 11.2 KiB, free: 433.9 MiB)
[2025-06-05T19:09:25.752+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: Registering RDD 186 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 26
[2025-06-05T19:09:25.752+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: Got map stage job 54 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-06-05T19:09:25.752+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: Final stage: ShuffleMapStage 92 (parquet at NativeMethodAccessorImpl.java:0)
[2025-06-05T19:09:25.753+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 91)
[2025-06-05T19:09:25.753+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:25.753+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: Submitting ShuffleMapStage 92 (MapPartitionsRDD[186] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-06-05T19:09:25.757+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO MemoryStore: Block broadcast_68 stored as values in memory (estimated size 32.7 KiB, free 431.1 MiB)
[2025-06-05T19:09:25.758+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 15.6 KiB, free 431.1 MiB)
[2025-06-05T19:09:25.758+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on 10.255.255.254:43723 (size: 15.6 KiB, free: 433.9 MiB)
[2025-06-05T19:09:25.759+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO SparkContext: Created broadcast 68 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:25.759+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 92 (MapPartitionsRDD[186] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:25.759+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO TaskSchedulerImpl: Adding task set 92.0 with 1 tasks resource profile 0
[2025-06-05T19:09:25.761+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO TaskSetManager: Starting task 0.0 in stage 92.0 (TID 54) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8988 bytes)
[2025-06-05T19:09:25.761+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO Executor: Running task 0.0 in stage 92.0 (TID 54)
[2025-06-05T19:09:25.767+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO ShuffleBlockFetcherIterator: Getting 1 (22.6 KiB) non-empty blocks including 1 (22.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T19:09:25.768+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T19:09:25.774+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO Executor: Finished task 0.0 in stage 92.0 (TID 54). 4884 bytes result sent to driver
[2025-06-05T19:09:25.775+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO TaskSetManager: Finished task 0.0 in stage 92.0 (TID 54) in 15 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:25.776+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO TaskSchedulerImpl: Removed TaskSet 92.0, whose tasks have all completed, from pool
[2025-06-05T19:09:25.776+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: ShuffleMapStage 92 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.021 s
[2025-06-05T19:09:25.776+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T19:09:25.777+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: running: Set()
[2025-06-05T19:09:25.777+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: waiting: Set()
[2025-06-05T19:09:25.777+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO DAGScheduler: failed: Set()
[2025-06-05T19:09:25.802+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.803+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.803+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.803+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.803+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.803+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.804+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.806+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.806+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.807+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.807+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.807+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.807+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.808+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T19:09:25.856+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-06-05T19:09:25.874+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-06-05T19:09:25.875+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-06-05T19:09:25.875+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-06-05T19:09:25.875+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-06-05T19:09:25.875+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-06-05T19:09:25.875+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-06-05T19:09:26.011+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 52.702927 ms
[2025-06-05T19:09:26.031+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 10.914224 ms
[2025-06-05T19:09:26.053+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 13.234361 ms
[2025-06-05T19:09:26.068+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 11.996579 ms
[2025-06-05T19:09:26.081+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 8.933298 ms
[2025-06-05T19:09:26.094+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 8.482081 ms
[2025-06-05T19:09:26.103+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 6.021683 ms
[2025-06-05T19:09:26.113+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 8.019114 ms
[2025-06-05T19:09:26.198+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-06-05T19:09:26.200+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO DAGScheduler: Got job 55 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-06-05T19:09:26.200+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO DAGScheduler: Final stage: ResultStage 95 (parquet at NativeMethodAccessorImpl.java:0)
[2025-06-05T19:09:26.200+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 94)
[2025-06-05T19:09:26.200+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO DAGScheduler: Missing parents: List()
[2025-06-05T19:09:26.200+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO DAGScheduler: Submitting ResultStage 95 (MapPartitionsRDD[203] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-06-05T19:09:26.254+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO MemoryStore: Block broadcast_69 stored as values in memory (estimated size 359.5 KiB, free 430.8 MiB)
[2025-06-05T19:09:26.256+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 116.2 KiB, free 430.7 MiB)
[2025-06-05T19:09:26.256+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on 10.255.255.254:43723 (size: 116.2 KiB, free: 433.8 MiB)
[2025-06-05T19:09:26.257+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1585
[2025-06-05T19:09:26.257+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 95 (MapPartitionsRDD[203] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-06-05T19:09:26.257+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO TaskSchedulerImpl: Adding task set 95.0 with 1 tasks resource profile 0
[2025-06-05T19:09:26.258+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO TaskSetManager: Starting task 0.0 in stage 95.0 (TID 55) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8999 bytes)
[2025-06-05T19:09:26.267+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO Executor: Running task 0.0 in stage 95.0 (TID 55)
[2025-06-05T19:09:26.305+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO ShuffleBlockFetcherIterator: Getting 1 (7.6 KiB) non-empty blocks including 1 (7.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T19:09:26.305+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T19:09:26.312+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 6.986577 ms
[2025-06-05T19:09:26.339+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 8.486244 ms
[2025-06-05T19:09:26.364+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 4.639665 ms
[2025-06-05T19:09:26.376+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 6.346723 ms
[2025-06-05T19:09:26.387+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 6.94188 ms
[2025-06-05T19:09:26.397+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 8.555934 ms
[2025-06-05T19:09:26.409+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 7.592527 ms
[2025-06-05T19:09:26.442+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO BlockManagerInfo: Removed broadcast_68_piece0 on 10.255.255.254:43723 in memory (size: 15.6 KiB, free: 433.8 MiB)
[2025-06-05T19:09:26.444+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 19.611345 ms
[2025-06-05T19:09:26.462+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 9.888601 ms
[2025-06-05T19:09:26.478+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 10.811912 ms
[2025-06-05T19:09:26.486+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 5.021275 ms
[2025-06-05T19:09:26.496+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 7.171355 ms
[2025-06-05T19:09:26.502+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 4.155956 ms
[2025-06-05T19:09:26.511+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 7.694875 ms
[2025-06-05T19:09:26.523+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 7.870815 ms
[2025-06-05T19:09:26.543+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 9.404211 ms
[2025-06-05T19:09:26.550+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 3.722409 ms
[2025-06-05T19:09:26.558+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 5.060416 ms
[2025-06-05T19:09:26.565+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 6.183506 ms
[2025-06-05T19:09:26.575+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 7.468888 ms
[2025-06-05T19:09:26.611+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 9.881473 ms
[2025-06-05T19:09:26.619+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 4.754223 ms
[2025-06-05T19:09:26.627+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 5.307316 ms
[2025-06-05T19:09:26.636+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 7.689908 ms
[2025-06-05T19:09:26.649+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 9.085909 ms
[2025-06-05T19:09:26.681+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 8.319705 ms
[2025-06-05T19:09:26.689+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 4.392707 ms
[2025-06-05T19:09:26.698+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 5.924799 ms
[2025-06-05T19:09:26.709+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 9.932322 ms
[2025-06-05T19:09:26.720+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 7.611048 ms
[2025-06-05T19:09:26.750+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 9.929953 ms
[2025-06-05T19:09:26.760+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 6.820809 ms
[2025-06-05T19:09:26.768+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 5.277687 ms
[2025-06-05T19:09:26.783+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 13.317316 ms
[2025-06-05T19:09:26.798+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 11.282594 ms
[2025-06-05T19:09:26.823+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 7.554316 ms
[2025-06-05T19:09:26.869+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodeGenerator: Code generated in 41.689299 ms
[2025-06-05T19:09:26.873+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-06-05T19:09:26.873+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-06-05T19:09:26.874+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-06-05T19:09:26.874+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-06-05T19:09:26.874+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-06-05T19:09:26.874+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-06-05T19:09:26.882+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodecConfig: Compression: SNAPPY
[2025-06-05T19:09:26.884+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO CodecConfig: Compression: SNAPPY
[2025-06-05T19:09:26.932+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-06-05T19:09:26.979+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:26 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2025-06-05T19:09:26.980+0200] {spark_submit.py:521} INFO - {
[2025-06-05T19:09:26.980+0200] {spark_submit.py:521} INFO - "type" : "struct",
[2025-06-05T19:09:26.980+0200] {spark_submit.py:521} INFO - "fields" : [ {
[2025-06-05T19:09:26.980+0200] {spark_submit.py:521} INFO - "name" : "date",
[2025-06-05T19:09:26.980+0200] {spark_submit.py:521} INFO - "type" : "date",
[2025-06-05T19:09:26.980+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T19:09:26.980+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.980+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.980+0200] {spark_submit.py:521} INFO - "name" : "INFLATION",
[2025-06-05T19:09:26.980+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T19:09:26.981+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T19:09:26.981+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.981+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.981+0200] {spark_submit.py:521} INFO - "name" : "UNEMPLOYMENT",
[2025-06-05T19:09:26.981+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T19:09:26.981+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T19:09:26.981+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.981+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.981+0200] {spark_submit.py:521} INFO - "name" : "CONSUMER_SENTIMENT",
[2025-06-05T19:09:26.981+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T19:09:26.981+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T19:09:26.981+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.981+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.982+0200] {spark_submit.py:521} INFO - "name" : "High_Yield_Bond_SPREAD",
[2025-06-05T19:09:26.982+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T19:09:26.982+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T19:09:26.982+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.982+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.982+0200] {spark_submit.py:521} INFO - "name" : "10-2Year_Treasury_Yield_Bond",
[2025-06-05T19:09:26.982+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T19:09:26.982+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T19:09:26.982+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.982+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.982+0200] {spark_submit.py:521} INFO - "name" : "TAUX_FED",
[2025-06-05T19:09:26.982+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T19:09:26.983+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T19:09:26.983+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.983+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.983+0200] {spark_submit.py:521} INFO - "name" : "INFLATION_delta",
[2025-06-05T19:09:26.983+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T19:09:26.983+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T19:09:26.983+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.983+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.984+0200] {spark_submit.py:521} INFO - "name" : "UNEMPLOYMENT_delta",
[2025-06-05T19:09:26.984+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T19:09:26.984+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T19:09:26.984+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.984+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.984+0200] {spark_submit.py:521} INFO - "name" : "CONSUMER_SENTIMENT_delta",
[2025-06-05T19:09:26.984+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T19:09:26.984+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T19:09:26.984+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.984+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.984+0200] {spark_submit.py:521} INFO - "name" : "High_Yield_Bond_SPREAD_delta",
[2025-06-05T19:09:26.984+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T19:09:26.984+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T19:09:26.985+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.985+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.985+0200] {spark_submit.py:521} INFO - "name" : "10-2Year_Treasury_Yield_Bond_delta",
[2025-06-05T19:09:26.985+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T19:09:26.985+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T19:09:26.985+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.985+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.985+0200] {spark_submit.py:521} INFO - "name" : "TAUX_FED_delta",
[2025-06-05T19:09:26.985+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T19:09:26.985+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T19:09:26.985+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.985+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.985+0200] {spark_submit.py:521} INFO - "name" : "INFLATION_zscore",
[2025-06-05T19:09:26.986+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T19:09:26.986+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T19:09:26.986+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.986+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.986+0200] {spark_submit.py:521} INFO - "name" : "UNEMPLOYMENT_zscore",
[2025-06-05T19:09:26.986+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T19:09:26.986+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T19:09:26.986+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.986+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.986+0200] {spark_submit.py:521} INFO - "name" : "CONSUMER_SENTIMENT_zscore",
[2025-06-05T19:09:26.987+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T19:09:26.987+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T19:09:26.987+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.987+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.987+0200] {spark_submit.py:521} INFO - "name" : "High_Yield_Bond_SPREAD_zscore",
[2025-06-05T19:09:26.987+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T19:09:26.987+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T19:09:26.987+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.987+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.987+0200] {spark_submit.py:521} INFO - "name" : "10-2Year_Treasury_Yield_Bond_zscore",
[2025-06-05T19:09:26.987+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T19:09:26.987+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T19:09:26.988+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.988+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.988+0200] {spark_submit.py:521} INFO - "name" : "TAUX_FED_zscore",
[2025-06-05T19:09:26.988+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T19:09:26.988+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T19:09:26.988+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.988+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.988+0200] {spark_submit.py:521} INFO - "name" : "INFLATION_pos_score",
[2025-06-05T19:09:26.988+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T19:09:26.988+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T19:09:26.989+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.989+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.989+0200] {spark_submit.py:521} INFO - "name" : "UNEMPLOYMENT_pos_score",
[2025-06-05T19:09:26.989+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T19:09:26.989+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T19:09:26.989+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.989+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.989+0200] {spark_submit.py:521} INFO - "name" : "CONSUMER_SENTIMENT_pos_score",
[2025-06-05T19:09:26.989+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T19:09:26.989+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T19:09:26.989+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.990+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.990+0200] {spark_submit.py:521} INFO - "name" : "High_Yield_Bond_SPREAD_pos_score",
[2025-06-05T19:09:26.990+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T19:09:26.990+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T19:09:26.990+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.990+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.990+0200] {spark_submit.py:521} INFO - "name" : "10-2Year_Treasury_Yield_Bond_pos_score",
[2025-06-05T19:09:26.991+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T19:09:26.991+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T19:09:26.991+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.991+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.991+0200] {spark_submit.py:521} INFO - "name" : "TAUX_FED_pos_score",
[2025-06-05T19:09:26.991+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T19:09:26.991+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T19:09:26.991+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.991+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.991+0200] {spark_submit.py:521} INFO - "name" : "INFLATION_var_score",
[2025-06-05T19:09:26.991+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T19:09:26.992+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T19:09:26.992+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.992+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.992+0200] {spark_submit.py:521} INFO - "name" : "UNEMPLOYMENT_var_score",
[2025-06-05T19:09:26.992+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T19:09:26.992+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T19:09:26.992+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.992+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.992+0200] {spark_submit.py:521} INFO - "name" : "CONSUMER_SENTIMENT_var_score",
[2025-06-05T19:09:26.992+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T19:09:26.992+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T19:09:26.992+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.993+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.993+0200] {spark_submit.py:521} INFO - "name" : "High_Yield_Bond_SPREAD_var_score",
[2025-06-05T19:09:26.993+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T19:09:26.993+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T19:09:26.993+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.993+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.994+0200] {spark_submit.py:521} INFO - "name" : "10-2Year_Treasury_Yield_Bond_var_score",
[2025-06-05T19:09:26.994+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T19:09:26.994+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T19:09:26.994+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.994+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.994+0200] {spark_submit.py:521} INFO - "name" : "TAUX_FED_var_score",
[2025-06-05T19:09:26.995+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T19:09:26.995+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T19:09:26.995+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.995+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.995+0200] {spark_submit.py:521} INFO - "name" : "INFLATION_combined",
[2025-06-05T19:09:26.995+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T19:09:26.995+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T19:09:26.995+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.995+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.996+0200] {spark_submit.py:521} INFO - "name" : "UNEMPLOYMENT_combined",
[2025-06-05T19:09:26.996+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T19:09:26.996+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T19:09:26.996+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.996+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.996+0200] {spark_submit.py:521} INFO - "name" : "CONSUMER_SENTIMENT_combined",
[2025-06-05T19:09:26.996+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T19:09:26.996+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T19:09:26.996+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.996+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.996+0200] {spark_submit.py:521} INFO - "name" : "High_Yield_Bond_SPREAD_combined",
[2025-06-05T19:09:26.996+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T19:09:26.997+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T19:09:26.997+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.997+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.997+0200] {spark_submit.py:521} INFO - "name" : "10-2Year_Treasury_Yield_Bond_combined",
[2025-06-05T19:09:26.997+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T19:09:26.997+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T19:09:26.997+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.997+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.997+0200] {spark_submit.py:521} INFO - "name" : "TAUX_FED_combined",
[2025-06-05T19:09:26.997+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T19:09:26.997+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T19:09:26.997+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.998+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.998+0200] {spark_submit.py:521} INFO - "name" : "score_Q1",
[2025-06-05T19:09:26.998+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T19:09:26.998+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T19:09:26.998+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.998+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.998+0200] {spark_submit.py:521} INFO - "name" : "score_Q2",
[2025-06-05T19:09:26.998+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T19:09:26.998+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T19:09:26.998+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.998+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.999+0200] {spark_submit.py:521} INFO - "name" : "score_Q3",
[2025-06-05T19:09:26.999+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T19:09:26.999+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T19:09:26.999+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:26.999+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:26.999+0200] {spark_submit.py:521} INFO - "name" : "score_Q4",
[2025-06-05T19:09:26.999+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T19:09:26.999+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T19:09:27.000+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:27.000+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T19:09:27.000+0200] {spark_submit.py:521} INFO - "name" : "assigned_quadrant",
[2025-06-05T19:09:27.000+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T19:09:27.000+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T19:09:27.000+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T19:09:27.000+0200] {spark_submit.py:521} INFO - } ]
[2025-06-05T19:09:27.000+0200] {spark_submit.py:521} INFO - }
[2025-06-05T19:09:27.000+0200] {spark_submit.py:521} INFO - and corresponding Parquet message type:
[2025-06-05T19:09:27.000+0200] {spark_submit.py:521} INFO - message spark_schema {
[2025-06-05T19:09:27.000+0200] {spark_submit.py:521} INFO - optional int32 date (DATE);
[2025-06-05T19:09:27.001+0200] {spark_submit.py:521} INFO - optional double INFLATION;
[2025-06-05T19:09:27.001+0200] {spark_submit.py:521} INFO - optional double UNEMPLOYMENT;
[2025-06-05T19:09:27.001+0200] {spark_submit.py:521} INFO - optional double CONSUMER_SENTIMENT;
[2025-06-05T19:09:27.001+0200] {spark_submit.py:521} INFO - optional double High_Yield_Bond_SPREAD;
[2025-06-05T19:09:27.001+0200] {spark_submit.py:521} INFO - optional double 10-2Year_Treasury_Yield_Bond;
[2025-06-05T19:09:27.001+0200] {spark_submit.py:521} INFO - optional double TAUX_FED;
[2025-06-05T19:09:27.001+0200] {spark_submit.py:521} INFO - optional double INFLATION_delta;
[2025-06-05T19:09:27.001+0200] {spark_submit.py:521} INFO - optional double UNEMPLOYMENT_delta;
[2025-06-05T19:09:27.001+0200] {spark_submit.py:521} INFO - optional double CONSUMER_SENTIMENT_delta;
[2025-06-05T19:09:27.001+0200] {spark_submit.py:521} INFO - optional double High_Yield_Bond_SPREAD_delta;
[2025-06-05T19:09:27.001+0200] {spark_submit.py:521} INFO - optional double 10-2Year_Treasury_Yield_Bond_delta;
[2025-06-05T19:09:27.001+0200] {spark_submit.py:521} INFO - optional double TAUX_FED_delta;
[2025-06-05T19:09:27.001+0200] {spark_submit.py:521} INFO - optional double INFLATION_zscore;
[2025-06-05T19:09:27.001+0200] {spark_submit.py:521} INFO - optional double UNEMPLOYMENT_zscore;
[2025-06-05T19:09:27.002+0200] {spark_submit.py:521} INFO - optional double CONSUMER_SENTIMENT_zscore;
[2025-06-05T19:09:27.002+0200] {spark_submit.py:521} INFO - optional double High_Yield_Bond_SPREAD_zscore;
[2025-06-05T19:09:27.002+0200] {spark_submit.py:521} INFO - optional double 10-2Year_Treasury_Yield_Bond_zscore;
[2025-06-05T19:09:27.002+0200] {spark_submit.py:521} INFO - optional double TAUX_FED_zscore;
[2025-06-05T19:09:27.002+0200] {spark_submit.py:521} INFO - required int32 INFLATION_pos_score;
[2025-06-05T19:09:27.002+0200] {spark_submit.py:521} INFO - required int32 UNEMPLOYMENT_pos_score;
[2025-06-05T19:09:27.002+0200] {spark_submit.py:521} INFO - required int32 CONSUMER_SENTIMENT_pos_score;
[2025-06-05T19:09:27.002+0200] {spark_submit.py:521} INFO - required int32 High_Yield_Bond_SPREAD_pos_score;
[2025-06-05T19:09:27.002+0200] {spark_submit.py:521} INFO - required int32 10-2Year_Treasury_Yield_Bond_pos_score;
[2025-06-05T19:09:27.002+0200] {spark_submit.py:521} INFO - required int32 TAUX_FED_pos_score;
[2025-06-05T19:09:27.002+0200] {spark_submit.py:521} INFO - required int32 INFLATION_var_score;
[2025-06-05T19:09:27.002+0200] {spark_submit.py:521} INFO - required int32 UNEMPLOYMENT_var_score;
[2025-06-05T19:09:27.002+0200] {spark_submit.py:521} INFO - required int32 CONSUMER_SENTIMENT_var_score;
[2025-06-05T19:09:27.003+0200] {spark_submit.py:521} INFO - required int32 High_Yield_Bond_SPREAD_var_score;
[2025-06-05T19:09:27.003+0200] {spark_submit.py:521} INFO - required int32 10-2Year_Treasury_Yield_Bond_var_score;
[2025-06-05T19:09:27.003+0200] {spark_submit.py:521} INFO - required int32 TAUX_FED_var_score;
[2025-06-05T19:09:27.003+0200] {spark_submit.py:521} INFO - required int32 INFLATION_combined;
[2025-06-05T19:09:27.003+0200] {spark_submit.py:521} INFO - required int32 UNEMPLOYMENT_combined;
[2025-06-05T19:09:27.003+0200] {spark_submit.py:521} INFO - required int32 CONSUMER_SENTIMENT_combined;
[2025-06-05T19:09:27.003+0200] {spark_submit.py:521} INFO - required int32 High_Yield_Bond_SPREAD_combined;
[2025-06-05T19:09:27.003+0200] {spark_submit.py:521} INFO - required int32 10-2Year_Treasury_Yield_Bond_combined;
[2025-06-05T19:09:27.003+0200] {spark_submit.py:521} INFO - required int32 TAUX_FED_combined;
[2025-06-05T19:09:27.003+0200] {spark_submit.py:521} INFO - required int32 score_Q1;
[2025-06-05T19:09:27.003+0200] {spark_submit.py:521} INFO - required int32 score_Q2;
[2025-06-05T19:09:27.003+0200] {spark_submit.py:521} INFO - required int32 score_Q3;
[2025-06-05T19:09:27.003+0200] {spark_submit.py:521} INFO - required int32 score_Q4;
[2025-06-05T19:09:27.003+0200] {spark_submit.py:521} INFO - required int32 assigned_quadrant;
[2025-06-05T19:09:27.004+0200] {spark_submit.py:521} INFO - }
[2025-06-05T19:09:27.004+0200] {spark_submit.py:521} INFO - 
[2025-06-05T19:09:27.004+0200] {spark_submit.py:521} INFO - 
[2025-06-05T19:09:27.019+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:27 INFO CodecPool: Got brand-new compressor [.snappy]
[2025-06-05T19:09:27.444+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:27 INFO FileOutputCommitter: Saved output of task 'attempt_202506051909261595146163100781294_0095_m_000000_55' to file:/home/leoja/airflow/data/quadrants.parquet/_temporary/0/task_202506051909261595146163100781294_0095_m_000000
[2025-06-05T19:09:27.444+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:27 INFO SparkHadoopMapRedUtil: attempt_202506051909261595146163100781294_0095_m_000000_55: Committed. Elapsed time: 0 ms.
[2025-06-05T19:09:27.449+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:27 INFO Executor: Finished task 0.0 in stage 95.0 (TID 55). 8043 bytes result sent to driver
[2025-06-05T19:09:27.450+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:27 INFO TaskSetManager: Finished task 0.0 in stage 95.0 (TID 55) in 1192 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T19:09:27.450+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:27 INFO TaskSchedulerImpl: Removed TaskSet 95.0, whose tasks have all completed, from pool
[2025-06-05T19:09:27.451+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:27 INFO DAGScheduler: ResultStage 95 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.249 s
[2025-06-05T19:09:27.451+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:27 INFO DAGScheduler: Job 55 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T19:09:27.451+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 95: Stage finished
[2025-06-05T19:09:27.452+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:27 INFO DAGScheduler: Job 55 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.253268 s
[2025-06-05T19:09:27.453+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:27 INFO FileFormatWriter: Start to commit write Job 8e40b588-320b-43de-8a1d-3e83ce5ba38f.
[2025-06-05T19:09:27.469+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:27 INFO FileFormatWriter: Write Job 8e40b588-320b-43de-8a1d-3e83ce5ba38f committed. Elapsed time: 15 ms.
[2025-06-05T19:09:27.472+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:27 INFO FileFormatWriter: Finished processing stats for write job 8e40b588-320b-43de-8a1d-3e83ce5ba38f.
[2025-06-05T19:09:27.476+0200] {spark_submit.py:521} INFO - Écriture full refresh terminée dans /home/leoja/airflow/data/quadrants.parquet.
[2025-06-05T19:09:27.528+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:27 INFO SparkContext: Invoking stop() from shutdown hook
[2025-06-05T19:09:27.529+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:27 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-06-05T19:09:27.542+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:27 INFO SparkUI: Stopped Spark web UI at http://10.255.255.254:4040
[2025-06-05T19:09:27.572+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:27 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-06-05T19:09:27.593+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:27 INFO MemoryStore: MemoryStore cleared
[2025-06-05T19:09:27.594+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:27 INFO BlockManager: BlockManager stopped
[2025-06-05T19:09:27.597+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:27 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-06-05T19:09:27.600+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:27 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-06-05T19:09:27.627+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:27 INFO SparkContext: Successfully stopped SparkContext
[2025-06-05T19:09:27.627+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:27 INFO ShutdownHookManager: Shutdown hook called
[2025-06-05T19:09:27.627+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-0d5a3a5e-ead2-4bce-a141-66f74df3b1d4/pyspark-687ea3f9-8b20-4a90-82ee-fcdce8e7c60e
[2025-06-05T19:09:27.631+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-529136b6-d632-4a77-a4cd-1539013e12f1
[2025-06-05T19:09:27.635+0200] {spark_submit.py:521} INFO - 25/06/05 19:09:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-0d5a3a5e-ead2-4bce-a141-66f74df3b1d4
[2025-06-05T19:09:27.691+0200] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-06-05T19:09:27.696+0200] {taskinstance.py:1205} INFO - Marking task as SUCCESS. dag_id=macro_trading_dag, task_id=compute_economic_quadrants, execution_date=20250605T170847, start_date=20250605T170907, end_date=20250605T170927
[2025-06-05T19:09:27.818+0200] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2025-06-05T19:09:27.826+0200] {taskinstance.py:3482} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-06-05T19:09:27.827+0200] {local_task_job_runner.py:222} INFO - ::endgroup::
