[2025-06-05T22:04:48.885+0200] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-06-05T22:04:48.897+0200] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: macro_trading_dag.compute_economic_quadrants manual__2025-06-05T20:01:06.490277+00:00 [queued]>
[2025-06-05T22:04:48.901+0200] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: macro_trading_dag.compute_economic_quadrants manual__2025-06-05T20:01:06.490277+00:00 [queued]>
[2025-06-05T22:04:48.901+0200] {taskinstance.py:2303} INFO - Starting attempt 2 of 3
[2025-06-05T22:04:48.916+0200] {taskinstance.py:2327} INFO - Executing <Task(SparkSubmitOperator): compute_economic_quadrants> on 2025-06-05 20:01:06.490277+00:00
[2025-06-05T22:04:48.925+0200] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'macro_trading_dag', 'compute_economic_quadrants', 'manual__2025-06-05T20:01:06.490277+00:00', '--job-id', '295', '--raw', '--subdir', 'DAGS_FOLDER/macro_trading_dag.py', '--cfg-path', '/tmp/tmpmgy7gb9t']
[2025-06-05T22:04:48.926+0200] {standard_task_runner.py:91} INFO - Job 295: Subtask compute_economic_quadrants
[2025-06-05T22:04:48.931+0200] {logging_mixin.py:188} WARNING - /home/leoja/airflow/airflow_venv/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=587466) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-06-05T22:04:48.932+0200] {standard_task_runner.py:63} INFO - Started process 587479 to run task
[2025-06-05T22:04:48.966+0200] {task_command.py:426} INFO - Running <TaskInstance: macro_trading_dag.compute_economic_quadrants manual__2025-06-05T20:01:06.490277+00:00 [running]> on host N15I711-16GR512.localdomain
[2025-06-05T22:04:49.013+0200] {taskinstance.py:2644} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='macro_trading_dag' AIRFLOW_CTX_TASK_ID='compute_economic_quadrants' AIRFLOW_CTX_EXECUTION_DATE='2025-06-05T20:01:06.490277+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-06-05T20:01:06.490277+00:00'
[2025-06-05T22:04:49.014+0200] {taskinstance.py:430} INFO - ::endgroup::
[2025-06-05T22:04:49.017+0200] {base.py:84} INFO - Using connection ID 'spark_local' for task execution.
[2025-06-05T22:04:49.018+0200] {spark_submit.py:351} INFO - Spark-Submit cmd: spark-submit --master local[*] --conf spark.pyspark.python=/home/leoja/airflow_venv/bin/python --conf spark.pyspark.driver.python=/home/leoja/airflow_venv/bin/python --name compute_economic_quadrants /home/leoja/airflow/spark_jobs/compute_quadrants.py /home/leoja/airflow/data/Indicators.parquet /home/leoja/airflow/data/quadrants.parquet
[2025-06-05T22:04:50.948+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:50 WARN Utils: Your hostname, N15I711-16GR512 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
[2025-06-05T22:04:50.950+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2025-06-05T22:04:51.993+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:51 INFO SparkContext: Running Spark version 3.5.5
[2025-06-05T22:04:51.993+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:51 INFO SparkContext: OS info Linux, 6.6.87.1-microsoft-standard-WSL2, amd64
[2025-06-05T22:04:51.993+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:51 INFO SparkContext: Java version 11.0.27
[2025-06-05T22:04:52.059+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-06-05T22:04:52.138+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO ResourceUtils: ==============================================================
[2025-06-05T22:04:52.138+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-06-05T22:04:52.138+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO ResourceUtils: ==============================================================
[2025-06-05T22:04:52.138+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO SparkContext: Submitted application: ComputeEconomicQuadrantsSingleFile
[2025-06-05T22:04:52.156+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-06-05T22:04:52.169+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO ResourceProfile: Limiting resource is cpu
[2025-06-05T22:04:52.169+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-06-05T22:04:52.214+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO SecurityManager: Changing view acls to: leoja
[2025-06-05T22:04:52.214+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO SecurityManager: Changing modify acls to: leoja
[2025-06-05T22:04:52.214+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO SecurityManager: Changing view acls groups to:
[2025-06-05T22:04:52.215+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO SecurityManager: Changing modify acls groups to:
[2025-06-05T22:04:52.215+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: leoja; groups with view permissions: EMPTY; users with modify permissions: leoja; groups with modify permissions: EMPTY
[2025-06-05T22:04:52.458+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO Utils: Successfully started service 'sparkDriver' on port 46773.
[2025-06-05T22:04:52.491+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO SparkEnv: Registering MapOutputTracker
[2025-06-05T22:04:52.529+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO SparkEnv: Registering BlockManagerMaster
[2025-06-05T22:04:52.553+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-06-05T22:04:52.554+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-06-05T22:04:52.560+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-06-05T22:04:52.583+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e86c16e6-554e-47a1-b00b-22f5340e9a7a
[2025-06-05T22:04:52.601+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-06-05T22:04:52.621+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-06-05T22:04:52.751+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-06-05T22:04:52.804+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-06-05T22:04:52.910+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO Executor: Starting executor ID driver on host 10.255.255.254
[2025-06-05T22:04:52.910+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO Executor: OS info Linux, 6.6.87.1-microsoft-standard-WSL2, amd64
[2025-06-05T22:04:52.911+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO Executor: Java version 11.0.27
[2025-06-05T22:04:52.918+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-06-05T22:04:52.919+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@77fbf566 for default.
[2025-06-05T22:04:52.949+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45911.
[2025-06-05T22:04:52.950+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO NettyBlockTransferService: Server created on 10.255.255.254:45911
[2025-06-05T22:04:52.952+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-06-05T22:04:52.959+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.255.255.254, 45911, None)
[2025-06-05T22:04:52.963+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO BlockManagerMasterEndpoint: Registering block manager 10.255.255.254:45911 with 434.4 MiB RAM, BlockManagerId(driver, 10.255.255.254, 45911, None)
[2025-06-05T22:04:52.965+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.255.255.254, 45911, None)
[2025-06-05T22:04:52.967+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.255.255.254, 45911, None)
[2025-06-05T22:04:53.437+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:53 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-06-05T22:04:53.447+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:53 INFO SharedState: Warehouse path is 'file:/home/leoja/airflow/airflow_venv/spark-warehouse'.
[2025-06-05T22:04:54.170+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:54 INFO InMemoryFileIndex: It took 37 ms to list leaf files for 1 paths.
[2025-06-05T22:04:54.551+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:54 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-06-05T22:04:54.565+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:54 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-06-05T22:04:54.566+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:54 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
[2025-06-05T22:04:54.566+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:54 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T22:04:54.567+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:54 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:04:54.571+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:54 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-06-05T22:04:54.634+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 103.5 KiB, free 434.3 MiB)
[2025-06-05T22:04:54.662+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:54 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.3 MiB)
[2025-06-05T22:04:54.665+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:54 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.255.255.254:45911 (size: 37.3 KiB, free: 434.4 MiB)
[2025-06-05T22:04:54.668+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:54 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:04:54.683+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:04:54.684+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:54 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-06-05T22:04:54.735+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:54 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9142 bytes)
[2025-06-05T22:04:54.762+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:54 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-06-05T22:04:55.080+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:55 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1931 bytes result sent to driver
[2025-06-05T22:04:55.089+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:55 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 375 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:04:55.091+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:55 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-06-05T22:04:55.095+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:55 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.512 s
[2025-06-05T22:04:55.098+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:55 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T22:04:55.098+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2025-06-05T22:04:55.100+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:55 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.549527 s
[2025-06-05T22:04:55.418+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:55 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.255.255.254:45911 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-06-05T22:04:56.963+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:56 INFO FileSourceStrategy: Pushed Filters:
[2025-06-05T22:04:56.964+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:56 INFO FileSourceStrategy: Post-Scan Filters:
[2025-06-05T22:04:57.320+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:57 INFO CodeGenerator: Code generated in 164.021583 ms
[2025-06-05T22:04:57.344+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:57 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 201.4 KiB, free 434.2 MiB)
[2025-06-05T22:04:57.412+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:57 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 434.2 MiB)
[2025-06-05T22:04:57.412+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.255.255.254:45911 (size: 35.0 KiB, free: 434.4 MiB)
[2025-06-05T22:04:57.415+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:57 INFO SparkContext: Created broadcast 1 from first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178
[2025-06-05T22:04:57.433+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-06-05T22:04:57.573+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:57 INFO CodeGenerator: Code generated in 74.945394 ms
[2025-06-05T22:04:57.671+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:57 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178
[2025-06-05T22:04:57.672+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:57 INFO DAGScheduler: Got job 1 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) with 1 output partitions
[2025-06-05T22:04:57.673+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:57 INFO DAGScheduler: Final stage: ResultStage 1 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178)
[2025-06-05T22:04:57.673+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:57 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T22:04:57.673+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:57 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:04:57.675+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:57 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178), which has no missing parents
[2025-06-05T22:04:57.680+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:57 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 19.3 KiB, free 434.2 MiB)
[2025-06-05T22:04:57.714+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:57 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 434.1 MiB)
[2025-06-05T22:04:57.716+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.255.255.254:45911 (size: 8.6 KiB, free: 434.4 MiB)
[2025-06-05T22:04:57.717+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:57 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:04:57.718+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:04:57.718+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:57 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2025-06-05T22:04:57.724+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:57 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9612 bytes)
[2025-06-05T22:04:57.725+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:57 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2025-06-05T22:04:57.808+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:57 INFO CodeGenerator: Code generated in 23.839354 ms
[2025-06-05T22:04:57.829+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:57 INFO CodeGenerator: Code generated in 10.124091 ms
[2025-06-05T22:04:57.831+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:57 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T22:04:58.016+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO CodecPool: Got brand-new decompressor [.snappy]
[2025-06-05T22:04:58.250+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 10110 bytes result sent to driver
[2025-06-05T22:04:58.257+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 537 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:04:58.258+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2025-06-05T22:04:58.258+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: ResultStage 1 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) finished in 0.581 s
[2025-06-05T22:04:58.259+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T22:04:58.259+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2025-06-05T22:04:58.260+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: Job 1 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178, took 0.588682 s
[2025-06-05T22:04:58.301+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: Registering RDD 8 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) as input to shuffle 0
[2025-06-05T22:04:58.308+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: Got map stage job 2 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) with 1 output partitions
[2025-06-05T22:04:58.308+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178)
[2025-06-05T22:04:58.308+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T22:04:58.309+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:04:58.311+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[8] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178), which has no missing parents
[2025-06-05T22:04:58.329+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 27.6 KiB, free 434.1 MiB)
[2025-06-05T22:04:58.333+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 434.1 MiB)
[2025-06-05T22:04:58.333+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.255.255.254:45911 (size: 10.7 KiB, free: 434.3 MiB)
[2025-06-05T22:04:58.334+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:04:58.336+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[8] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:04:58.337+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2025-06-05T22:04:58.339+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9601 bytes)
[2025-06-05T22:04:58.340+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2025-06-05T22:04:58.370+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO CodeGenerator: Code generated in 8.270731 ms
[2025-06-05T22:04:58.391+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T22:04:58.521+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2242 bytes result sent to driver
[2025-06-05T22:04:58.525+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 187 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:04:58.525+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-06-05T22:04:58.528+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: ShuffleMapStage 2 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) finished in 0.213 s
[2025-06-05T22:04:58.528+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T22:04:58.529+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: running: Set()
[2025-06-05T22:04:58.529+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: waiting: Set()
[2025-06-05T22:04:58.530+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: failed: Set()
[2025-06-05T22:04:58.566+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-06-05T22:04:58.607+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO CodeGenerator: Code generated in 14.014144 ms
[2025-06-05T22:04:58.670+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.255.255.254:45911 in memory (size: 10.7 KiB, free: 434.4 MiB)
[2025-06-05T22:04:58.676+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: Registering RDD 12 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) as input to shuffle 1
[2025-06-05T22:04:58.676+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: Got map stage job 3 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) with 1 output partitions
[2025-06-05T22:04:58.676+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178)
[2025-06-05T22:04:58.677+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
[2025-06-05T22:04:58.678+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.255.255.254:45911 in memory (size: 8.6 KiB, free: 434.4 MiB)
[2025-06-05T22:04:58.678+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:04:58.679+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[12] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178), which has no missing parents
[2025-06-05T22:04:58.715+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 35.4 KiB, free 434.1 MiB)
[2025-06-05T22:04:58.717+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 17.0 KiB, free 434.1 MiB)
[2025-06-05T22:04:58.718+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.255.255.254:45911 (size: 17.0 KiB, free: 434.3 MiB)
[2025-06-05T22:04:58.719+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:04:58.720+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[12] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:04:58.720+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2025-06-05T22:04:58.728+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8988 bytes)
[2025-06-05T22:04:58.729+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
[2025-06-05T22:04:58.774+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO ShuffleBlockFetcherIterator: Getting 1 (13.5 KiB) non-empty blocks including 1 (13.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T22:04:58.776+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
[2025-06-05T22:04:58.795+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO CodeGenerator: Code generated in 14.474041 ms
[2025-06-05T22:04:58.817+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO CodeGenerator: Code generated in 7.742116 ms
[2025-06-05T22:04:58.875+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO CodeGenerator: Code generated in 5.946882 ms
[2025-06-05T22:04:58.887+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO CodeGenerator: Code generated in 5.419851 ms
[2025-06-05T22:04:58.898+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO CodeGenerator: Code generated in 6.399616 ms
[2025-06-05T22:04:58.927+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 5170 bytes result sent to driver
[2025-06-05T22:04:58.930+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 205 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:04:58.930+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2025-06-05T22:04:58.931+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: ShuffleMapStage 4 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) finished in 0.234 s
[2025-06-05T22:04:58.932+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T22:04:58.932+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: running: Set()
[2025-06-05T22:04:58.932+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: waiting: Set()
[2025-06-05T22:04:58.932+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: failed: Set()
[2025-06-05T22:04:58.968+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178
[2025-06-05T22:04:58.970+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: Got job 4 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) with 1 output partitions
[2025-06-05T22:04:58.971+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: Final stage: ResultStage 7 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178)
[2025-06-05T22:04:58.971+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
[2025-06-05T22:04:58.971+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:04:58.972+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[15] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178), which has no missing parents
[2025-06-05T22:04:58.975+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:58 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 34.9 KiB, free 434.1 MiB)
[2025-06-05T22:04:59.065+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 16.6 KiB, free 434.1 MiB)
[2025-06-05T22:04:59.066+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.255.255.254:45911 (size: 16.6 KiB, free: 434.3 MiB)
[2025-06-05T22:04:59.066+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:04:59.067+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:04:59.067+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2025-06-05T22:04:59.069+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 4) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8999 bytes)
[2025-06-05T22:04:59.070+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO Executor: Running task 0.0 in stage 7.0 (TID 4)
[2025-06-05T22:04:59.100+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO ShuffleBlockFetcherIterator: Getting 1 (2.2 KiB) non-empty blocks including 1 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T22:04:59.100+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-06-05T22:04:59.118+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO CodeGenerator: Code generated in 8.495457 ms
[2025-06-05T22:04:59.204+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO Executor: Finished task 0.0 in stage 7.0 (TID 4). 6480 bytes result sent to driver
[2025-06-05T22:04:59.206+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 4) in 138 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:04:59.206+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2025-06-05T22:04:59.207+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: ResultStage 7 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) finished in 0.235 s
[2025-06-05T22:04:59.208+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T22:04:59.208+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2025-06-05T22:04:59.208+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: Job 4 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178, took 0.240407 s
[2025-06-05T22:04:59.493+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO FileSourceStrategy: Pushed Filters:
[2025-06-05T22:04:59.493+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO FileSourceStrategy: Post-Scan Filters:
[2025-06-05T22:04:59.526+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.4 KiB, free 433.9 MiB)
[2025-06-05T22:04:59.565+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 433.8 MiB)
[2025-06-05T22:04:59.566+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.255.255.254:45911 (size: 35.0 KiB, free: 434.3 MiB)
[2025-06-05T22:04:59.567+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO SparkContext: Created broadcast 6 from first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179
[2025-06-05T22:04:59.569+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-06-05T22:04:59.592+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179
[2025-06-05T22:04:59.593+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: Got job 5 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) with 1 output partitions
[2025-06-05T22:04:59.594+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: Final stage: ResultStage 8 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179)
[2025-06-05T22:04:59.594+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T22:04:59.594+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:04:59.595+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[21] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179), which has no missing parents
[2025-06-05T22:04:59.598+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 19.3 KiB, free 433.8 MiB)
[2025-06-05T22:04:59.599+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 433.8 MiB)
[2025-06-05T22:04:59.599+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.255.255.254:45911 (size: 8.6 KiB, free: 434.3 MiB)
[2025-06-05T22:04:59.600+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:04:59.601+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[21] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:04:59.601+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2025-06-05T22:04:59.603+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 5) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9612 bytes)
[2025-06-05T22:04:59.604+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO Executor: Running task 0.0 in stage 8.0 (TID 5)
[2025-06-05T22:04:59.611+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T22:04:59.627+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO Executor: Finished task 0.0 in stage 8.0 (TID 5). 10024 bytes result sent to driver
[2025-06-05T22:04:59.628+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 5) in 25 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:04:59.629+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2025-06-05T22:04:59.630+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: ResultStage 8 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) finished in 0.033 s
[2025-06-05T22:04:59.630+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T22:04:59.630+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2025-06-05T22:04:59.631+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: Job 5 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179, took 0.039107 s
[2025-06-05T22:04:59.636+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: Registering RDD 22 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) as input to shuffle 2
[2025-06-05T22:04:59.636+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: Got map stage job 6 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) with 1 output partitions
[2025-06-05T22:04:59.636+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179)
[2025-06-05T22:04:59.636+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T22:04:59.637+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:04:59.638+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[22] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179), which has no missing parents
[2025-06-05T22:04:59.642+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 27.6 KiB, free 433.8 MiB)
[2025-06-05T22:04:59.669+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 433.8 MiB)
[2025-06-05T22:04:59.675+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.255.255.254:45911 (size: 10.7 KiB, free: 434.3 MiB)
[2025-06-05T22:04:59.675+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:04:59.675+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[22] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:04:59.676+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2025-06-05T22:04:59.676+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 6) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9601 bytes)
[2025-06-05T22:04:59.677+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 10.255.255.254:45911 in memory (size: 8.6 KiB, free: 434.3 MiB)
[2025-06-05T22:04:59.677+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO Executor: Running task 0.0 in stage 9.0 (TID 6)
[2025-06-05T22:04:59.691+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T22:04:59.692+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.255.255.254:45911 in memory (size: 16.6 KiB, free: 434.3 MiB)
[2025-06-05T22:04:59.714+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.255.255.254:45911 in memory (size: 17.0 KiB, free: 434.3 MiB)
[2025-06-05T22:04:59.771+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO Executor: Finished task 0.0 in stage 9.0 (TID 6). 2199 bytes result sent to driver
[2025-06-05T22:04:59.772+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 6) in 97 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:04:59.772+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2025-06-05T22:04:59.774+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: ShuffleMapStage 9 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) finished in 0.134 s
[2025-06-05T22:04:59.774+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T22:04:59.774+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: running: Set()
[2025-06-05T22:04:59.774+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: waiting: Set()
[2025-06-05T22:04:59.775+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: failed: Set()
[2025-06-05T22:04:59.782+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-06-05T22:04:59.844+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO CodeGenerator: Code generated in 23.895776 ms
[2025-06-05T22:04:59.868+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: Registering RDD 25 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) as input to shuffle 3
[2025-06-05T22:04:59.869+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: Got map stage job 7 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) with 1 output partitions
[2025-06-05T22:04:59.869+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: Final stage: ShuffleMapStage 11 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179)
[2025-06-05T22:04:59.869+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
[2025-06-05T22:04:59.870+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:04:59.871+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[25] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179), which has no missing parents
[2025-06-05T22:04:59.878+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 45.0 KiB, free 433.9 MiB)
[2025-06-05T22:04:59.879+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 433.8 MiB)
[2025-06-05T22:04:59.879+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.255.255.254:45911 (size: 20.4 KiB, free: 434.3 MiB)
[2025-06-05T22:04:59.880+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:04:59.881+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[25] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:04:59.881+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2025-06-05T22:04:59.882+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 7) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8988 bytes)
[2025-06-05T22:04:59.883+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO Executor: Running task 0.0 in stage 11.0 (TID 7)
[2025-06-05T22:04:59.894+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO ShuffleBlockFetcherIterator: Getting 1 (13.5 KiB) non-empty blocks including 1 (13.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T22:04:59.895+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-06-05T22:04:59.911+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO CodeGenerator: Code generated in 16.140399 ms
[2025-06-05T22:04:59.922+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO Executor: Finished task 0.0 in stage 11.0 (TID 7). 5314 bytes result sent to driver
[2025-06-05T22:04:59.924+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 7) in 42 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:04:59.925+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2025-06-05T22:04:59.926+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: ShuffleMapStage 11 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) finished in 0.050 s
[2025-06-05T22:04:59.926+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T22:04:59.926+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: running: Set()
[2025-06-05T22:04:59.926+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: waiting: Set()
[2025-06-05T22:04:59.926+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO DAGScheduler: failed: Set()
[2025-06-05T22:04:59.987+0200] {spark_submit.py:521} INFO - 25/06/05 22:04:59 INFO CodeGenerator: Code generated in 18.946232 ms
[2025-06-05T22:05:00.003+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179
[2025-06-05T22:05:00.004+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Got job 8 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) with 1 output partitions
[2025-06-05T22:05:00.005+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Final stage: ResultStage 14 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179)
[2025-06-05T22:05:00.005+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)
[2025-06-05T22:05:00.005+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:00.006+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[28] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179), which has no missing parents
[2025-06-05T22:05:00.008+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 16.9 KiB, free 433.8 MiB)
[2025-06-05T22:05:00.036+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 433.8 MiB)
[2025-06-05T22:05:00.037+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.255.255.254:45911 (size: 7.2 KiB, free: 434.3 MiB)
[2025-06-05T22:05:00.038+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:00.038+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[28] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:00.038+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
[2025-06-05T22:05:00.040+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 8) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8999 bytes)
[2025-06-05T22:05:00.041+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO Executor: Running task 0.0 in stage 14.0 (TID 8)
[2025-06-05T22:05:00.047+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T22:05:00.047+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-06-05T22:05:00.065+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO CodeGenerator: Code generated in 16.711507 ms
[2025-06-05T22:05:00.069+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO Executor: Finished task 0.0 in stage 14.0 (TID 8). 3995 bytes result sent to driver
[2025-06-05T22:05:00.071+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 8) in 31 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:00.071+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
[2025-06-05T22:05:00.072+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: ResultStage 14 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) finished in 0.065 s
[2025-06-05T22:05:00.072+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T22:05:00.072+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
[2025-06-05T22:05:00.073+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Job 8 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179, took 0.069898 s
[2025-06-05T22:05:00.180+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO FileSourceStrategy: Pushed Filters:
[2025-06-05T22:05:00.180+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO FileSourceStrategy: Post-Scan Filters:
[2025-06-05T22:05:00.198+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 201.4 KiB, free 433.6 MiB)
[2025-06-05T22:05:00.205+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 433.6 MiB)
[2025-06-05T22:05:00.205+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.255.255.254:45911 (size: 35.0 KiB, free: 434.3 MiB)
[2025-06-05T22:05:00.206+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO SparkContext: Created broadcast 11 from first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178
[2025-06-05T22:05:00.208+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-06-05T22:05:00.233+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178
[2025-06-05T22:05:00.234+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Got job 9 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) with 1 output partitions
[2025-06-05T22:05:00.236+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Final stage: ResultStage 15 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178)
[2025-06-05T22:05:00.236+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T22:05:00.236+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:00.237+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[34] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178), which has no missing parents
[2025-06-05T22:05:00.240+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 19.3 KiB, free 433.6 MiB)
[2025-06-05T22:05:00.255+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 433.6 MiB)
[2025-06-05T22:05:00.257+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.255.255.254:45911 (size: 8.7 KiB, free: 434.3 MiB)
[2025-06-05T22:05:00.257+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:00.259+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[34] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:00.260+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
[2025-06-05T22:05:00.262+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 9) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9612 bytes)
[2025-06-05T22:05:00.263+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 10.255.255.254:45911 in memory (size: 10.7 KiB, free: 434.3 MiB)
[2025-06-05T22:05:00.264+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO Executor: Running task 0.0 in stage 15.0 (TID 9)
[2025-06-05T22:05:00.274+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T22:05:00.279+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 10.255.255.254:45911 in memory (size: 20.4 KiB, free: 434.3 MiB)
[2025-06-05T22:05:00.290+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO Executor: Finished task 0.0 in stage 15.0 (TID 9). 10024 bytes result sent to driver
[2025-06-05T22:05:00.292+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 10.255.255.254:45911 in memory (size: 7.2 KiB, free: 434.3 MiB)
[2025-06-05T22:05:00.302+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 9) in 40 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:00.302+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
[2025-06-05T22:05:00.303+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: ResultStage 15 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) finished in 0.065 s
[2025-06-05T22:05:00.303+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T22:05:00.303+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
[2025-06-05T22:05:00.304+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Job 9 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178, took 0.071099 s
[2025-06-05T22:05:00.308+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Registering RDD 35 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) as input to shuffle 4
[2025-06-05T22:05:00.308+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Got map stage job 10 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) with 1 output partitions
[2025-06-05T22:05:00.309+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Final stage: ShuffleMapStage 16 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178)
[2025-06-05T22:05:00.309+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T22:05:00.312+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:00.314+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[35] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178), which has no missing parents
[2025-06-05T22:05:00.319+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 27.6 KiB, free 433.7 MiB)
[2025-06-05T22:05:00.320+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 433.6 MiB)
[2025-06-05T22:05:00.321+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.255.255.254:45911 (size: 10.7 KiB, free: 434.3 MiB)
[2025-06-05T22:05:00.322+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:00.322+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[35] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:00.323+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
[2025-06-05T22:05:00.324+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 10) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9601 bytes)
[2025-06-05T22:05:00.328+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO Executor: Running task 0.0 in stage 16.0 (TID 10)
[2025-06-05T22:05:00.336+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T22:05:00.401+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO Executor: Finished task 0.0 in stage 16.0 (TID 10). 2199 bytes result sent to driver
[2025-06-05T22:05:00.402+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 10) in 78 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:00.402+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool
[2025-06-05T22:05:00.403+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: ShuffleMapStage 16 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) finished in 0.089 s
[2025-06-05T22:05:00.403+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T22:05:00.403+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: running: Set()
[2025-06-05T22:05:00.403+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: waiting: Set()
[2025-06-05T22:05:00.403+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: failed: Set()
[2025-06-05T22:05:00.411+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-06-05T22:05:00.428+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Registering RDD 39 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) as input to shuffle 5
[2025-06-05T22:05:00.429+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Got map stage job 11 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) with 1 output partitions
[2025-06-05T22:05:00.429+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Final stage: ShuffleMapStage 18 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178)
[2025-06-05T22:05:00.429+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)
[2025-06-05T22:05:00.430+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:00.430+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[39] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178), which has no missing parents
[2025-06-05T22:05:00.440+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 35.4 KiB, free 433.6 MiB)
[2025-06-05T22:05:00.441+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 17.1 KiB, free 433.6 MiB)
[2025-06-05T22:05:00.442+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.255.255.254:45911 (size: 17.1 KiB, free: 434.3 MiB)
[2025-06-05T22:05:00.443+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:00.443+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[39] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:00.444+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
[2025-06-05T22:05:00.445+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 11) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8988 bytes)
[2025-06-05T22:05:00.446+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO Executor: Running task 0.0 in stage 18.0 (TID 11)
[2025-06-05T22:05:00.453+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO ShuffleBlockFetcherIterator: Getting 1 (13.5 KiB) non-empty blocks including 1 (13.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T22:05:00.454+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-06-05T22:05:00.473+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO Executor: Finished task 0.0 in stage 18.0 (TID 11). 5170 bytes result sent to driver
[2025-06-05T22:05:00.474+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 11) in 29 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:00.474+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
[2025-06-05T22:05:00.475+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: ShuffleMapStage 18 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) finished in 0.040 s
[2025-06-05T22:05:00.475+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T22:05:00.475+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: running: Set()
[2025-06-05T22:05:00.476+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: waiting: Set()
[2025-06-05T22:05:00.476+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: failed: Set()
[2025-06-05T22:05:00.497+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178
[2025-06-05T22:05:00.498+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Got job 12 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) with 1 output partitions
[2025-06-05T22:05:00.498+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Final stage: ResultStage 21 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178)
[2025-06-05T22:05:00.498+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)
[2025-06-05T22:05:00.499+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:00.500+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[42] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178), which has no missing parents
[2025-06-05T22:05:00.503+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 35.0 KiB, free 433.6 MiB)
[2025-06-05T22:05:00.504+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 16.6 KiB, free 433.5 MiB)
[2025-06-05T22:05:00.504+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.255.255.254:45911 (size: 16.6 KiB, free: 434.2 MiB)
[2025-06-05T22:05:00.505+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:00.506+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[42] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:00.506+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
[2025-06-05T22:05:00.508+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 12) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8999 bytes)
[2025-06-05T22:05:00.509+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO Executor: Running task 0.0 in stage 21.0 (TID 12)
[2025-06-05T22:05:00.515+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO ShuffleBlockFetcherIterator: Getting 1 (717.0 B) non-empty blocks including 1 (717.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T22:05:00.515+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T22:05:00.535+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO Executor: Finished task 0.0 in stage 21.0 (TID 12). 6474 bytes result sent to driver
[2025-06-05T22:05:00.539+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 12) in 31 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:00.539+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
[2025-06-05T22:05:00.540+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 10.255.255.254:45911 in memory (size: 8.7 KiB, free: 434.3 MiB)
[2025-06-05T22:05:00.540+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: ResultStage 21 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) finished in 0.039 s
[2025-06-05T22:05:00.540+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T22:05:00.540+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
[2025-06-05T22:05:00.540+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Job 12 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178, took 0.043480 s
[2025-06-05T22:05:00.588+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 10.255.255.254:45911 in memory (size: 10.7 KiB, free: 434.3 MiB)
[2025-06-05T22:05:00.613+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 10.255.255.254:45911 in memory (size: 17.1 KiB, free: 434.3 MiB)
[2025-06-05T22:05:00.666+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO FileSourceStrategy: Pushed Filters:
[2025-06-05T22:05:00.667+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO FileSourceStrategy: Post-Scan Filters:
[2025-06-05T22:05:00.684+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 201.4 KiB, free 433.5 MiB)
[2025-06-05T22:05:00.689+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 433.4 MiB)
[2025-06-05T22:05:00.690+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.255.255.254:45911 (size: 35.0 KiB, free: 434.2 MiB)
[2025-06-05T22:05:00.691+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO SparkContext: Created broadcast 16 from first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179
[2025-06-05T22:05:00.692+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-06-05T22:05:00.705+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179
[2025-06-05T22:05:00.707+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Got job 13 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) with 1 output partitions
[2025-06-05T22:05:00.707+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Final stage: ResultStage 22 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179)
[2025-06-05T22:05:00.707+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T22:05:00.707+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:00.711+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[48] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179), which has no missing parents
[2025-06-05T22:05:00.712+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 19.3 KiB, free 433.4 MiB)
[2025-06-05T22:05:00.718+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 433.4 MiB)
[2025-06-05T22:05:00.719+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.255.255.254:45911 (size: 8.7 KiB, free: 434.2 MiB)
[2025-06-05T22:05:00.719+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:00.720+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[48] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:00.720+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
[2025-06-05T22:05:00.722+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 13) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9612 bytes)
[2025-06-05T22:05:00.723+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO Executor: Running task 0.0 in stage 22.0 (TID 13)
[2025-06-05T22:05:00.729+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T22:05:00.739+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO Executor: Finished task 0.0 in stage 22.0 (TID 13). 10024 bytes result sent to driver
[2025-06-05T22:05:00.740+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 13) in 19 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:00.741+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool
[2025-06-05T22:05:00.742+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: ResultStage 22 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) finished in 0.032 s
[2025-06-05T22:05:00.742+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T22:05:00.742+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
[2025-06-05T22:05:00.742+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Job 13 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179, took 0.036632 s
[2025-06-05T22:05:00.746+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Registering RDD 49 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) as input to shuffle 6
[2025-06-05T22:05:00.746+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Got map stage job 14 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) with 1 output partitions
[2025-06-05T22:05:00.746+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Final stage: ShuffleMapStage 23 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179)
[2025-06-05T22:05:00.746+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T22:05:00.747+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:00.747+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Submitting ShuffleMapStage 23 (MapPartitionsRDD[49] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179), which has no missing parents
[2025-06-05T22:05:00.751+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 27.6 KiB, free 433.4 MiB)
[2025-06-05T22:05:00.752+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 433.4 MiB)
[2025-06-05T22:05:00.753+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.255.255.254:45911 (size: 10.7 KiB, free: 434.2 MiB)
[2025-06-05T22:05:00.753+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:00.754+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[49] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:00.754+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
[2025-06-05T22:05:00.756+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 14) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9601 bytes)
[2025-06-05T22:05:00.756+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO Executor: Running task 0.0 in stage 23.0 (TID 14)
[2025-06-05T22:05:00.764+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T22:05:00.830+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO Executor: Finished task 0.0 in stage 23.0 (TID 14). 2199 bytes result sent to driver
[2025-06-05T22:05:00.831+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 14) in 76 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:00.832+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool
[2025-06-05T22:05:00.832+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: ShuffleMapStage 23 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) finished in 0.084 s
[2025-06-05T22:05:00.832+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T22:05:00.832+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: running: Set()
[2025-06-05T22:05:00.833+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: waiting: Set()
[2025-06-05T22:05:00.833+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: failed: Set()
[2025-06-05T22:05:00.842+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO ShufflePartitionsUtil: For shuffle(6), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-06-05T22:05:00.863+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Registering RDD 52 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) as input to shuffle 7
[2025-06-05T22:05:00.864+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Got map stage job 15 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) with 1 output partitions
[2025-06-05T22:05:00.864+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Final stage: ShuffleMapStage 25 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179)
[2025-06-05T22:05:00.865+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 24)
[2025-06-05T22:05:00.865+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:00.865+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Submitting ShuffleMapStage 25 (MapPartitionsRDD[52] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179), which has no missing parents
[2025-06-05T22:05:00.872+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 45.0 KiB, free 433.3 MiB)
[2025-06-05T22:05:00.892+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 20.5 KiB, free 433.3 MiB)
[2025-06-05T22:05:00.894+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.255.255.254:45911 (size: 20.5 KiB, free: 434.2 MiB)
[2025-06-05T22:05:00.894+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:00.896+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 10.255.255.254:45911 in memory (size: 10.7 KiB, free: 434.2 MiB)
[2025-06-05T22:05:00.896+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 25 (MapPartitionsRDD[52] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:00.897+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
[2025-06-05T22:05:00.898+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 15) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8988 bytes)
[2025-06-05T22:05:00.899+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO Executor: Running task 0.0 in stage 25.0 (TID 15)
[2025-06-05T22:05:00.911+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO ShuffleBlockFetcherIterator: Getting 1 (13.5 KiB) non-empty blocks including 1 (13.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T22:05:00.912+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-06-05T22:05:00.922+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO Executor: Finished task 0.0 in stage 25.0 (TID 15). 5314 bytes result sent to driver
[2025-06-05T22:05:00.924+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 10.255.255.254:45911 in memory (size: 16.6 KiB, free: 434.2 MiB)
[2025-06-05T22:05:00.924+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 15) in 27 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:00.925+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool
[2025-06-05T22:05:00.925+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: ShuffleMapStage 25 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) finished in 0.056 s
[2025-06-05T22:05:00.925+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T22:05:00.925+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: running: Set()
[2025-06-05T22:05:00.925+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: waiting: Set()
[2025-06-05T22:05:00.926+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: failed: Set()
[2025-06-05T22:05:00.964+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 10.255.255.254:45911 in memory (size: 8.7 KiB, free: 434.2 MiB)
[2025-06-05T22:05:00.971+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179
[2025-06-05T22:05:00.972+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Got job 16 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) with 1 output partitions
[2025-06-05T22:05:00.973+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Final stage: ResultStage 28 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179)
[2025-06-05T22:05:00.973+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)
[2025-06-05T22:05:00.973+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:00.973+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[55] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179), which has no missing parents
[2025-06-05T22:05:00.977+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 16.9 KiB, free 433.4 MiB)
[2025-06-05T22:05:00.977+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 433.4 MiB)
[2025-06-05T22:05:00.977+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.255.255.254:45911 (size: 7.2 KiB, free: 434.2 MiB)
[2025-06-05T22:05:00.978+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:00.978+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[55] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:00.979+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0
[2025-06-05T22:05:00.980+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 16) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8999 bytes)
[2025-06-05T22:05:00.981+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO Executor: Running task 0.0 in stage 28.0 (TID 16)
[2025-06-05T22:05:00.986+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T22:05:00.987+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T22:05:00.988+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO Executor: Finished task 0.0 in stage 28.0 (TID 16). 3995 bytes result sent to driver
[2025-06-05T22:05:00.989+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 16) in 10 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:00.990+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool
[2025-06-05T22:05:00.990+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: ResultStage 28 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) finished in 0.016 s
[2025-06-05T22:05:00.990+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T22:05:00.991+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished
[2025-06-05T22:05:00.991+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:00 INFO DAGScheduler: Job 16 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179, took 0.020075 s
[2025-06-05T22:05:01.086+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO FileSourceStrategy: Pushed Filters:
[2025-06-05T22:05:01.087+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO FileSourceStrategy: Post-Scan Filters:
[2025-06-05T22:05:01.109+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 201.4 KiB, free 433.2 MiB)
[2025-06-05T22:05:01.117+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 433.2 MiB)
[2025-06-05T22:05:01.117+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.255.255.254:45911 (size: 35.0 KiB, free: 434.2 MiB)
[2025-06-05T22:05:01.118+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO SparkContext: Created broadcast 21 from first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178
[2025-06-05T22:05:01.119+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-06-05T22:05:01.131+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178
[2025-06-05T22:05:01.132+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Got job 17 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) with 1 output partitions
[2025-06-05T22:05:01.132+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Final stage: ResultStage 29 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178)
[2025-06-05T22:05:01.133+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T22:05:01.133+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:01.133+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[61] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178), which has no missing parents
[2025-06-05T22:05:01.136+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 19.3 KiB, free 433.1 MiB)
[2025-06-05T22:05:01.137+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 433.1 MiB)
[2025-06-05T22:05:01.137+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.255.255.254:45911 (size: 8.7 KiB, free: 434.2 MiB)
[2025-06-05T22:05:01.138+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:01.138+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[61] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:01.138+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0
[2025-06-05T22:05:01.139+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 17) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9612 bytes)
[2025-06-05T22:05:01.140+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO Executor: Running task 0.0 in stage 29.0 (TID 17)
[2025-06-05T22:05:01.147+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T22:05:01.170+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO Executor: Finished task 0.0 in stage 29.0 (TID 17). 10110 bytes result sent to driver
[2025-06-05T22:05:01.172+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 17) in 32 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:01.172+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool
[2025-06-05T22:05:01.173+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: ResultStage 29 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) finished in 0.038 s
[2025-06-05T22:05:01.174+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T22:05:01.174+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished
[2025-06-05T22:05:01.174+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Job 17 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178, took 0.042315 s
[2025-06-05T22:05:01.177+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Registering RDD 62 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) as input to shuffle 8
[2025-06-05T22:05:01.177+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Got map stage job 18 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) with 1 output partitions
[2025-06-05T22:05:01.177+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Final stage: ShuffleMapStage 30 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178)
[2025-06-05T22:05:01.177+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T22:05:01.178+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:01.185+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Submitting ShuffleMapStage 30 (MapPartitionsRDD[62] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178), which has no missing parents
[2025-06-05T22:05:01.191+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 10.255.255.254:45911 in memory (size: 35.0 KiB, free: 434.2 MiB)
[2025-06-05T22:05:01.192+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 27.6 KiB, free 433.3 MiB)
[2025-06-05T22:05:01.192+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 433.3 MiB)
[2025-06-05T22:05:01.193+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.255.255.254:45911 (size: 10.7 KiB, free: 434.2 MiB)
[2025-06-05T22:05:01.198+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:01.198+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 30 (MapPartitionsRDD[62] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:01.199+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
[2025-06-05T22:05:01.199+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 18) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9601 bytes)
[2025-06-05T22:05:01.199+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO Executor: Running task 0.0 in stage 30.0 (TID 18)
[2025-06-05T22:05:01.208+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T22:05:01.230+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 10.255.255.254:45911 in memory (size: 7.2 KiB, free: 434.2 MiB)
[2025-06-05T22:05:01.242+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 10.255.255.254:45911 in memory (size: 20.5 KiB, free: 434.2 MiB)
[2025-06-05T22:05:01.273+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO Executor: Finished task 0.0 in stage 30.0 (TID 18). 2199 bytes result sent to driver
[2025-06-05T22:05:01.274+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 18) in 77 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:01.274+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool
[2025-06-05T22:05:01.275+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: ShuffleMapStage 30 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) finished in 0.090 s
[2025-06-05T22:05:01.275+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T22:05:01.275+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: running: Set()
[2025-06-05T22:05:01.275+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: waiting: Set()
[2025-06-05T22:05:01.275+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: failed: Set()
[2025-06-05T22:05:01.283+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO ShufflePartitionsUtil: For shuffle(8), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-06-05T22:05:01.312+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Registering RDD 66 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) as input to shuffle 9
[2025-06-05T22:05:01.312+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Got map stage job 19 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) with 1 output partitions
[2025-06-05T22:05:01.312+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Final stage: ShuffleMapStage 32 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178)
[2025-06-05T22:05:01.313+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 31)
[2025-06-05T22:05:01.313+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:01.313+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Submitting ShuffleMapStage 32 (MapPartitionsRDD[66] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178), which has no missing parents
[2025-06-05T22:05:01.321+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 35.4 KiB, free 433.4 MiB)
[2025-06-05T22:05:01.322+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 17.1 KiB, free 433.4 MiB)
[2025-06-05T22:05:01.322+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.255.255.254:45911 (size: 17.1 KiB, free: 434.2 MiB)
[2025-06-05T22:05:01.323+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:01.327+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 32 (MapPartitionsRDD[66] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:01.328+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0
[2025-06-05T22:05:01.329+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 19) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8988 bytes)
[2025-06-05T22:05:01.330+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO Executor: Running task 0.0 in stage 32.0 (TID 19)
[2025-06-05T22:05:01.335+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO ShuffleBlockFetcherIterator: Getting 1 (13.5 KiB) non-empty blocks including 1 (13.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T22:05:01.336+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T22:05:01.346+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO Executor: Finished task 0.0 in stage 32.0 (TID 19). 5170 bytes result sent to driver
[2025-06-05T22:05:01.349+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 19) in 20 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:01.350+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool
[2025-06-05T22:05:01.350+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: ShuffleMapStage 32 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) finished in 0.034 s
[2025-06-05T22:05:01.350+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T22:05:01.350+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: running: Set()
[2025-06-05T22:05:01.351+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: waiting: Set()
[2025-06-05T22:05:01.351+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: failed: Set()
[2025-06-05T22:05:01.370+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178
[2025-06-05T22:05:01.371+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Got job 20 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) with 1 output partitions
[2025-06-05T22:05:01.371+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Final stage: ResultStage 35 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178)
[2025-06-05T22:05:01.371+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)
[2025-06-05T22:05:01.372+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:01.372+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[69] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178), which has no missing parents
[2025-06-05T22:05:01.374+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 35.0 KiB, free 433.3 MiB)
[2025-06-05T22:05:01.375+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 16.6 KiB, free 433.3 MiB)
[2025-06-05T22:05:01.375+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.255.255.254:45911 (size: 16.6 KiB, free: 434.2 MiB)
[2025-06-05T22:05:01.376+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:01.376+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[69] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:01.377+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0
[2025-06-05T22:05:01.378+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 20) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8999 bytes)
[2025-06-05T22:05:01.378+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO Executor: Running task 0.0 in stage 35.0 (TID 20)
[2025-06-05T22:05:01.384+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO ShuffleBlockFetcherIterator: Getting 1 (1399.0 B) non-empty blocks including 1 (1399.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T22:05:01.385+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T22:05:01.391+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO Executor: Finished task 0.0 in stage 35.0 (TID 20). 6437 bytes result sent to driver
[2025-06-05T22:05:01.392+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 20) in 15 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:01.392+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool
[2025-06-05T22:05:01.393+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: ResultStage 35 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) finished in 0.020 s
[2025-06-05T22:05:01.394+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T22:05:01.394+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
[2025-06-05T22:05:01.394+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Job 20 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178, took 0.024458 s
[2025-06-05T22:05:01.477+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO FileSourceStrategy: Pushed Filters:
[2025-06-05T22:05:01.478+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO FileSourceStrategy: Post-Scan Filters:
[2025-06-05T22:05:01.495+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 201.4 KiB, free 433.1 MiB)
[2025-06-05T22:05:01.517+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 433.1 MiB)
[2025-06-05T22:05:01.518+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.255.255.254:45911 (size: 35.0 KiB, free: 434.2 MiB)
[2025-06-05T22:05:01.519+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO SparkContext: Created broadcast 26 from first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179
[2025-06-05T22:05:01.520+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 10.255.255.254:45911 in memory (size: 10.7 KiB, free: 434.2 MiB)
[2025-06-05T22:05:01.520+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-06-05T22:05:01.543+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179
[2025-06-05T22:05:01.543+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 10.255.255.254:45911 in memory (size: 16.6 KiB, free: 434.2 MiB)
[2025-06-05T22:05:01.543+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Got job 21 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) with 1 output partitions
[2025-06-05T22:05:01.543+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Final stage: ResultStage 36 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179)
[2025-06-05T22:05:01.544+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T22:05:01.544+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:01.547+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Submitting ResultStage 36 (MapPartitionsRDD[75] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179), which has no missing parents
[2025-06-05T22:05:01.548+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 19.3 KiB, free 433.1 MiB)
[2025-06-05T22:05:01.548+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 433.1 MiB)
[2025-06-05T22:05:01.549+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.255.255.254:45911 (size: 8.7 KiB, free: 434.2 MiB)
[2025-06-05T22:05:01.549+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:01.550+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (MapPartitionsRDD[75] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:01.550+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks resource profile 0
[2025-06-05T22:05:01.562+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 21) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9612 bytes)
[2025-06-05T22:05:01.565+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO Executor: Running task 0.0 in stage 36.0 (TID 21)
[2025-06-05T22:05:01.566+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.255.255.254:45911 in memory (size: 35.0 KiB, free: 434.2 MiB)
[2025-06-05T22:05:01.573+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T22:05:01.585+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO Executor: Finished task 0.0 in stage 36.0 (TID 21). 10024 bytes result sent to driver
[2025-06-05T22:05:01.586+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 21) in 24 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:01.586+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool
[2025-06-05T22:05:01.586+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: ResultStage 36 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) finished in 0.041 s
[2025-06-05T22:05:01.587+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T22:05:01.587+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 36: Stage finished
[2025-06-05T22:05:01.587+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Job 21 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179, took 0.045649 s
[2025-06-05T22:05:01.589+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Registering RDD 76 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) as input to shuffle 10
[2025-06-05T22:05:01.589+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Got map stage job 22 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) with 1 output partitions
[2025-06-05T22:05:01.589+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Final stage: ShuffleMapStage 37 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179)
[2025-06-05T22:05:01.590+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T22:05:01.590+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:01.596+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Submitting ShuffleMapStage 37 (MapPartitionsRDD[76] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179), which has no missing parents
[2025-06-05T22:05:01.596+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 10.255.255.254:45911 in memory (size: 8.7 KiB, free: 434.2 MiB)
[2025-06-05T22:05:01.616+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 27.6 KiB, free 433.4 MiB)
[2025-06-05T22:05:01.618+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 433.4 MiB)
[2025-06-05T22:05:01.618+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.255.255.254:45911 (size: 10.7 KiB, free: 434.2 MiB)
[2025-06-05T22:05:01.619+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:01.619+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 37 (MapPartitionsRDD[76] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:01.619+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks resource profile 0
[2025-06-05T22:05:01.620+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 22) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9601 bytes)
[2025-06-05T22:05:01.621+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO Executor: Running task 0.0 in stage 37.0 (TID 22)
[2025-06-05T22:05:01.628+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T22:05:01.636+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 10.255.255.254:45911 in memory (size: 17.1 KiB, free: 434.2 MiB)
[2025-06-05T22:05:01.671+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO Executor: Finished task 0.0 in stage 37.0 (TID 22). 2199 bytes result sent to driver
[2025-06-05T22:05:01.672+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 22) in 51 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:01.672+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool
[2025-06-05T22:05:01.673+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: ShuffleMapStage 37 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) finished in 0.076 s
[2025-06-05T22:05:01.673+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T22:05:01.674+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: running: Set()
[2025-06-05T22:05:01.674+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: waiting: Set()
[2025-06-05T22:05:01.674+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: failed: Set()
[2025-06-05T22:05:01.681+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO ShufflePartitionsUtil: For shuffle(10), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-06-05T22:05:01.704+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Registering RDD 79 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) as input to shuffle 11
[2025-06-05T22:05:01.704+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Got map stage job 23 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) with 1 output partitions
[2025-06-05T22:05:01.704+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Final stage: ShuffleMapStage 39 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179)
[2025-06-05T22:05:01.704+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 38)
[2025-06-05T22:05:01.705+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:01.705+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Submitting ShuffleMapStage 39 (MapPartitionsRDD[79] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179), which has no missing parents
[2025-06-05T22:05:01.710+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 45.1 KiB, free 433.4 MiB)
[2025-06-05T22:05:01.711+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 20.5 KiB, free 433.3 MiB)
[2025-06-05T22:05:01.711+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.255.255.254:45911 (size: 20.5 KiB, free: 434.2 MiB)
[2025-06-05T22:05:01.712+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:01.712+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 39 (MapPartitionsRDD[79] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:01.713+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks resource profile 0
[2025-06-05T22:05:01.714+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 23) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8988 bytes)
[2025-06-05T22:05:01.715+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO Executor: Running task 0.0 in stage 39.0 (TID 23)
[2025-06-05T22:05:01.722+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO ShuffleBlockFetcherIterator: Getting 1 (13.5 KiB) non-empty blocks including 1 (13.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T22:05:01.722+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-06-05T22:05:01.738+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO Executor: Finished task 0.0 in stage 39.0 (TID 23). 5400 bytes result sent to driver
[2025-06-05T22:05:01.743+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 23) in 26 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:01.743+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool
[2025-06-05T22:05:01.745+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: ShuffleMapStage 39 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) finished in 0.037 s
[2025-06-05T22:05:01.745+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T22:05:01.745+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: running: Set()
[2025-06-05T22:05:01.746+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 10.255.255.254:45911 in memory (size: 10.7 KiB, free: 434.2 MiB)
[2025-06-05T22:05:01.746+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: waiting: Set()
[2025-06-05T22:05:01.747+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: failed: Set()
[2025-06-05T22:05:01.800+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179
[2025-06-05T22:05:01.803+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Got job 24 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) with 1 output partitions
[2025-06-05T22:05:01.803+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Final stage: ResultStage 42 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179)
[2025-06-05T22:05:01.804+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 41)
[2025-06-05T22:05:01.804+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:01.804+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 10.255.255.254:45911 in memory (size: 8.7 KiB, free: 434.2 MiB)
[2025-06-05T22:05:01.805+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[82] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179), which has no missing parents
[2025-06-05T22:05:01.810+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 16.9 KiB, free 433.4 MiB)
[2025-06-05T22:05:01.810+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 433.4 MiB)
[2025-06-05T22:05:01.811+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.255.255.254:45911 (size: 7.2 KiB, free: 434.2 MiB)
[2025-06-05T22:05:01.812+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:01.813+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[82] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:01.813+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks resource profile 0
[2025-06-05T22:05:01.814+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 24) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8999 bytes)
[2025-06-05T22:05:01.816+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO Executor: Running task 0.0 in stage 42.0 (TID 24)
[2025-06-05T22:05:01.820+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T22:05:01.822+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-06-05T22:05:01.823+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO Executor: Finished task 0.0 in stage 42.0 (TID 24). 3995 bytes result sent to driver
[2025-06-05T22:05:01.825+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 24) in 11 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:01.825+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool
[2025-06-05T22:05:01.825+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: ResultStage 42 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) finished in 0.020 s
[2025-06-05T22:05:01.826+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T22:05:01.826+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 42: Stage finished
[2025-06-05T22:05:01.828+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Job 24 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179, took 0.025497 s
[2025-06-05T22:05:01.924+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO FileSourceStrategy: Pushed Filters:
[2025-06-05T22:05:01.925+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO FileSourceStrategy: Post-Scan Filters:
[2025-06-05T22:05:01.947+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 201.4 KiB, free 433.2 MiB)
[2025-06-05T22:05:01.953+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 433.2 MiB)
[2025-06-05T22:05:01.954+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.255.255.254:45911 (size: 35.0 KiB, free: 434.2 MiB)
[2025-06-05T22:05:01.954+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO SparkContext: Created broadcast 31 from first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178
[2025-06-05T22:05:01.955+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-06-05T22:05:01.967+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178
[2025-06-05T22:05:01.968+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Got job 25 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) with 1 output partitions
[2025-06-05T22:05:01.968+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Final stage: ResultStage 43 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178)
[2025-06-05T22:05:01.968+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T22:05:01.969+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:01.969+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Submitting ResultStage 43 (MapPartitionsRDD[88] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178), which has no missing parents
[2025-06-05T22:05:01.971+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 19.4 KiB, free 433.1 MiB)
[2025-06-05T22:05:01.972+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 433.1 MiB)
[2025-06-05T22:05:01.973+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.255.255.254:45911 (size: 8.7 KiB, free: 434.2 MiB)
[2025-06-05T22:05:01.973+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:01.973+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 43 (MapPartitionsRDD[88] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:01.974+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSchedulerImpl: Adding task set 43.0 with 1 tasks resource profile 0
[2025-06-05T22:05:01.975+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 25) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9612 bytes)
[2025-06-05T22:05:01.975+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO Executor: Running task 0.0 in stage 43.0 (TID 25)
[2025-06-05T22:05:01.982+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T22:05:01.991+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO Executor: Finished task 0.0 in stage 43.0 (TID 25). 10024 bytes result sent to driver
[2025-06-05T22:05:01.992+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 25) in 18 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:01.992+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool
[2025-06-05T22:05:01.993+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: ResultStage 43 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) finished in 0.022 s
[2025-06-05T22:05:01.993+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T22:05:01.993+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 43: Stage finished
[2025-06-05T22:05:01.993+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Job 25 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178, took 0.026159 s
[2025-06-05T22:05:01.995+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Registering RDD 89 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) as input to shuffle 12
[2025-06-05T22:05:01.996+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Got map stage job 26 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) with 1 output partitions
[2025-06-05T22:05:01.996+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Final stage: ShuffleMapStage 44 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178)
[2025-06-05T22:05:01.996+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T22:05:01.996+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:01.998+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:01 INFO DAGScheduler: Submitting ShuffleMapStage 44 (MapPartitionsRDD[89] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178), which has no missing parents
[2025-06-05T22:05:02.001+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 27.6 KiB, free 433.1 MiB)
[2025-06-05T22:05:02.010+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 433.1 MiB)
[2025-06-05T22:05:02.011+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.255.255.254:45911 (size: 10.7 KiB, free: 434.2 MiB)
[2025-06-05T22:05:02.011+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:02.012+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 44 (MapPartitionsRDD[89] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:02.012+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSchedulerImpl: Adding task set 44.0 with 1 tasks resource profile 0
[2025-06-05T22:05:02.014+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 26) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9601 bytes)
[2025-06-05T22:05:02.015+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 10.255.255.254:45911 in memory (size: 7.2 KiB, free: 434.2 MiB)
[2025-06-05T22:05:02.015+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO Executor: Running task 0.0 in stage 44.0 (TID 26)
[2025-06-05T22:05:02.023+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T22:05:02.031+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 10.255.255.254:45911 in memory (size: 8.7 KiB, free: 434.2 MiB)
[2025-06-05T22:05:02.060+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 10.255.255.254:45911 in memory (size: 20.5 KiB, free: 434.2 MiB)
[2025-06-05T22:05:02.091+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO Executor: Finished task 0.0 in stage 44.0 (TID 26). 2199 bytes result sent to driver
[2025-06-05T22:05:02.092+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 26) in 78 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:02.092+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool
[2025-06-05T22:05:02.093+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: ShuffleMapStage 44 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) finished in 0.094 s
[2025-06-05T22:05:02.093+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T22:05:02.093+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: running: Set()
[2025-06-05T22:05:02.093+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: waiting: Set()
[2025-06-05T22:05:02.093+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: failed: Set()
[2025-06-05T22:05:02.099+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO ShufflePartitionsUtil: For shuffle(12), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-06-05T22:05:02.113+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Registering RDD 93 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) as input to shuffle 13
[2025-06-05T22:05:02.114+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Got map stage job 27 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) with 1 output partitions
[2025-06-05T22:05:02.114+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Final stage: ShuffleMapStage 46 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178)
[2025-06-05T22:05:02.114+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 45)
[2025-06-05T22:05:02.115+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:02.115+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Submitting ShuffleMapStage 46 (MapPartitionsRDD[93] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178), which has no missing parents
[2025-06-05T22:05:02.120+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 35.5 KiB, free 433.2 MiB)
[2025-06-05T22:05:02.121+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 17.1 KiB, free 433.2 MiB)
[2025-06-05T22:05:02.121+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.255.255.254:45911 (size: 17.1 KiB, free: 434.2 MiB)
[2025-06-05T22:05:02.122+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:02.122+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 46 (MapPartitionsRDD[93] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:02.122+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSchedulerImpl: Adding task set 46.0 with 1 tasks resource profile 0
[2025-06-05T22:05:02.123+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSetManager: Starting task 0.0 in stage 46.0 (TID 27) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8988 bytes)
[2025-06-05T22:05:02.124+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO Executor: Running task 0.0 in stage 46.0 (TID 27)
[2025-06-05T22:05:02.130+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO ShuffleBlockFetcherIterator: Getting 1 (13.5 KiB) non-empty blocks including 1 (13.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T22:05:02.130+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T22:05:02.141+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO Executor: Finished task 0.0 in stage 46.0 (TID 27). 5170 bytes result sent to driver
[2025-06-05T22:05:02.142+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSetManager: Finished task 0.0 in stage 46.0 (TID 27) in 19 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:02.143+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSchedulerImpl: Removed TaskSet 46.0, whose tasks have all completed, from pool
[2025-06-05T22:05:02.143+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: ShuffleMapStage 46 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) finished in 0.027 s
[2025-06-05T22:05:02.143+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T22:05:02.144+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: running: Set()
[2025-06-05T22:05:02.144+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: waiting: Set()
[2025-06-05T22:05:02.144+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: failed: Set()
[2025-06-05T22:05:02.158+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178
[2025-06-05T22:05:02.159+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Got job 28 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) with 1 output partitions
[2025-06-05T22:05:02.160+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Final stage: ResultStage 49 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178)
[2025-06-05T22:05:02.160+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 48)
[2025-06-05T22:05:02.160+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:02.160+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Submitting ResultStage 49 (MapPartitionsRDD[96] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178), which has no missing parents
[2025-06-05T22:05:02.162+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 35.0 KiB, free 433.1 MiB)
[2025-06-05T22:05:02.163+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 16.6 KiB, free 433.1 MiB)
[2025-06-05T22:05:02.164+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.255.255.254:45911 (size: 16.6 KiB, free: 434.2 MiB)
[2025-06-05T22:05:02.164+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:02.165+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 49 (MapPartitionsRDD[96] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:02.165+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSchedulerImpl: Adding task set 49.0 with 1 tasks resource profile 0
[2025-06-05T22:05:02.166+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSetManager: Starting task 0.0 in stage 49.0 (TID 28) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8999 bytes)
[2025-06-05T22:05:02.167+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO Executor: Running task 0.0 in stage 49.0 (TID 28)
[2025-06-05T22:05:02.173+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO ShuffleBlockFetcherIterator: Getting 1 (1538.0 B) non-empty blocks including 1 (1538.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T22:05:02.173+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T22:05:02.179+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO Executor: Finished task 0.0 in stage 49.0 (TID 28). 6437 bytes result sent to driver
[2025-06-05T22:05:02.180+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSetManager: Finished task 0.0 in stage 49.0 (TID 28) in 14 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:02.181+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool
[2025-06-05T22:05:02.181+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: ResultStage 49 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) finished in 0.021 s
[2025-06-05T22:05:02.181+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T22:05:02.181+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 49: Stage finished
[2025-06-05T22:05:02.181+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Job 28 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178, took 0.023152 s
[2025-06-05T22:05:02.268+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO FileSourceStrategy: Pushed Filters:
[2025-06-05T22:05:02.268+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO FileSourceStrategy: Post-Scan Filters:
[2025-06-05T22:05:02.285+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 201.4 KiB, free 432.9 MiB)
[2025-06-05T22:05:02.312+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 10.255.255.254:45911 in memory (size: 10.7 KiB, free: 434.2 MiB)
[2025-06-05T22:05:02.316+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 432.9 MiB)
[2025-06-05T22:05:02.317+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.255.255.254:45911 (size: 35.0 KiB, free: 434.2 MiB)
[2025-06-05T22:05:02.317+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO SparkContext: Created broadcast 36 from first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179
[2025-06-05T22:05:02.318+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-06-05T22:05:02.324+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 10.255.255.254:45911 in memory (size: 17.1 KiB, free: 434.2 MiB)
[2025-06-05T22:05:02.332+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 10.255.255.254:45911 in memory (size: 16.6 KiB, free: 434.2 MiB)
[2025-06-05T22:05:02.337+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179
[2025-06-05T22:05:02.348+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Got job 29 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) with 1 output partitions
[2025-06-05T22:05:02.349+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Final stage: ResultStage 50 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179)
[2025-06-05T22:05:02.349+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T22:05:02.349+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:02.349+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Submitting ResultStage 50 (MapPartitionsRDD[102] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179), which has no missing parents
[2025-06-05T22:05:02.351+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 19.4 KiB, free 433.0 MiB)
[2025-06-05T22:05:02.352+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 433.0 MiB)
[2025-06-05T22:05:02.352+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.255.255.254:45911 (size: 8.7 KiB, free: 434.2 MiB)
[2025-06-05T22:05:02.352+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:02.353+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 50 (MapPartitionsRDD[102] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:02.353+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSchedulerImpl: Adding task set 50.0 with 1 tasks resource profile 0
[2025-06-05T22:05:02.354+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSetManager: Starting task 0.0 in stage 50.0 (TID 29) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9612 bytes)
[2025-06-05T22:05:02.356+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO Executor: Running task 0.0 in stage 50.0 (TID 29)
[2025-06-05T22:05:02.364+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T22:05:02.373+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO Executor: Finished task 0.0 in stage 50.0 (TID 29). 10024 bytes result sent to driver
[2025-06-05T22:05:02.374+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSetManager: Finished task 0.0 in stage 50.0 (TID 29) in 20 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:02.375+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSchedulerImpl: Removed TaskSet 50.0, whose tasks have all completed, from pool
[2025-06-05T22:05:02.375+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: ResultStage 50 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) finished in 0.026 s
[2025-06-05T22:05:02.375+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T22:05:02.375+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 50: Stage finished
[2025-06-05T22:05:02.376+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Job 29 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179, took 0.038293 s
[2025-06-05T22:05:02.378+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Registering RDD 103 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) as input to shuffle 14
[2025-06-05T22:05:02.378+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Got map stage job 30 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) with 1 output partitions
[2025-06-05T22:05:02.379+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Final stage: ShuffleMapStage 51 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179)
[2025-06-05T22:05:02.379+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T22:05:02.379+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:02.379+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Submitting ShuffleMapStage 51 (MapPartitionsRDD[103] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179), which has no missing parents
[2025-06-05T22:05:02.385+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 27.6 KiB, free 433.0 MiB)
[2025-06-05T22:05:02.386+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 433.0 MiB)
[2025-06-05T22:05:02.386+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 10.255.255.254:45911 (size: 10.7 KiB, free: 434.2 MiB)
[2025-06-05T22:05:02.387+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:02.387+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 51 (MapPartitionsRDD[103] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:02.387+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSchedulerImpl: Adding task set 51.0 with 1 tasks resource profile 0
[2025-06-05T22:05:02.389+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSetManager: Starting task 0.0 in stage 51.0 (TID 30) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9601 bytes)
[2025-06-05T22:05:02.389+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO Executor: Running task 0.0 in stage 51.0 (TID 30)
[2025-06-05T22:05:02.397+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T22:05:02.429+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO Executor: Finished task 0.0 in stage 51.0 (TID 30). 2199 bytes result sent to driver
[2025-06-05T22:05:02.430+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSetManager: Finished task 0.0 in stage 51.0 (TID 30) in 42 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:02.431+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSchedulerImpl: Removed TaskSet 51.0, whose tasks have all completed, from pool
[2025-06-05T22:05:02.431+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: ShuffleMapStage 51 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) finished in 0.052 s
[2025-06-05T22:05:02.431+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T22:05:02.431+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: running: Set()
[2025-06-05T22:05:02.431+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: waiting: Set()
[2025-06-05T22:05:02.432+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: failed: Set()
[2025-06-05T22:05:02.439+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO ShufflePartitionsUtil: For shuffle(14), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-06-05T22:05:02.459+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Registering RDD 106 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) as input to shuffle 15
[2025-06-05T22:05:02.459+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Got map stage job 31 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) with 1 output partitions
[2025-06-05T22:05:02.459+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Final stage: ShuffleMapStage 53 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179)
[2025-06-05T22:05:02.459+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 52)
[2025-06-05T22:05:02.460+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:02.460+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Submitting ShuffleMapStage 53 (MapPartitionsRDD[106] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179), which has no missing parents
[2025-06-05T22:05:02.463+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 45.1 KiB, free 432.9 MiB)
[2025-06-05T22:05:02.464+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 20.5 KiB, free 432.9 MiB)
[2025-06-05T22:05:02.465+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 10.255.255.254:45911 (size: 20.5 KiB, free: 434.2 MiB)
[2025-06-05T22:05:02.465+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:02.465+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 53 (MapPartitionsRDD[106] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:02.466+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSchedulerImpl: Adding task set 53.0 with 1 tasks resource profile 0
[2025-06-05T22:05:02.467+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSetManager: Starting task 0.0 in stage 53.0 (TID 31) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8988 bytes)
[2025-06-05T22:05:02.468+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO Executor: Running task 0.0 in stage 53.0 (TID 31)
[2025-06-05T22:05:02.475+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO ShuffleBlockFetcherIterator: Getting 1 (13.5 KiB) non-empty blocks including 1 (13.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T22:05:02.476+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T22:05:02.493+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 10.255.255.254:45911 in memory (size: 8.7 KiB, free: 434.2 MiB)
[2025-06-05T22:05:02.493+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO Executor: Finished task 0.0 in stage 53.0 (TID 31). 5400 bytes result sent to driver
[2025-06-05T22:05:02.499+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 10.255.255.254:45911 in memory (size: 10.7 KiB, free: 434.2 MiB)
[2025-06-05T22:05:02.500+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSetManager: Finished task 0.0 in stage 53.0 (TID 31) in 32 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:02.500+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSchedulerImpl: Removed TaskSet 53.0, whose tasks have all completed, from pool
[2025-06-05T22:05:02.501+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: ShuffleMapStage 53 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) finished in 0.038 s
[2025-06-05T22:05:02.501+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T22:05:02.501+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: running: Set()
[2025-06-05T22:05:02.502+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: waiting: Set()
[2025-06-05T22:05:02.502+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: failed: Set()
[2025-06-05T22:05:02.537+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179
[2025-06-05T22:05:02.538+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Got job 32 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) with 1 output partitions
[2025-06-05T22:05:02.539+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Final stage: ResultStage 56 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179)
[2025-06-05T22:05:02.539+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 55)
[2025-06-05T22:05:02.539+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:02.539+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Submitting ResultStage 56 (MapPartitionsRDD[109] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179), which has no missing parents
[2025-06-05T22:05:02.541+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 16.9 KiB, free 432.9 MiB)
[2025-06-05T22:05:02.542+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 432.9 MiB)
[2025-06-05T22:05:02.542+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 10.255.255.254:45911 (size: 7.2 KiB, free: 434.2 MiB)
[2025-06-05T22:05:02.543+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:02.543+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 56 (MapPartitionsRDD[109] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:02.543+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSchedulerImpl: Adding task set 56.0 with 1 tasks resource profile 0
[2025-06-05T22:05:02.545+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSetManager: Starting task 0.0 in stage 56.0 (TID 32) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8999 bytes)
[2025-06-05T22:05:02.545+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO Executor: Running task 0.0 in stage 56.0 (TID 32)
[2025-06-05T22:05:02.550+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T22:05:02.550+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T22:05:02.552+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO Executor: Finished task 0.0 in stage 56.0 (TID 32). 3995 bytes result sent to driver
[2025-06-05T22:05:02.553+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSetManager: Finished task 0.0 in stage 56.0 (TID 32) in 9 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:02.553+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSchedulerImpl: Removed TaskSet 56.0, whose tasks have all completed, from pool
[2025-06-05T22:05:02.554+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: ResultStage 56 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) finished in 0.014 s
[2025-06-05T22:05:02.554+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T22:05:02.554+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 56: Stage finished
[2025-06-05T22:05:02.554+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Job 32 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179, took 0.017229 s
[2025-06-05T22:05:02.679+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO FileSourceStrategy: Pushed Filters:
[2025-06-05T22:05:02.680+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO FileSourceStrategy: Post-Scan Filters:
[2025-06-05T22:05:02.693+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 201.4 KiB, free 432.7 MiB)
[2025-06-05T22:05:02.699+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 432.7 MiB)
[2025-06-05T22:05:02.700+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 10.255.255.254:45911 (size: 35.1 KiB, free: 434.1 MiB)
[2025-06-05T22:05:02.700+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO SparkContext: Created broadcast 41 from first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178
[2025-06-05T22:05:02.701+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-06-05T22:05:02.712+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178
[2025-06-05T22:05:02.713+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Got job 33 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) with 1 output partitions
[2025-06-05T22:05:02.713+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Final stage: ResultStage 57 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178)
[2025-06-05T22:05:02.713+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T22:05:02.714+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:02.714+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Submitting ResultStage 57 (MapPartitionsRDD[115] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178), which has no missing parents
[2025-06-05T22:05:02.715+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 19.4 KiB, free 432.7 MiB)
[2025-06-05T22:05:02.715+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 432.7 MiB)
[2025-06-05T22:05:02.716+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 10.255.255.254:45911 (size: 8.7 KiB, free: 434.1 MiB)
[2025-06-05T22:05:02.716+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:02.717+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 57 (MapPartitionsRDD[115] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:02.717+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSchedulerImpl: Adding task set 57.0 with 1 tasks resource profile 0
[2025-06-05T22:05:02.717+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSetManager: Starting task 0.0 in stage 57.0 (TID 33) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9612 bytes)
[2025-06-05T22:05:02.718+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO Executor: Running task 0.0 in stage 57.0 (TID 33)
[2025-06-05T22:05:02.723+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T22:05:02.730+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO Executor: Finished task 0.0 in stage 57.0 (TID 33). 10024 bytes result sent to driver
[2025-06-05T22:05:02.731+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSetManager: Finished task 0.0 in stage 57.0 (TID 33) in 14 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:02.732+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSchedulerImpl: Removed TaskSet 57.0, whose tasks have all completed, from pool
[2025-06-05T22:05:02.732+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: ResultStage 57 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) finished in 0.018 s
[2025-06-05T22:05:02.732+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T22:05:02.732+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 57: Stage finished
[2025-06-05T22:05:02.732+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Job 33 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178, took 0.020000 s
[2025-06-05T22:05:02.734+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Registering RDD 116 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) as input to shuffle 16
[2025-06-05T22:05:02.734+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Got map stage job 34 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) with 1 output partitions
[2025-06-05T22:05:02.734+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Final stage: ShuffleMapStage 58 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178)
[2025-06-05T22:05:02.735+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T22:05:02.735+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:02.735+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Submitting ShuffleMapStage 58 (MapPartitionsRDD[116] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178), which has no missing parents
[2025-06-05T22:05:02.737+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 27.6 KiB, free 432.6 MiB)
[2025-06-05T22:05:02.747+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 432.6 MiB)
[2025-06-05T22:05:02.748+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 10.255.255.254:45911 (size: 10.7 KiB, free: 434.1 MiB)
[2025-06-05T22:05:02.749+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 10.255.255.254:45911 in memory (size: 20.5 KiB, free: 434.1 MiB)
[2025-06-05T22:05:02.750+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:02.751+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 58 (MapPartitionsRDD[116] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:02.751+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSchedulerImpl: Adding task set 58.0 with 1 tasks resource profile 0
[2025-06-05T22:05:02.751+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSetManager: Starting task 0.0 in stage 58.0 (TID 34) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9601 bytes)
[2025-06-05T22:05:02.752+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO Executor: Running task 0.0 in stage 58.0 (TID 34)
[2025-06-05T22:05:02.759+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO BlockManagerInfo: Removed broadcast_40_piece0 on 10.255.255.254:45911 in memory (size: 7.2 KiB, free: 434.1 MiB)
[2025-06-05T22:05:02.760+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T22:05:02.768+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO BlockManagerInfo: Removed broadcast_42_piece0 on 10.255.255.254:45911 in memory (size: 8.7 KiB, free: 434.2 MiB)
[2025-06-05T22:05:02.820+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO Executor: Finished task 0.0 in stage 58.0 (TID 34). 2199 bytes result sent to driver
[2025-06-05T22:05:02.821+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSetManager: Finished task 0.0 in stage 58.0 (TID 34) in 70 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:02.821+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSchedulerImpl: Removed TaskSet 58.0, whose tasks have all completed, from pool
[2025-06-05T22:05:02.822+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: ShuffleMapStage 58 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) finished in 0.086 s
[2025-06-05T22:05:02.822+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T22:05:02.822+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: running: Set()
[2025-06-05T22:05:02.822+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: waiting: Set()
[2025-06-05T22:05:02.822+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: failed: Set()
[2025-06-05T22:05:02.825+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO ShufflePartitionsUtil: For shuffle(16), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-06-05T22:05:02.837+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Registering RDD 120 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) as input to shuffle 17
[2025-06-05T22:05:02.837+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Got map stage job 35 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) with 1 output partitions
[2025-06-05T22:05:02.837+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Final stage: ShuffleMapStage 60 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178)
[2025-06-05T22:05:02.838+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 59)
[2025-06-05T22:05:02.838+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:02.838+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Submitting ShuffleMapStage 60 (MapPartitionsRDD[120] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178), which has no missing parents
[2025-06-05T22:05:02.844+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 35.5 KiB, free 432.7 MiB)
[2025-06-05T22:05:02.844+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 17.1 KiB, free 432.7 MiB)
[2025-06-05T22:05:02.845+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 10.255.255.254:45911 (size: 17.1 KiB, free: 434.1 MiB)
[2025-06-05T22:05:02.845+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:02.846+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 60 (MapPartitionsRDD[120] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:02.846+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSchedulerImpl: Adding task set 60.0 with 1 tasks resource profile 0
[2025-06-05T22:05:02.847+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSetManager: Starting task 0.0 in stage 60.0 (TID 35) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8988 bytes)
[2025-06-05T22:05:02.848+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO Executor: Running task 0.0 in stage 60.0 (TID 35)
[2025-06-05T22:05:02.853+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO ShuffleBlockFetcherIterator: Getting 1 (13.5 KiB) non-empty blocks including 1 (13.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T22:05:02.853+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T22:05:02.861+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO Executor: Finished task 0.0 in stage 60.0 (TID 35). 5170 bytes result sent to driver
[2025-06-05T22:05:02.862+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSetManager: Finished task 0.0 in stage 60.0 (TID 35) in 15 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:02.862+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSchedulerImpl: Removed TaskSet 60.0, whose tasks have all completed, from pool
[2025-06-05T22:05:02.863+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: ShuffleMapStage 60 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) finished in 0.024 s
[2025-06-05T22:05:02.863+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T22:05:02.863+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: running: Set()
[2025-06-05T22:05:02.863+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: waiting: Set()
[2025-06-05T22:05:02.864+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: failed: Set()
[2025-06-05T22:05:02.877+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178
[2025-06-05T22:05:02.878+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Got job 36 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) with 1 output partitions
[2025-06-05T22:05:02.879+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Final stage: ResultStage 63 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178)
[2025-06-05T22:05:02.879+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 62)
[2025-06-05T22:05:02.879+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:02.879+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Submitting ResultStage 63 (MapPartitionsRDD[123] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178), which has no missing parents
[2025-06-05T22:05:02.882+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 35.1 KiB, free 432.7 MiB)
[2025-06-05T22:05:02.883+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 16.7 KiB, free 432.6 MiB)
[2025-06-05T22:05:02.883+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 10.255.255.254:45911 (size: 16.7 KiB, free: 434.1 MiB)
[2025-06-05T22:05:02.884+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:02.884+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 63 (MapPartitionsRDD[123] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:02.884+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSchedulerImpl: Adding task set 63.0 with 1 tasks resource profile 0
[2025-06-05T22:05:02.885+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSetManager: Starting task 0.0 in stage 63.0 (TID 36) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8999 bytes)
[2025-06-05T22:05:02.886+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO Executor: Running task 0.0 in stage 63.0 (TID 36)
[2025-06-05T22:05:02.891+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO ShuffleBlockFetcherIterator: Getting 1 (1538.0 B) non-empty blocks including 1 (1538.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T22:05:02.891+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T22:05:02.896+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO Executor: Finished task 0.0 in stage 63.0 (TID 36). 6437 bytes result sent to driver
[2025-06-05T22:05:02.897+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSetManager: Finished task 0.0 in stage 63.0 (TID 36) in 12 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:02.897+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSchedulerImpl: Removed TaskSet 63.0, whose tasks have all completed, from pool
[2025-06-05T22:05:02.898+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: ResultStage 63 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) finished in 0.019 s
[2025-06-05T22:05:02.898+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T22:05:02.898+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 63: Stage finished
[2025-06-05T22:05:02.898+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO DAGScheduler: Job 36 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178, took 0.020584 s
[2025-06-05T22:05:02.981+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO FileSourceStrategy: Pushed Filters:
[2025-06-05T22:05:02.981+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO FileSourceStrategy: Post-Scan Filters:
[2025-06-05T22:05:02.993+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:02 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 201.4 KiB, free 432.4 MiB)
[2025-06-05T22:05:03.005+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO BlockManagerInfo: Removed broadcast_45_piece0 on 10.255.255.254:45911 in memory (size: 16.7 KiB, free: 434.1 MiB)
[2025-06-05T22:05:03.010+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 432.5 MiB)
[2025-06-05T22:05:03.010+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 10.255.255.254:45911 (size: 35.1 KiB, free: 434.1 MiB)
[2025-06-05T22:05:03.015+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO SparkContext: Created broadcast 46 from first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179
[2025-06-05T22:05:03.016+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-06-05T22:05:03.017+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO BlockManagerInfo: Removed broadcast_44_piece0 on 10.255.255.254:45911 in memory (size: 17.1 KiB, free: 434.1 MiB)
[2025-06-05T22:05:03.021+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO BlockManagerInfo: Removed broadcast_43_piece0 on 10.255.255.254:45911 in memory (size: 10.7 KiB, free: 434.1 MiB)
[2025-06-05T22:05:03.028+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179
[2025-06-05T22:05:03.029+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Got job 37 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) with 1 output partitions
[2025-06-05T22:05:03.029+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Final stage: ResultStage 64 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179)
[2025-06-05T22:05:03.029+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T22:05:03.029+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:03.030+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Submitting ResultStage 64 (MapPartitionsRDD[129] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179), which has no missing parents
[2025-06-05T22:05:03.031+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 19.4 KiB, free 432.5 MiB)
[2025-06-05T22:05:03.032+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 432.5 MiB)
[2025-06-05T22:05:03.033+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 10.255.255.254:45911 (size: 8.7 KiB, free: 434.1 MiB)
[2025-06-05T22:05:03.033+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:03.034+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 64 (MapPartitionsRDD[129] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:03.034+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSchedulerImpl: Adding task set 64.0 with 1 tasks resource profile 0
[2025-06-05T22:05:03.044+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSetManager: Starting task 0.0 in stage 64.0 (TID 37) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9612 bytes)
[2025-06-05T22:05:03.045+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO Executor: Running task 0.0 in stage 64.0 (TID 37)
[2025-06-05T22:05:03.049+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T22:05:03.060+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO Executor: Finished task 0.0 in stage 64.0 (TID 37). 10024 bytes result sent to driver
[2025-06-05T22:05:03.061+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSetManager: Finished task 0.0 in stage 64.0 (TID 37) in 18 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:03.061+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSchedulerImpl: Removed TaskSet 64.0, whose tasks have all completed, from pool
[2025-06-05T22:05:03.062+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: ResultStage 64 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) finished in 0.031 s
[2025-06-05T22:05:03.062+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Job 37 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T22:05:03.062+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 64: Stage finished
[2025-06-05T22:05:03.062+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Job 37 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179, took 0.034229 s
[2025-06-05T22:05:03.064+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Registering RDD 130 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) as input to shuffle 18
[2025-06-05T22:05:03.064+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Got map stage job 38 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) with 1 output partitions
[2025-06-05T22:05:03.064+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Final stage: ShuffleMapStage 65 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179)
[2025-06-05T22:05:03.065+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T22:05:03.065+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:03.065+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Submitting ShuffleMapStage 65 (MapPartitionsRDD[130] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179), which has no missing parents
[2025-06-05T22:05:03.067+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 27.6 KiB, free 432.5 MiB)
[2025-06-05T22:05:03.068+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 432.5 MiB)
[2025-06-05T22:05:03.068+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 10.255.255.254:45911 (size: 10.7 KiB, free: 434.1 MiB)
[2025-06-05T22:05:03.069+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:03.069+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 65 (MapPartitionsRDD[130] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:03.069+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSchedulerImpl: Adding task set 65.0 with 1 tasks resource profile 0
[2025-06-05T22:05:03.070+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSetManager: Starting task 0.0 in stage 65.0 (TID 38) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9601 bytes)
[2025-06-05T22:05:03.071+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO Executor: Running task 0.0 in stage 65.0 (TID 38)
[2025-06-05T22:05:03.076+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T22:05:03.116+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO Executor: Finished task 0.0 in stage 65.0 (TID 38). 2199 bytes result sent to driver
[2025-06-05T22:05:03.117+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSetManager: Finished task 0.0 in stage 65.0 (TID 38) in 47 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:03.117+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSchedulerImpl: Removed TaskSet 65.0, whose tasks have all completed, from pool
[2025-06-05T22:05:03.118+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: ShuffleMapStage 65 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) finished in 0.053 s
[2025-06-05T22:05:03.118+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T22:05:03.118+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: running: Set()
[2025-06-05T22:05:03.118+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: waiting: Set()
[2025-06-05T22:05:03.118+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: failed: Set()
[2025-06-05T22:05:03.123+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO ShufflePartitionsUtil: For shuffle(18), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-06-05T22:05:03.145+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Registering RDD 133 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) as input to shuffle 19
[2025-06-05T22:05:03.146+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Got map stage job 39 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) with 1 output partitions
[2025-06-05T22:05:03.146+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Final stage: ShuffleMapStage 67 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179)
[2025-06-05T22:05:03.146+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 66)
[2025-06-05T22:05:03.147+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:03.147+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Submitting ShuffleMapStage 67 (MapPartitionsRDD[133] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179), which has no missing parents
[2025-06-05T22:05:03.150+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 45.1 KiB, free 432.4 MiB)
[2025-06-05T22:05:03.159+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 20.5 KiB, free 432.4 MiB)
[2025-06-05T22:05:03.159+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on 10.255.255.254:45911 (size: 20.5 KiB, free: 434.1 MiB)
[2025-06-05T22:05:03.160+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:03.164+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO BlockManagerInfo: Removed broadcast_47_piece0 on 10.255.255.254:45911 in memory (size: 8.7 KiB, free: 434.1 MiB)
[2025-06-05T22:05:03.165+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 67 (MapPartitionsRDD[133] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:03.165+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSchedulerImpl: Adding task set 67.0 with 1 tasks resource profile 0
[2025-06-05T22:05:03.166+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSetManager: Starting task 0.0 in stage 67.0 (TID 39) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8988 bytes)
[2025-06-05T22:05:03.167+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO Executor: Running task 0.0 in stage 67.0 (TID 39)
[2025-06-05T22:05:03.171+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO BlockManagerInfo: Removed broadcast_48_piece0 on 10.255.255.254:45911 in memory (size: 10.7 KiB, free: 434.1 MiB)
[2025-06-05T22:05:03.175+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO ShuffleBlockFetcherIterator: Getting 1 (13.5 KiB) non-empty blocks including 1 (13.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T22:05:03.175+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T22:05:03.183+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO Executor: Finished task 0.0 in stage 67.0 (TID 39). 5314 bytes result sent to driver
[2025-06-05T22:05:03.186+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSetManager: Finished task 0.0 in stage 67.0 (TID 39) in 20 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:03.186+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSchedulerImpl: Removed TaskSet 67.0, whose tasks have all completed, from pool
[2025-06-05T22:05:03.187+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: ShuffleMapStage 67 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) finished in 0.038 s
[2025-06-05T22:05:03.187+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T22:05:03.187+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: running: Set()
[2025-06-05T22:05:03.187+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: waiting: Set()
[2025-06-05T22:05:03.188+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: failed: Set()
[2025-06-05T22:05:03.216+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179
[2025-06-05T22:05:03.218+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Got job 40 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) with 1 output partitions
[2025-06-05T22:05:03.218+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Final stage: ResultStage 70 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179)
[2025-06-05T22:05:03.218+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 69)
[2025-06-05T22:05:03.218+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:03.218+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Submitting ResultStage 70 (MapPartitionsRDD[136] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179), which has no missing parents
[2025-06-05T22:05:03.220+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 16.9 KiB, free 432.5 MiB)
[2025-06-05T22:05:03.221+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 432.5 MiB)
[2025-06-05T22:05:03.222+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on 10.255.255.254:45911 (size: 7.2 KiB, free: 434.1 MiB)
[2025-06-05T22:05:03.222+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:03.223+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 70 (MapPartitionsRDD[136] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:03.223+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSchedulerImpl: Adding task set 70.0 with 1 tasks resource profile 0
[2025-06-05T22:05:03.224+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSetManager: Starting task 0.0 in stage 70.0 (TID 40) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8999 bytes)
[2025-06-05T22:05:03.225+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO Executor: Running task 0.0 in stage 70.0 (TID 40)
[2025-06-05T22:05:03.228+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T22:05:03.228+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T22:05:03.229+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO Executor: Finished task 0.0 in stage 70.0 (TID 40). 3995 bytes result sent to driver
[2025-06-05T22:05:03.230+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSetManager: Finished task 0.0 in stage 70.0 (TID 40) in 6 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:03.230+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSchedulerImpl: Removed TaskSet 70.0, whose tasks have all completed, from pool
[2025-06-05T22:05:03.231+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: ResultStage 70 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) finished in 0.012 s
[2025-06-05T22:05:03.231+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Job 40 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T22:05:03.231+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 70: Stage finished
[2025-06-05T22:05:03.232+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Job 40 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179, took 0.014837 s
[2025-06-05T22:05:03.293+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO FileSourceStrategy: Pushed Filters:
[2025-06-05T22:05:03.294+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO FileSourceStrategy: Post-Scan Filters:
[2025-06-05T22:05:03.308+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 201.4 KiB, free 432.3 MiB)
[2025-06-05T22:05:03.315+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 432.2 MiB)
[2025-06-05T22:05:03.316+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on 10.255.255.254:45911 (size: 35.0 KiB, free: 434.1 MiB)
[2025-06-05T22:05:03.316+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO SparkContext: Created broadcast 51 from first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178
[2025-06-05T22:05:03.317+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-06-05T22:05:03.331+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178
[2025-06-05T22:05:03.331+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Got job 41 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) with 1 output partitions
[2025-06-05T22:05:03.332+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Final stage: ResultStage 71 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178)
[2025-06-05T22:05:03.332+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T22:05:03.332+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:03.332+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Submitting ResultStage 71 (MapPartitionsRDD[142] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178), which has no missing parents
[2025-06-05T22:05:03.334+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 19.3 KiB, free 432.2 MiB)
[2025-06-05T22:05:03.344+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 432.2 MiB)
[2025-06-05T22:05:03.345+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on 10.255.255.254:45911 (size: 8.6 KiB, free: 434.1 MiB)
[2025-06-05T22:05:03.345+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:03.345+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 71 (MapPartitionsRDD[142] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:03.346+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSchedulerImpl: Adding task set 71.0 with 1 tasks resource profile 0
[2025-06-05T22:05:03.348+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSetManager: Starting task 0.0 in stage 71.0 (TID 41) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9612 bytes)
[2025-06-05T22:05:03.349+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO BlockManagerInfo: Removed broadcast_50_piece0 on 10.255.255.254:45911 in memory (size: 7.2 KiB, free: 434.1 MiB)
[2025-06-05T22:05:03.349+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO Executor: Running task 0.0 in stage 71.0 (TID 41)
[2025-06-05T22:05:03.355+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T22:05:03.364+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO BlockManagerInfo: Removed broadcast_49_piece0 on 10.255.255.254:45911 in memory (size: 20.5 KiB, free: 434.1 MiB)
[2025-06-05T22:05:03.368+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO Executor: Finished task 0.0 in stage 71.0 (TID 41). 10024 bytes result sent to driver
[2025-06-05T22:05:03.369+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSetManager: Finished task 0.0 in stage 71.0 (TID 41) in 22 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:03.369+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSchedulerImpl: Removed TaskSet 71.0, whose tasks have all completed, from pool
[2025-06-05T22:05:03.369+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: ResultStage 71 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) finished in 0.037 s
[2025-06-05T22:05:03.370+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Job 41 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T22:05:03.370+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 71: Stage finished
[2025-06-05T22:05:03.370+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Job 41 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178, took 0.039251 s
[2025-06-05T22:05:03.372+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Registering RDD 143 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) as input to shuffle 20
[2025-06-05T22:05:03.372+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Got map stage job 42 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) with 1 output partitions
[2025-06-05T22:05:03.373+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Final stage: ShuffleMapStage 72 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178)
[2025-06-05T22:05:03.373+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T22:05:03.373+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:03.373+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Submitting ShuffleMapStage 72 (MapPartitionsRDD[143] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178), which has no missing parents
[2025-06-05T22:05:03.376+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 27.6 KiB, free 432.3 MiB)
[2025-06-05T22:05:03.377+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 432.3 MiB)
[2025-06-05T22:05:03.377+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on 10.255.255.254:45911 (size: 10.7 KiB, free: 434.1 MiB)
[2025-06-05T22:05:03.378+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:03.378+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 72 (MapPartitionsRDD[143] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:03.379+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSchedulerImpl: Adding task set 72.0 with 1 tasks resource profile 0
[2025-06-05T22:05:03.389+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSetManager: Starting task 0.0 in stage 72.0 (TID 42) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9601 bytes)
[2025-06-05T22:05:03.389+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO Executor: Running task 0.0 in stage 72.0 (TID 42)
[2025-06-05T22:05:03.396+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T22:05:03.441+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO Executor: Finished task 0.0 in stage 72.0 (TID 42). 2199 bytes result sent to driver
[2025-06-05T22:05:03.443+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSetManager: Finished task 0.0 in stage 72.0 (TID 42) in 55 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:03.444+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSchedulerImpl: Removed TaskSet 72.0, whose tasks have all completed, from pool
[2025-06-05T22:05:03.444+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: ShuffleMapStage 72 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) finished in 0.071 s
[2025-06-05T22:05:03.445+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T22:05:03.445+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: running: Set()
[2025-06-05T22:05:03.445+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: waiting: Set()
[2025-06-05T22:05:03.445+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: failed: Set()
[2025-06-05T22:05:03.449+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO ShufflePartitionsUtil: For shuffle(20), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-06-05T22:05:03.466+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Registering RDD 147 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) as input to shuffle 21
[2025-06-05T22:05:03.467+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Got map stage job 43 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) with 1 output partitions
[2025-06-05T22:05:03.467+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Final stage: ShuffleMapStage 74 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178)
[2025-06-05T22:05:03.467+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 73)
[2025-06-05T22:05:03.467+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:03.468+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Submitting ShuffleMapStage 74 (MapPartitionsRDD[147] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178), which has no missing parents
[2025-06-05T22:05:03.474+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 35.4 KiB, free 432.2 MiB)
[2025-06-05T22:05:03.475+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 17.1 KiB, free 432.2 MiB)
[2025-06-05T22:05:03.475+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on 10.255.255.254:45911 (size: 17.1 KiB, free: 434.1 MiB)
[2025-06-05T22:05:03.476+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:03.476+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 74 (MapPartitionsRDD[147] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:03.476+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSchedulerImpl: Adding task set 74.0 with 1 tasks resource profile 0
[2025-06-05T22:05:03.477+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSetManager: Starting task 0.0 in stage 74.0 (TID 43) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8988 bytes)
[2025-06-05T22:05:03.478+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO Executor: Running task 0.0 in stage 74.0 (TID 43)
[2025-06-05T22:05:03.485+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO ShuffleBlockFetcherIterator: Getting 1 (13.5 KiB) non-empty blocks including 1 (13.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T22:05:03.485+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T22:05:03.494+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO Executor: Finished task 0.0 in stage 74.0 (TID 43). 5170 bytes result sent to driver
[2025-06-05T22:05:03.495+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSetManager: Finished task 0.0 in stage 74.0 (TID 43) in 18 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:03.495+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSchedulerImpl: Removed TaskSet 74.0, whose tasks have all completed, from pool
[2025-06-05T22:05:03.496+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: ShuffleMapStage 74 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) finished in 0.026 s
[2025-06-05T22:05:03.496+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T22:05:03.496+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: running: Set()
[2025-06-05T22:05:03.497+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: waiting: Set()
[2025-06-05T22:05:03.497+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: failed: Set()
[2025-06-05T22:05:03.511+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178
[2025-06-05T22:05:03.512+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Got job 44 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) with 1 output partitions
[2025-06-05T22:05:03.512+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Final stage: ResultStage 77 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178)
[2025-06-05T22:05:03.512+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 76)
[2025-06-05T22:05:03.513+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:03.513+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Submitting ResultStage 77 (MapPartitionsRDD[150] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178), which has no missing parents
[2025-06-05T22:05:03.515+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 35.0 KiB, free 432.2 MiB)
[2025-06-05T22:05:03.528+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 16.6 KiB, free 432.2 MiB)
[2025-06-05T22:05:03.528+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO BlockManagerInfo: Removed broadcast_53_piece0 on 10.255.255.254:45911 in memory (size: 10.7 KiB, free: 434.1 MiB)
[2025-06-05T22:05:03.529+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on 10.255.255.254:45911 (size: 16.6 KiB, free: 434.1 MiB)
[2025-06-05T22:05:03.529+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO SparkContext: Created broadcast 55 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:03.531+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 77 (MapPartitionsRDD[150] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:03.532+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSchedulerImpl: Adding task set 77.0 with 1 tasks resource profile 0
[2025-06-05T22:05:03.532+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSetManager: Starting task 0.0 in stage 77.0 (TID 44) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8999 bytes)
[2025-06-05T22:05:03.533+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO Executor: Running task 0.0 in stage 77.0 (TID 44)
[2025-06-05T22:05:03.533+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO BlockManagerInfo: Removed broadcast_52_piece0 on 10.255.255.254:45911 in memory (size: 8.6 KiB, free: 434.1 MiB)
[2025-06-05T22:05:03.541+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO BlockManagerInfo: Removed broadcast_54_piece0 on 10.255.255.254:45911 in memory (size: 17.1 KiB, free: 434.1 MiB)
[2025-06-05T22:05:03.542+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO ShuffleBlockFetcherIterator: Getting 1 (1051.0 B) non-empty blocks including 1 (1051.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T22:05:03.543+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T22:05:03.556+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO Executor: Finished task 0.0 in stage 77.0 (TID 44). 6437 bytes result sent to driver
[2025-06-05T22:05:03.558+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSetManager: Finished task 0.0 in stage 77.0 (TID 44) in 26 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:03.559+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSchedulerImpl: Removed TaskSet 77.0, whose tasks have all completed, from pool
[2025-06-05T22:05:03.570+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: ResultStage 77 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178) finished in 0.057 s
[2025-06-05T22:05:03.571+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Job 44 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T22:05:03.571+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 77: Stage finished
[2025-06-05T22:05:03.571+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Job 44 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:178, took 0.060157 s
[2025-06-05T22:05:03.864+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO FileSourceStrategy: Pushed Filters:
[2025-06-05T22:05:03.865+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO FileSourceStrategy: Post-Scan Filters:
[2025-06-05T22:05:03.891+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO MemoryStore: Block broadcast_56 stored as values in memory (estimated size 201.4 KiB, free 432.1 MiB)
[2025-06-05T22:05:03.910+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 432.0 MiB)
[2025-06-05T22:05:03.910+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on 10.255.255.254:45911 (size: 35.0 KiB, free: 434.0 MiB)
[2025-06-05T22:05:03.912+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO SparkContext: Created broadcast 56 from first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179
[2025-06-05T22:05:03.913+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-06-05T22:05:03.946+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179
[2025-06-05T22:05:03.947+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Got job 45 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) with 1 output partitions
[2025-06-05T22:05:03.948+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Final stage: ResultStage 78 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179)
[2025-06-05T22:05:03.948+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T22:05:03.948+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:03.948+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Submitting ResultStage 78 (MapPartitionsRDD[156] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179), which has no missing parents
[2025-06-05T22:05:03.953+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO MemoryStore: Block broadcast_57 stored as values in memory (estimated size 19.3 KiB, free 432.0 MiB)
[2025-06-05T22:05:03.955+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 432.0 MiB)
[2025-06-05T22:05:03.955+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on 10.255.255.254:45911 (size: 8.6 KiB, free: 434.0 MiB)
[2025-06-05T22:05:03.956+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO SparkContext: Created broadcast 57 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:03.956+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 78 (MapPartitionsRDD[156] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:03.957+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSchedulerImpl: Adding task set 78.0 with 1 tasks resource profile 0
[2025-06-05T22:05:03.958+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSetManager: Starting task 0.0 in stage 78.0 (TID 45) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9612 bytes)
[2025-06-05T22:05:03.959+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO Executor: Running task 0.0 in stage 78.0 (TID 45)
[2025-06-05T22:05:03.964+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T22:05:03.978+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO Executor: Finished task 0.0 in stage 78.0 (TID 45). 10024 bytes result sent to driver
[2025-06-05T22:05:03.979+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSetManager: Finished task 0.0 in stage 78.0 (TID 45) in 22 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:03.979+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSchedulerImpl: Removed TaskSet 78.0, whose tasks have all completed, from pool
[2025-06-05T22:05:03.980+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: ResultStage 78 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) finished in 0.031 s
[2025-06-05T22:05:03.980+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Job 45 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T22:05:03.980+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 78: Stage finished
[2025-06-05T22:05:03.981+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Job 45 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179, took 0.034049 s
[2025-06-05T22:05:03.985+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Registering RDD 157 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) as input to shuffle 22
[2025-06-05T22:05:03.986+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Got map stage job 46 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) with 1 output partitions
[2025-06-05T22:05:03.986+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Final stage: ShuffleMapStage 79 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179)
[2025-06-05T22:05:03.986+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T22:05:03.986+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:03.986+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Submitting ShuffleMapStage 79 (MapPartitionsRDD[157] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179), which has no missing parents
[2025-06-05T22:05:03.991+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO MemoryStore: Block broadcast_58 stored as values in memory (estimated size 27.6 KiB, free 432.0 MiB)
[2025-06-05T22:05:03.993+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 432.0 MiB)
[2025-06-05T22:05:03.993+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on 10.255.255.254:45911 (size: 10.7 KiB, free: 434.0 MiB)
[2025-06-05T22:05:03.994+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:03.995+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 79 (MapPartitionsRDD[157] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:03.995+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSchedulerImpl: Adding task set 79.0 with 1 tasks resource profile 0
[2025-06-05T22:05:03.997+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO TaskSetManager: Starting task 0.0 in stage 79.0 (TID 46) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9601 bytes)
[2025-06-05T22:05:03.998+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:03 INFO Executor: Running task 0.0 in stage 79.0 (TID 46)
[2025-06-05T22:05:04.009+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T22:05:04.113+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO Executor: Finished task 0.0 in stage 79.0 (TID 46). 2199 bytes result sent to driver
[2025-06-05T22:05:04.114+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO TaskSetManager: Finished task 0.0 in stage 79.0 (TID 46) in 118 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:04.115+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO TaskSchedulerImpl: Removed TaskSet 79.0, whose tasks have all completed, from pool
[2025-06-05T22:05:04.115+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO DAGScheduler: ShuffleMapStage 79 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) finished in 0.128 s
[2025-06-05T22:05:04.115+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T22:05:04.116+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO DAGScheduler: running: Set()
[2025-06-05T22:05:04.116+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO DAGScheduler: waiting: Set()
[2025-06-05T22:05:04.116+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO DAGScheduler: failed: Set()
[2025-06-05T22:05:04.127+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO ShufflePartitionsUtil: For shuffle(22), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-06-05T22:05:04.145+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO DAGScheduler: Registering RDD 160 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) as input to shuffle 23
[2025-06-05T22:05:04.145+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO DAGScheduler: Got map stage job 47 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) with 1 output partitions
[2025-06-05T22:05:04.146+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO DAGScheduler: Final stage: ShuffleMapStage 81 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179)
[2025-06-05T22:05:04.146+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 80)
[2025-06-05T22:05:04.146+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:04.146+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO DAGScheduler: Submitting ShuffleMapStage 81 (MapPartitionsRDD[160] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179), which has no missing parents
[2025-06-05T22:05:04.151+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO MemoryStore: Block broadcast_59 stored as values in memory (estimated size 45.0 KiB, free 431.9 MiB)
[2025-06-05T22:05:04.152+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 20.5 KiB, free 431.9 MiB)
[2025-06-05T22:05:04.152+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on 10.255.255.254:45911 (size: 20.5 KiB, free: 434.0 MiB)
[2025-06-05T22:05:04.153+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:04.153+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 81 (MapPartitionsRDD[160] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:04.153+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO TaskSchedulerImpl: Adding task set 81.0 with 1 tasks resource profile 0
[2025-06-05T22:05:04.154+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO TaskSetManager: Starting task 0.0 in stage 81.0 (TID 47) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8988 bytes)
[2025-06-05T22:05:04.155+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO Executor: Running task 0.0 in stage 81.0 (TID 47)
[2025-06-05T22:05:04.161+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO ShuffleBlockFetcherIterator: Getting 1 (13.5 KiB) non-empty blocks including 1 (13.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T22:05:04.162+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T22:05:04.169+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO Executor: Finished task 0.0 in stage 81.0 (TID 47). 5314 bytes result sent to driver
[2025-06-05T22:05:04.170+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO TaskSetManager: Finished task 0.0 in stage 81.0 (TID 47) in 15 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:04.170+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO TaskSchedulerImpl: Removed TaskSet 81.0, whose tasks have all completed, from pool
[2025-06-05T22:05:04.170+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO DAGScheduler: ShuffleMapStage 81 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) finished in 0.021 s
[2025-06-05T22:05:04.171+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T22:05:04.171+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO DAGScheduler: running: Set()
[2025-06-05T22:05:04.171+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO DAGScheduler: waiting: Set()
[2025-06-05T22:05:04.171+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO DAGScheduler: failed: Set()
[2025-06-05T22:05:04.187+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO SparkContext: Starting job: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179
[2025-06-05T22:05:04.188+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO DAGScheduler: Got job 48 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) with 1 output partitions
[2025-06-05T22:05:04.189+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO DAGScheduler: Final stage: ResultStage 84 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179)
[2025-06-05T22:05:04.189+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 83)
[2025-06-05T22:05:04.189+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:04.189+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO DAGScheduler: Submitting ResultStage 84 (MapPartitionsRDD[163] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179), which has no missing parents
[2025-06-05T22:05:04.191+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 16.9 KiB, free 431.9 MiB)
[2025-06-05T22:05:04.202+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 431.9 MiB)
[2025-06-05T22:05:04.202+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on 10.255.255.254:45911 (size: 7.3 KiB, free: 434.0 MiB)
[2025-06-05T22:05:04.203+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO BlockManagerInfo: Removed broadcast_58_piece0 on 10.255.255.254:45911 in memory (size: 10.7 KiB, free: 434.0 MiB)
[2025-06-05T22:05:04.203+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:04.203+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 84 (MapPartitionsRDD[163] at first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:04.203+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO TaskSchedulerImpl: Adding task set 84.0 with 1 tasks resource profile 0
[2025-06-05T22:05:04.204+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO BlockManagerInfo: Removed broadcast_57_piece0 on 10.255.255.254:45911 in memory (size: 8.6 KiB, free: 434.0 MiB)
[2025-06-05T22:05:04.205+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO TaskSetManager: Starting task 0.0 in stage 84.0 (TID 48) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8999 bytes)
[2025-06-05T22:05:04.205+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO Executor: Running task 0.0 in stage 84.0 (TID 48)
[2025-06-05T22:05:04.206+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO BlockManagerInfo: Removed broadcast_55_piece0 on 10.255.255.254:45911 in memory (size: 16.6 KiB, free: 434.0 MiB)
[2025-06-05T22:05:04.208+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO BlockManagerInfo: Removed broadcast_59_piece0 on 10.255.255.254:45911 in memory (size: 20.5 KiB, free: 434.1 MiB)
[2025-06-05T22:05:04.209+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T22:05:04.209+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T22:05:04.213+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO Executor: Finished task 0.0 in stage 84.0 (TID 48). 3995 bytes result sent to driver
[2025-06-05T22:05:04.216+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO TaskSetManager: Finished task 0.0 in stage 84.0 (TID 48) in 12 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:04.217+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO TaskSchedulerImpl: Removed TaskSet 84.0, whose tasks have all completed, from pool
[2025-06-05T22:05:04.221+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO DAGScheduler: ResultStage 84 (first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179) finished in 0.029 s
[2025-06-05T22:05:04.222+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO DAGScheduler: Job 48 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T22:05:04.222+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 84: Stage finished
[2025-06-05T22:05:04.222+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO DAGScheduler: Job 48 finished: first at /home/leoja/airflow/spark_jobs/compute_quadrants.py:179, took 0.032017 s
[2025-06-05T22:05:04.826+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:04 INFO BlockManagerInfo: Removed broadcast_60_piece0 on 10.255.255.254:45911 in memory (size: 7.3 KiB, free: 434.1 MiB)
[2025-06-05T22:05:06.028+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO FileSourceStrategy: Pushed Filters:
[2025-06-05T22:05:06.028+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO FileSourceStrategy: Post-Scan Filters:
[2025-06-05T22:05:06.035+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.040+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.063+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.070+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.072+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.073+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.074+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.075+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.086+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.087+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.087+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.087+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.087+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.087+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.088+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.104+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[2025-06-05T22:05:06.144+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO CodeGenerator: Code generated in 15.027353 ms
[2025-06-05T22:05:06.147+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO MemoryStore: Block broadcast_61 stored as values in memory (estimated size 202.1 KiB, free 431.9 MiB)
[2025-06-05T22:05:06.154+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 35.3 KiB, free 431.9 MiB)
[2025-06-05T22:05:06.155+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on 10.255.255.254:45911 (size: 35.3 KiB, free: 434.0 MiB)
[2025-06-05T22:05:06.156+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO SparkContext: Created broadcast 61 from parquet at NativeMethodAccessorImpl.java:0
[2025-06-05T22:05:06.157+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-06-05T22:05:06.169+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-06-05T22:05:06.170+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: Got job 49 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-06-05T22:05:06.171+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: Final stage: ResultStage 85 (parquet at NativeMethodAccessorImpl.java:0)
[2025-06-05T22:05:06.171+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T22:05:06.171+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:06.171+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: Submitting ResultStage 85 (MapPartitionsRDD[169] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-06-05T22:05:06.173+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO MemoryStore: Block broadcast_62 stored as values in memory (estimated size 22.7 KiB, free 431.8 MiB)
[2025-06-05T22:05:06.174+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 431.8 MiB)
[2025-06-05T22:05:06.174+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on 10.255.255.254:45911 (size: 9.2 KiB, free: 434.0 MiB)
[2025-06-05T22:05:06.174+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:06.175+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 85 (MapPartitionsRDD[169] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:06.175+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO TaskSchedulerImpl: Adding task set 85.0 with 1 tasks resource profile 0
[2025-06-05T22:05:06.176+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO TaskSetManager: Starting task 0.0 in stage 85.0 (TID 49) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9612 bytes)
[2025-06-05T22:05:06.177+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO Executor: Running task 0.0 in stage 85.0 (TID 49)
[2025-06-05T22:05:06.198+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO CodeGenerator: Code generated in 17.434117 ms
[2025-06-05T22:05:06.201+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T22:05:06.212+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO Executor: Finished task 0.0 in stage 85.0 (TID 49). 10024 bytes result sent to driver
[2025-06-05T22:05:06.214+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO TaskSetManager: Finished task 0.0 in stage 85.0 (TID 49) in 38 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:06.214+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO TaskSchedulerImpl: Removed TaskSet 85.0, whose tasks have all completed, from pool
[2025-06-05T22:05:06.214+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: ResultStage 85 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.043 s
[2025-06-05T22:05:06.215+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: Job 49 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T22:05:06.215+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 85: Stage finished
[2025-06-05T22:05:06.215+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: Job 49 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.045286 s
[2025-06-05T22:05:06.217+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: Registering RDD 170 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 24
[2025-06-05T22:05:06.217+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: Got map stage job 50 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-06-05T22:05:06.217+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: Final stage: ShuffleMapStage 86 (parquet at NativeMethodAccessorImpl.java:0)
[2025-06-05T22:05:06.217+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T22:05:06.217+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:06.217+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: Submitting ShuffleMapStage 86 (MapPartitionsRDD[170] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-06-05T22:05:06.220+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO MemoryStore: Block broadcast_63 stored as values in memory (estimated size 30.9 KiB, free 431.8 MiB)
[2025-06-05T22:05:06.221+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 11.2 KiB, free 431.8 MiB)
[2025-06-05T22:05:06.221+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on 10.255.255.254:45911 (size: 11.2 KiB, free: 434.0 MiB)
[2025-06-05T22:05:06.222+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:06.222+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 86 (MapPartitionsRDD[170] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:06.222+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO TaskSchedulerImpl: Adding task set 86.0 with 1 tasks resource profile 0
[2025-06-05T22:05:06.223+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO TaskSetManager: Starting task 0.0 in stage 86.0 (TID 50) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9601 bytes)
[2025-06-05T22:05:06.224+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO Executor: Running task 0.0 in stage 86.0 (TID 50)
[2025-06-05T22:05:06.229+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T22:05:06.259+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO BlockManagerInfo: Removed broadcast_62_piece0 on 10.255.255.254:45911 in memory (size: 9.2 KiB, free: 434.0 MiB)
[2025-06-05T22:05:06.288+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO Executor: Finished task 0.0 in stage 86.0 (TID 50). 2242 bytes result sent to driver
[2025-06-05T22:05:06.289+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO TaskSetManager: Finished task 0.0 in stage 86.0 (TID 50) in 65 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:06.289+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO TaskSchedulerImpl: Removed TaskSet 86.0, whose tasks have all completed, from pool
[2025-06-05T22:05:06.289+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: ShuffleMapStage 86 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.071 s
[2025-06-05T22:05:06.289+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T22:05:06.289+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: running: Set()
[2025-06-05T22:05:06.290+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: waiting: Set()
[2025-06-05T22:05:06.290+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: failed: Set()
[2025-06-05T22:05:06.337+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.337+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.337+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.337+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.338+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.338+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.339+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.348+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.349+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.349+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.349+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.349+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.349+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.349+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.356+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO ShufflePartitionsUtil: For shuffle(24), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-06-05T22:05:06.388+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO CodeGenerator: Code generated in 10.482913 ms
[2025-06-05T22:05:06.393+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: Registering RDD 173 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 25
[2025-06-05T22:05:06.393+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: Got map stage job 51 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-06-05T22:05:06.393+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: Final stage: ShuffleMapStage 88 (parquet at NativeMethodAccessorImpl.java:0)
[2025-06-05T22:05:06.394+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 87)
[2025-06-05T22:05:06.394+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:06.394+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: Submitting ShuffleMapStage 88 (MapPartitionsRDD[173] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-06-05T22:05:06.397+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO MemoryStore: Block broadcast_64 stored as values in memory (estimated size 32.7 KiB, free 431.8 MiB)
[2025-06-05T22:05:06.398+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 15.6 KiB, free 431.8 MiB)
[2025-06-05T22:05:06.398+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on 10.255.255.254:45911 (size: 15.6 KiB, free: 434.0 MiB)
[2025-06-05T22:05:06.398+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:06.399+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 88 (MapPartitionsRDD[173] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:06.399+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO TaskSchedulerImpl: Adding task set 88.0 with 1 tasks resource profile 0
[2025-06-05T22:05:06.400+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO TaskSetManager: Starting task 0.0 in stage 88.0 (TID 51) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8988 bytes)
[2025-06-05T22:05:06.401+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO Executor: Running task 0.0 in stage 88.0 (TID 51)
[2025-06-05T22:05:06.406+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO ShuffleBlockFetcherIterator: Getting 1 (22.6 KiB) non-empty blocks including 1 (22.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T22:05:06.407+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T22:05:06.417+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO CodeGenerator: Code generated in 10.206135 ms
[2025-06-05T22:05:06.425+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO Executor: Finished task 0.0 in stage 88.0 (TID 51). 4884 bytes result sent to driver
[2025-06-05T22:05:06.426+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO TaskSetManager: Finished task 0.0 in stage 88.0 (TID 51) in 26 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:06.427+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO TaskSchedulerImpl: Removed TaskSet 88.0, whose tasks have all completed, from pool
[2025-06-05T22:05:06.427+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: ShuffleMapStage 88 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.032 s
[2025-06-05T22:05:06.428+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T22:05:06.428+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: running: Set()
[2025-06-05T22:05:06.428+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: waiting: Set()
[2025-06-05T22:05:06.428+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: failed: Set()
[2025-06-05T22:05:06.459+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.459+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.460+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.460+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.461+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.461+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.461+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.465+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.466+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.466+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.466+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.466+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.466+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.466+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:06.517+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-06-05T22:05:06.534+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-06-05T22:05:06.534+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-06-05T22:05:06.535+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-06-05T22:05:06.535+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-06-05T22:05:06.535+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-06-05T22:05:06.535+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-06-05T22:05:06.632+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO BlockManagerInfo: Removed broadcast_64_piece0 on 10.255.255.254:45911 in memory (size: 15.6 KiB, free: 434.0 MiB)
[2025-06-05T22:05:06.704+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO CodeGenerator: Code generated in 60.35588 ms
[2025-06-05T22:05:06.723+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO CodeGenerator: Code generated in 9.951856 ms
[2025-06-05T22:05:06.744+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO CodeGenerator: Code generated in 12.702689 ms
[2025-06-05T22:05:06.760+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO CodeGenerator: Code generated in 12.647535 ms
[2025-06-05T22:05:06.772+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO CodeGenerator: Code generated in 8.183095 ms
[2025-06-05T22:05:06.785+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO CodeGenerator: Code generated in 9.803678 ms
[2025-06-05T22:05:06.796+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO CodeGenerator: Code generated in 7.934613 ms
[2025-06-05T22:05:06.805+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO CodeGenerator: Code generated in 7.120772 ms
[2025-06-05T22:05:06.903+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-06-05T22:05:06.905+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: Got job 52 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-06-05T22:05:06.906+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: Final stage: ResultStage 91 (parquet at NativeMethodAccessorImpl.java:0)
[2025-06-05T22:05:06.906+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 90)
[2025-06-05T22:05:06.906+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:06.906+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: Submitting ResultStage 91 (MapPartitionsRDD[191] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-06-05T22:05:06.938+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO MemoryStore: Block broadcast_65 stored as values in memory (estimated size 360.0 KiB, free 431.5 MiB)
[2025-06-05T22:05:06.940+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 116.5 KiB, free 431.4 MiB)
[2025-06-05T22:05:06.940+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on 10.255.255.254:45911 (size: 116.5 KiB, free: 433.9 MiB)
[2025-06-05T22:05:06.941+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO SparkContext: Created broadcast 65 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:06.941+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 91 (MapPartitionsRDD[191] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:06.941+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO TaskSchedulerImpl: Adding task set 91.0 with 1 tasks resource profile 0
[2025-06-05T22:05:06.945+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO TaskSetManager: Starting task 0.0 in stage 91.0 (TID 52) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 9275 bytes)
[2025-06-05T22:05:06.946+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO Executor: Running task 0.0 in stage 91.0 (TID 52)
[2025-06-05T22:05:06.992+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-06-05T22:05:06.993+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-06-05T22:05:06.993+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-06-05T22:05:06.993+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-06-05T22:05:06.993+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-06-05T22:05:06.994+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-06-05T22:05:07.002+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodecConfig: Compression: SNAPPY
[2025-06-05T22:05:07.004+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodecConfig: Compression: SNAPPY
[2025-06-05T22:05:07.034+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-06-05T22:05:07.050+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2025-06-05T22:05:07.050+0200] {spark_submit.py:521} INFO - {
[2025-06-05T22:05:07.050+0200] {spark_submit.py:521} INFO - "type" : "struct",
[2025-06-05T22:05:07.050+0200] {spark_submit.py:521} INFO - "fields" : [ {
[2025-06-05T22:05:07.050+0200] {spark_submit.py:521} INFO - "name" : "date",
[2025-06-05T22:05:07.050+0200] {spark_submit.py:521} INFO - "type" : "date",
[2025-06-05T22:05:07.051+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T22:05:07.051+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.051+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.051+0200] {spark_submit.py:521} INFO - "name" : "INFLATION",
[2025-06-05T22:05:07.051+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T22:05:07.051+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T22:05:07.051+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.051+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.051+0200] {spark_submit.py:521} INFO - "name" : "UNEMPLOYMENT",
[2025-06-05T22:05:07.051+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T22:05:07.052+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T22:05:07.052+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.052+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.052+0200] {spark_submit.py:521} INFO - "name" : "CONSUMER_SENTIMENT",
[2025-06-05T22:05:07.052+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T22:05:07.052+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T22:05:07.052+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.052+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.052+0200] {spark_submit.py:521} INFO - "name" : "High_Yield_Bond_SPREAD",
[2025-06-05T22:05:07.052+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T22:05:07.052+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T22:05:07.053+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.053+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.053+0200] {spark_submit.py:521} INFO - "name" : "10-2Year_Treasury_Yield_Bond",
[2025-06-05T22:05:07.053+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T22:05:07.053+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T22:05:07.053+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.053+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.053+0200] {spark_submit.py:521} INFO - "name" : "TAUX_FED",
[2025-06-05T22:05:07.053+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T22:05:07.053+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T22:05:07.053+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.054+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.054+0200] {spark_submit.py:521} INFO - "name" : "INFLATION_delta",
[2025-06-05T22:05:07.054+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T22:05:07.054+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T22:05:07.054+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.054+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.054+0200] {spark_submit.py:521} INFO - "name" : "UNEMPLOYMENT_delta",
[2025-06-05T22:05:07.054+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T22:05:07.054+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T22:05:07.054+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.054+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.055+0200] {spark_submit.py:521} INFO - "name" : "CONSUMER_SENTIMENT_delta",
[2025-06-05T22:05:07.055+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T22:05:07.055+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T22:05:07.055+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.055+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.055+0200] {spark_submit.py:521} INFO - "name" : "High_Yield_Bond_SPREAD_delta",
[2025-06-05T22:05:07.055+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T22:05:07.055+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T22:05:07.056+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.056+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.056+0200] {spark_submit.py:521} INFO - "name" : "10-2Year_Treasury_Yield_Bond_delta",
[2025-06-05T22:05:07.056+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T22:05:07.056+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T22:05:07.056+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.056+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.056+0200] {spark_submit.py:521} INFO - "name" : "TAUX_FED_delta",
[2025-06-05T22:05:07.056+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T22:05:07.056+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T22:05:07.057+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.057+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.057+0200] {spark_submit.py:521} INFO - "name" : "INFLATION_zscore",
[2025-06-05T22:05:07.057+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T22:05:07.057+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T22:05:07.057+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.057+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.057+0200] {spark_submit.py:521} INFO - "name" : "UNEMPLOYMENT_zscore",
[2025-06-05T22:05:07.058+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T22:05:07.058+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T22:05:07.058+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.058+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.058+0200] {spark_submit.py:521} INFO - "name" : "CONSUMER_SENTIMENT_zscore",
[2025-06-05T22:05:07.058+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T22:05:07.059+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T22:05:07.059+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.059+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.059+0200] {spark_submit.py:521} INFO - "name" : "High_Yield_Bond_SPREAD_zscore",
[2025-06-05T22:05:07.059+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T22:05:07.059+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T22:05:07.060+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.060+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.060+0200] {spark_submit.py:521} INFO - "name" : "10-2Year_Treasury_Yield_Bond_zscore",
[2025-06-05T22:05:07.060+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T22:05:07.060+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T22:05:07.060+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.060+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.060+0200] {spark_submit.py:521} INFO - "name" : "TAUX_FED_zscore",
[2025-06-05T22:05:07.060+0200] {spark_submit.py:521} INFO - "type" : "double",
[2025-06-05T22:05:07.061+0200] {spark_submit.py:521} INFO - "nullable" : true,
[2025-06-05T22:05:07.061+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.061+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.061+0200] {spark_submit.py:521} INFO - "name" : "INFLATION_pos_score",
[2025-06-05T22:05:07.061+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T22:05:07.061+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T22:05:07.061+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.061+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.061+0200] {spark_submit.py:521} INFO - "name" : "UNEMPLOYMENT_pos_score",
[2025-06-05T22:05:07.061+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T22:05:07.061+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T22:05:07.061+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.061+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.061+0200] {spark_submit.py:521} INFO - "name" : "CONSUMER_SENTIMENT_pos_score",
[2025-06-05T22:05:07.062+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T22:05:07.062+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T22:05:07.062+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.062+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.062+0200] {spark_submit.py:521} INFO - "name" : "High_Yield_Bond_SPREAD_pos_score",
[2025-06-05T22:05:07.062+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T22:05:07.062+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T22:05:07.062+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.062+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.062+0200] {spark_submit.py:521} INFO - "name" : "10-2Year_Treasury_Yield_Bond_pos_score",
[2025-06-05T22:05:07.062+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T22:05:07.062+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T22:05:07.062+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.062+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.063+0200] {spark_submit.py:521} INFO - "name" : "TAUX_FED_pos_score",
[2025-06-05T22:05:07.063+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T22:05:07.063+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T22:05:07.063+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.063+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.063+0200] {spark_submit.py:521} INFO - "name" : "INFLATION_var_score",
[2025-06-05T22:05:07.063+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T22:05:07.063+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T22:05:07.063+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.063+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.063+0200] {spark_submit.py:521} INFO - "name" : "UNEMPLOYMENT_var_score",
[2025-06-05T22:05:07.063+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T22:05:07.063+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T22:05:07.064+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.064+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.064+0200] {spark_submit.py:521} INFO - "name" : "CONSUMER_SENTIMENT_var_score",
[2025-06-05T22:05:07.064+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T22:05:07.064+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T22:05:07.064+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.064+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.064+0200] {spark_submit.py:521} INFO - "name" : "High_Yield_Bond_SPREAD_var_score",
[2025-06-05T22:05:07.064+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T22:05:07.064+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T22:05:07.064+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.064+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.064+0200] {spark_submit.py:521} INFO - "name" : "10-2Year_Treasury_Yield_Bond_var_score",
[2025-06-05T22:05:07.064+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T22:05:07.065+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T22:05:07.065+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.065+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.065+0200] {spark_submit.py:521} INFO - "name" : "TAUX_FED_var_score",
[2025-06-05T22:05:07.065+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T22:05:07.065+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T22:05:07.065+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.065+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.065+0200] {spark_submit.py:521} INFO - "name" : "INFLATION_combined",
[2025-06-05T22:05:07.065+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T22:05:07.065+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T22:05:07.065+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.065+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.065+0200] {spark_submit.py:521} INFO - "name" : "UNEMPLOYMENT_combined",
[2025-06-05T22:05:07.065+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T22:05:07.066+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T22:05:07.066+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.066+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.066+0200] {spark_submit.py:521} INFO - "name" : "CONSUMER_SENTIMENT_combined",
[2025-06-05T22:05:07.066+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T22:05:07.066+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T22:05:07.066+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.066+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.066+0200] {spark_submit.py:521} INFO - "name" : "High_Yield_Bond_SPREAD_combined",
[2025-06-05T22:05:07.066+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T22:05:07.066+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T22:05:07.066+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.066+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.066+0200] {spark_submit.py:521} INFO - "name" : "10-2Year_Treasury_Yield_Bond_combined",
[2025-06-05T22:05:07.067+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T22:05:07.067+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T22:05:07.067+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.067+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.067+0200] {spark_submit.py:521} INFO - "name" : "TAUX_FED_combined",
[2025-06-05T22:05:07.067+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T22:05:07.067+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T22:05:07.067+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.067+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.068+0200] {spark_submit.py:521} INFO - "name" : "score_Q1",
[2025-06-05T22:05:07.068+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T22:05:07.068+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T22:05:07.068+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.068+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.068+0200] {spark_submit.py:521} INFO - "name" : "score_Q2",
[2025-06-05T22:05:07.068+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T22:05:07.068+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T22:05:07.068+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.069+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.069+0200] {spark_submit.py:521} INFO - "name" : "score_Q3",
[2025-06-05T22:05:07.069+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T22:05:07.069+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T22:05:07.069+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.069+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.069+0200] {spark_submit.py:521} INFO - "name" : "score_Q4",
[2025-06-05T22:05:07.069+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T22:05:07.069+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T22:05:07.070+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.070+0200] {spark_submit.py:521} INFO - }, {
[2025-06-05T22:05:07.070+0200] {spark_submit.py:521} INFO - "name" : "assigned_quadrant",
[2025-06-05T22:05:07.070+0200] {spark_submit.py:521} INFO - "type" : "integer",
[2025-06-05T22:05:07.070+0200] {spark_submit.py:521} INFO - "nullable" : false,
[2025-06-05T22:05:07.070+0200] {spark_submit.py:521} INFO - "metadata" : { }
[2025-06-05T22:05:07.070+0200] {spark_submit.py:521} INFO - } ]
[2025-06-05T22:05:07.070+0200] {spark_submit.py:521} INFO - }
[2025-06-05T22:05:07.070+0200] {spark_submit.py:521} INFO - and corresponding Parquet message type:
[2025-06-05T22:05:07.071+0200] {spark_submit.py:521} INFO - message spark_schema {
[2025-06-05T22:05:07.071+0200] {spark_submit.py:521} INFO - optional int32 date (DATE);
[2025-06-05T22:05:07.071+0200] {spark_submit.py:521} INFO - optional double INFLATION;
[2025-06-05T22:05:07.071+0200] {spark_submit.py:521} INFO - optional double UNEMPLOYMENT;
[2025-06-05T22:05:07.071+0200] {spark_submit.py:521} INFO - optional double CONSUMER_SENTIMENT;
[2025-06-05T22:05:07.071+0200] {spark_submit.py:521} INFO - optional double High_Yield_Bond_SPREAD;
[2025-06-05T22:05:07.071+0200] {spark_submit.py:521} INFO - optional double 10-2Year_Treasury_Yield_Bond;
[2025-06-05T22:05:07.071+0200] {spark_submit.py:521} INFO - optional double TAUX_FED;
[2025-06-05T22:05:07.072+0200] {spark_submit.py:521} INFO - optional double INFLATION_delta;
[2025-06-05T22:05:07.072+0200] {spark_submit.py:521} INFO - optional double UNEMPLOYMENT_delta;
[2025-06-05T22:05:07.072+0200] {spark_submit.py:521} INFO - optional double CONSUMER_SENTIMENT_delta;
[2025-06-05T22:05:07.072+0200] {spark_submit.py:521} INFO - optional double High_Yield_Bond_SPREAD_delta;
[2025-06-05T22:05:07.072+0200] {spark_submit.py:521} INFO - optional double 10-2Year_Treasury_Yield_Bond_delta;
[2025-06-05T22:05:07.072+0200] {spark_submit.py:521} INFO - optional double TAUX_FED_delta;
[2025-06-05T22:05:07.072+0200] {spark_submit.py:521} INFO - optional double INFLATION_zscore;
[2025-06-05T22:05:07.072+0200] {spark_submit.py:521} INFO - optional double UNEMPLOYMENT_zscore;
[2025-06-05T22:05:07.072+0200] {spark_submit.py:521} INFO - optional double CONSUMER_SENTIMENT_zscore;
[2025-06-05T22:05:07.073+0200] {spark_submit.py:521} INFO - optional double High_Yield_Bond_SPREAD_zscore;
[2025-06-05T22:05:07.073+0200] {spark_submit.py:521} INFO - optional double 10-2Year_Treasury_Yield_Bond_zscore;
[2025-06-05T22:05:07.073+0200] {spark_submit.py:521} INFO - optional double TAUX_FED_zscore;
[2025-06-05T22:05:07.073+0200] {spark_submit.py:521} INFO - required int32 INFLATION_pos_score;
[2025-06-05T22:05:07.073+0200] {spark_submit.py:521} INFO - required int32 UNEMPLOYMENT_pos_score;
[2025-06-05T22:05:07.073+0200] {spark_submit.py:521} INFO - required int32 CONSUMER_SENTIMENT_pos_score;
[2025-06-05T22:05:07.073+0200] {spark_submit.py:521} INFO - required int32 High_Yield_Bond_SPREAD_pos_score;
[2025-06-05T22:05:07.073+0200] {spark_submit.py:521} INFO - required int32 10-2Year_Treasury_Yield_Bond_pos_score;
[2025-06-05T22:05:07.073+0200] {spark_submit.py:521} INFO - required int32 TAUX_FED_pos_score;
[2025-06-05T22:05:07.074+0200] {spark_submit.py:521} INFO - required int32 INFLATION_var_score;
[2025-06-05T22:05:07.074+0200] {spark_submit.py:521} INFO - required int32 UNEMPLOYMENT_var_score;
[2025-06-05T22:05:07.074+0200] {spark_submit.py:521} INFO - required int32 CONSUMER_SENTIMENT_var_score;
[2025-06-05T22:05:07.074+0200] {spark_submit.py:521} INFO - required int32 High_Yield_Bond_SPREAD_var_score;
[2025-06-05T22:05:07.074+0200] {spark_submit.py:521} INFO - required int32 10-2Year_Treasury_Yield_Bond_var_score;
[2025-06-05T22:05:07.074+0200] {spark_submit.py:521} INFO - required int32 TAUX_FED_var_score;
[2025-06-05T22:05:07.074+0200] {spark_submit.py:521} INFO - required int32 INFLATION_combined;
[2025-06-05T22:05:07.074+0200] {spark_submit.py:521} INFO - required int32 UNEMPLOYMENT_combined;
[2025-06-05T22:05:07.074+0200] {spark_submit.py:521} INFO - required int32 CONSUMER_SENTIMENT_combined;
[2025-06-05T22:05:07.075+0200] {spark_submit.py:521} INFO - required int32 High_Yield_Bond_SPREAD_combined;
[2025-06-05T22:05:07.075+0200] {spark_submit.py:521} INFO - required int32 10-2Year_Treasury_Yield_Bond_combined;
[2025-06-05T22:05:07.075+0200] {spark_submit.py:521} INFO - required int32 TAUX_FED_combined;
[2025-06-05T22:05:07.075+0200] {spark_submit.py:521} INFO - required int32 score_Q1;
[2025-06-05T22:05:07.075+0200] {spark_submit.py:521} INFO - required int32 score_Q2;
[2025-06-05T22:05:07.075+0200] {spark_submit.py:521} INFO - required int32 score_Q3;
[2025-06-05T22:05:07.075+0200] {spark_submit.py:521} INFO - required int32 score_Q4;
[2025-06-05T22:05:07.075+0200] {spark_submit.py:521} INFO - required int32 assigned_quadrant;
[2025-06-05T22:05:07.076+0200] {spark_submit.py:521} INFO - }
[2025-06-05T22:05:07.076+0200] {spark_submit.py:521} INFO - 
[2025-06-05T22:05:07.076+0200] {spark_submit.py:521} INFO - 
[2025-06-05T22:05:07.096+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodecPool: Got brand-new compressor [.snappy]
[2025-06-05T22:05:07.172+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO ShuffleBlockFetcherIterator: Getting 1 (7.6 KiB) non-empty blocks including 1 (7.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T22:05:07.172+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T22:05:07.182+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 8.960697 ms
[2025-06-05T22:05:07.213+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 11.283795 ms
[2025-06-05T22:05:07.237+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 8.311153 ms
[2025-06-05T22:05:07.253+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 8.29622 ms
[2025-06-05T22:05:07.265+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 8.642391 ms
[2025-06-05T22:05:07.278+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 10.925026 ms
[2025-06-05T22:05:07.299+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 14.193369 ms
[2025-06-05T22:05:07.320+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 5.601952 ms
[2025-06-05T22:05:07.351+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 14.737345 ms
[2025-06-05T22:05:07.367+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 10.986618 ms
[2025-06-05T22:05:07.378+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 6.922302 ms
[2025-06-05T22:05:07.389+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 7.331612 ms
[2025-06-05T22:05:07.399+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 6.481206 ms
[2025-06-05T22:05:07.412+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 12.32588 ms
[2025-06-05T22:05:07.428+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 10.027182 ms
[2025-06-05T22:05:07.455+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 12.556962 ms
[2025-06-05T22:05:07.466+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 5.959744 ms
[2025-06-05T22:05:07.478+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 7.860304 ms
[2025-06-05T22:05:07.487+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 7.257878 ms
[2025-06-05T22:05:07.503+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 11.098331 ms
[2025-06-05T22:05:07.543+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 8.80026 ms
[2025-06-05T22:05:07.556+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 7.616101 ms
[2025-06-05T22:05:07.566+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 6.243317 ms
[2025-06-05T22:05:07.580+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 12.416432 ms
[2025-06-05T22:05:07.595+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 10.689344 ms
[2025-06-05T22:05:07.633+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 10.980699 ms
[2025-06-05T22:05:07.644+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 6.010706 ms
[2025-06-05T22:05:07.654+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 5.866267 ms
[2025-06-05T22:05:07.668+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 12.096639 ms
[2025-06-05T22:05:07.686+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 13.036924 ms
[2025-06-05T22:05:07.735+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 14.95527 ms
[2025-06-05T22:05:07.747+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 5.003624 ms
[2025-06-05T22:05:07.755+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 4.67387 ms
[2025-06-05T22:05:07.766+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 10.321366 ms
[2025-06-05T22:05:07.789+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 17.446629 ms
[2025-06-05T22:05:07.828+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 12.697426 ms
[2025-06-05T22:05:07.875+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:07 INFO CodeGenerator: Code generated in 43.356146 ms
[2025-06-05T22:05:08.112+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO FileOutputCommitter: Saved output of task 'attempt_20250605220506424026142200689311_0091_m_000000_52' to file:/home/leoja/airflow/data/quadrants.parquet_tmp_parquet/_temporary/0/task_20250605220506424026142200689311_0091_m_000000
[2025-06-05T22:05:08.113+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO SparkHadoopMapRedUtil: attempt_20250605220506424026142200689311_0091_m_000000_52: Committed. Elapsed time: 1 ms.
[2025-06-05T22:05:08.121+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO Executor: Finished task 0.0 in stage 91.0 (TID 52). 8043 bytes result sent to driver
[2025-06-05T22:05:08.123+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO TaskSetManager: Finished task 0.0 in stage 91.0 (TID 52) in 1180 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:08.123+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO TaskSchedulerImpl: Removed TaskSet 91.0, whose tasks have all completed, from pool
[2025-06-05T22:05:08.124+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: ResultStage 91 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.217 s
[2025-06-05T22:05:08.124+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: Job 52 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T22:05:08.124+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 91: Stage finished
[2025-06-05T22:05:08.125+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: Job 52 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.222352 s
[2025-06-05T22:05:08.128+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO FileFormatWriter: Start to commit write Job a62c3b00-4029-4ec7-9fe4-00165bfdf8cd.
[2025-06-05T22:05:08.149+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO FileFormatWriter: Write Job a62c3b00-4029-4ec7-9fe4-00165bfdf8cd committed. Elapsed time: 19 ms.
[2025-06-05T22:05:08.153+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO FileFormatWriter: Finished processing stats for write job a62c3b00-4029-4ec7-9fe4-00165bfdf8cd.
[2025-06-05T22:05:08.157+0200] {spark_submit.py:521} INFO - > Gnr un fichier unique Parquet : /home/leoja/airflow/data/quadrants.parquet
[2025-06-05T22:05:08.344+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO FileSourceStrategy: Pushed Filters:
[2025-06-05T22:05:08.345+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO FileSourceStrategy: Post-Scan Filters:
[2025-06-05T22:05:08.346+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.349+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.349+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.349+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.349+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.350+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.350+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.350+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.355+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.356+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.356+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.356+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.356+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.356+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.356+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.393+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO BlockManagerInfo: Removed broadcast_65_piece0 on 10.255.255.254:45911 in memory (size: 116.5 KiB, free: 434.0 MiB)
[2025-06-05T22:05:08.405+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO MemoryStore: Block broadcast_66 stored as values in memory (estimated size 202.1 KiB, free 431.6 MiB)
[2025-06-05T22:05:08.415+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 35.3 KiB, free 431.6 MiB)
[2025-06-05T22:05:08.416+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on 10.255.255.254:45911 (size: 35.3 KiB, free: 434.0 MiB)
[2025-06-05T22:05:08.417+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO SparkContext: Created broadcast 66 from csv at NativeMethodAccessorImpl.java:0
[2025-06-05T22:05:08.418+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-06-05T22:05:08.435+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2025-06-05T22:05:08.436+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: Got job 53 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-06-05T22:05:08.437+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: Final stage: ResultStage 92 (csv at NativeMethodAccessorImpl.java:0)
[2025-06-05T22:05:08.437+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T22:05:08.437+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:08.437+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: Submitting ResultStage 92 (MapPartitionsRDD[197] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-06-05T22:05:08.439+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO MemoryStore: Block broadcast_67 stored as values in memory (estimated size 22.7 KiB, free 431.6 MiB)
[2025-06-05T22:05:08.453+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 431.6 MiB)
[2025-06-05T22:05:08.456+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on 10.255.255.254:45911 (size: 9.2 KiB, free: 434.0 MiB)
[2025-06-05T22:05:08.458+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO SparkContext: Created broadcast 67 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:08.458+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 92 (MapPartitionsRDD[197] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:08.459+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO TaskSchedulerImpl: Adding task set 92.0 with 1 tasks resource profile 0
[2025-06-05T22:05:08.459+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO TaskSetManager: Starting task 0.0 in stage 92.0 (TID 53) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9612 bytes)
[2025-06-05T22:05:08.461+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO Executor: Running task 0.0 in stage 92.0 (TID 53)
[2025-06-05T22:05:08.465+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T22:05:08.477+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO Executor: Finished task 0.0 in stage 92.0 (TID 53). 10024 bytes result sent to driver
[2025-06-05T22:05:08.478+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO TaskSetManager: Finished task 0.0 in stage 92.0 (TID 53) in 20 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:08.478+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO TaskSchedulerImpl: Removed TaskSet 92.0, whose tasks have all completed, from pool
[2025-06-05T22:05:08.478+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: ResultStage 92 (csv at NativeMethodAccessorImpl.java:0) finished in 0.041 s
[2025-06-05T22:05:08.479+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: Job 53 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T22:05:08.479+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 92: Stage finished
[2025-06-05T22:05:08.479+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: Job 53 finished: csv at NativeMethodAccessorImpl.java:0, took 0.043294 s
[2025-06-05T22:05:08.481+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: Registering RDD 198 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 26
[2025-06-05T22:05:08.482+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: Got map stage job 54 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-06-05T22:05:08.482+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: Final stage: ShuffleMapStage 93 (csv at NativeMethodAccessorImpl.java:0)
[2025-06-05T22:05:08.482+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: Parents of final stage: List()
[2025-06-05T22:05:08.482+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:08.482+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: Submitting ShuffleMapStage 93 (MapPartitionsRDD[198] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-06-05T22:05:08.485+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO MemoryStore: Block broadcast_68 stored as values in memory (estimated size 30.9 KiB, free 431.5 MiB)
[2025-06-05T22:05:08.486+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 11.2 KiB, free 431.5 MiB)
[2025-06-05T22:05:08.487+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on 10.255.255.254:45911 (size: 11.2 KiB, free: 434.0 MiB)
[2025-06-05T22:05:08.487+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO SparkContext: Created broadcast 68 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:08.488+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 93 (MapPartitionsRDD[198] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:08.488+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO TaskSchedulerImpl: Adding task set 93.0 with 1 tasks resource profile 0
[2025-06-05T22:05:08.489+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO TaskSetManager: Starting task 0.0 in stage 93.0 (TID 54) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9601 bytes)
[2025-06-05T22:05:08.492+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO Executor: Running task 0.0 in stage 93.0 (TID 54)
[2025-06-05T22:05:08.531+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 10.255.255.254:45911 in memory (size: 35.0 KiB, free: 434.0 MiB)
[2025-06-05T22:05:08.532+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO FileScanRDD: Reading File path: file:///home/leoja/airflow/data/Indicators.parquet, range: 0-13095, partition values: [empty row]
[2025-06-05T22:05:08.532+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO BlockManagerInfo: Removed broadcast_46_piece0 on 10.255.255.254:45911 in memory (size: 35.1 KiB, free: 434.0 MiB)
[2025-06-05T22:05:08.535+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 10.255.255.254:45911 in memory (size: 35.1 KiB, free: 434.1 MiB)
[2025-06-05T22:05:08.536+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 10.255.255.254:45911 in memory (size: 35.0 KiB, free: 434.1 MiB)
[2025-06-05T22:05:08.544+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 10.255.255.254:45911 in memory (size: 35.0 KiB, free: 434.1 MiB)
[2025-06-05T22:05:08.549+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 10.255.255.254:45911 in memory (size: 35.0 KiB, free: 434.2 MiB)
[2025-06-05T22:05:08.552+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 10.255.255.254:45911 in memory (size: 35.0 KiB, free: 434.2 MiB)
[2025-06-05T22:05:08.553+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO BlockManagerInfo: Removed broadcast_56_piece0 on 10.255.255.254:45911 in memory (size: 35.0 KiB, free: 434.2 MiB)
[2025-06-05T22:05:08.554+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO BlockManagerInfo: Removed broadcast_63_piece0 on 10.255.255.254:45911 in memory (size: 11.2 KiB, free: 434.2 MiB)
[2025-06-05T22:05:08.555+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO BlockManagerInfo: Removed broadcast_51_piece0 on 10.255.255.254:45911 in memory (size: 35.0 KiB, free: 434.3 MiB)
[2025-06-05T22:05:08.556+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 10.255.255.254:45911 in memory (size: 35.0 KiB, free: 434.3 MiB)
[2025-06-05T22:05:08.576+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO Executor: Finished task 0.0 in stage 93.0 (TID 54). 2199 bytes result sent to driver
[2025-06-05T22:05:08.577+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO TaskSetManager: Finished task 0.0 in stage 93.0 (TID 54) in 88 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:08.578+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO TaskSchedulerImpl: Removed TaskSet 93.0, whose tasks have all completed, from pool
[2025-06-05T22:05:08.578+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: ShuffleMapStage 93 (csv at NativeMethodAccessorImpl.java:0) finished in 0.095 s
[2025-06-05T22:05:08.578+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T22:05:08.578+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: running: Set()
[2025-06-05T22:05:08.578+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: waiting: Set()
[2025-06-05T22:05:08.578+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: failed: Set()
[2025-06-05T22:05:08.617+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.618+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.618+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.619+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.619+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.619+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.619+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.623+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.624+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.624+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.624+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.624+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.624+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.624+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.630+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO ShufflePartitionsUtil: For shuffle(26), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-06-05T22:05:08.653+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: Registering RDD 201 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 27
[2025-06-05T22:05:08.654+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: Got map stage job 55 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-06-05T22:05:08.654+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: Final stage: ShuffleMapStage 95 (csv at NativeMethodAccessorImpl.java:0)
[2025-06-05T22:05:08.654+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 94)
[2025-06-05T22:05:08.654+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:08.655+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: Submitting ShuffleMapStage 95 (MapPartitionsRDD[201] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-06-05T22:05:08.658+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO MemoryStore: Block broadcast_69 stored as values in memory (estimated size 32.7 KiB, free 433.8 MiB)
[2025-06-05T22:05:08.659+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 15.6 KiB, free 433.8 MiB)
[2025-06-05T22:05:08.659+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on 10.255.255.254:45911 (size: 15.6 KiB, free: 434.3 MiB)
[2025-06-05T22:05:08.660+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:08.660+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 95 (MapPartitionsRDD[201] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:08.660+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO TaskSchedulerImpl: Adding task set 95.0 with 1 tasks resource profile 0
[2025-06-05T22:05:08.661+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO TaskSetManager: Starting task 0.0 in stage 95.0 (TID 55) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 8988 bytes)
[2025-06-05T22:05:08.662+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO Executor: Running task 0.0 in stage 95.0 (TID 55)
[2025-06-05T22:05:08.669+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO ShuffleBlockFetcherIterator: Getting 1 (22.6 KiB) non-empty blocks including 1 (22.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T22:05:08.669+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T22:05:08.677+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO Executor: Finished task 0.0 in stage 95.0 (TID 55). 4884 bytes result sent to driver
[2025-06-05T22:05:08.678+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO TaskSetManager: Finished task 0.0 in stage 95.0 (TID 55) in 17 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:08.678+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO TaskSchedulerImpl: Removed TaskSet 95.0, whose tasks have all completed, from pool
[2025-06-05T22:05:08.679+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: ShuffleMapStage 95 (csv at NativeMethodAccessorImpl.java:0) finished in 0.022 s
[2025-06-05T22:05:08.679+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: looking for newly runnable stages
[2025-06-05T22:05:08.679+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: running: Set()
[2025-06-05T22:05:08.679+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: waiting: Set()
[2025-06-05T22:05:08.679+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: failed: Set()
[2025-06-05T22:05:08.710+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.711+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.711+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.711+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.711+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.712+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.712+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.714+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.715+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.715+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.715+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.715+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.715+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.716+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2025-06-05T22:05:08.751+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-06-05T22:05:08.752+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-06-05T22:05:08.752+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2025-06-05T22:05:08.811+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO BlockManagerInfo: Removed broadcast_69_piece0 on 10.255.255.254:45911 in memory (size: 15.6 KiB, free: 434.3 MiB)
[2025-06-05T22:05:08.812+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO BlockManagerInfo: Removed broadcast_67_piece0 on 10.255.255.254:45911 in memory (size: 9.2 KiB, free: 434.3 MiB)
[2025-06-05T22:05:08.813+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO BlockManagerInfo: Removed broadcast_68_piece0 on 10.255.255.254:45911 in memory (size: 11.2 KiB, free: 434.3 MiB)
[2025-06-05T22:05:08.877+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2025-06-05T22:05:08.879+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: Got job 56 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-06-05T22:05:08.879+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: Final stage: ResultStage 98 (csv at NativeMethodAccessorImpl.java:0)
[2025-06-05T22:05:08.879+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 97)
[2025-06-05T22:05:08.879+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: Missing parents: List()
[2025-06-05T22:05:08.879+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: Submitting ResultStage 98 (MapPartitionsRDD[219] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-06-05T22:05:08.912+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO MemoryStore: Block broadcast_70 stored as values in memory (estimated size 357.8 KiB, free 433.6 MiB)
[2025-06-05T22:05:08.914+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 116.6 KiB, free 433.5 MiB)
[2025-06-05T22:05:08.914+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on 10.255.255.254:45911 (size: 116.6 KiB, free: 434.2 MiB)
[2025-06-05T22:05:08.915+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO SparkContext: Created broadcast 70 from broadcast at DAGScheduler.scala:1585
[2025-06-05T22:05:08.915+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 98 (MapPartitionsRDD[219] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-06-05T22:05:08.915+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO TaskSchedulerImpl: Adding task set 98.0 with 1 tasks resource profile 0
[2025-06-05T22:05:08.917+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO TaskSetManager: Starting task 0.0 in stage 98.0 (TID 56) (10.255.255.254, executor driver, partition 0, NODE_LOCAL, 9275 bytes)
[2025-06-05T22:05:08.917+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO Executor: Running task 0.0 in stage 98.0 (TID 56)
[2025-06-05T22:05:08.932+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-06-05T22:05:08.932+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-06-05T22:05:08.932+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2025-06-05T22:05:08.996+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO ShuffleBlockFetcherIterator: Getting 1 (7.6 KiB) non-empty blocks including 1 (7.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-06-05T22:05:08.996+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-06-05T22:05:09.191+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:09 INFO FileOutputCommitter: Saved output of task 'attempt_202506052205088189749640268126103_0098_m_000000_56' to file:/home/leoja/airflow/data/quadrants.csv_tmp_csv/_temporary/0/task_202506052205088189749640268126103_0098_m_000000
[2025-06-05T22:05:09.191+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:09 INFO SparkHadoopMapRedUtil: attempt_202506052205088189749640268126103_0098_m_000000_56: Committed. Elapsed time: 0 ms.
[2025-06-05T22:05:09.193+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:09 INFO Executor: Finished task 0.0 in stage 98.0 (TID 56). 7957 bytes result sent to driver
[2025-06-05T22:05:09.194+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:09 INFO TaskSetManager: Finished task 0.0 in stage 98.0 (TID 56) in 278 ms on 10.255.255.254 (executor driver) (1/1)
[2025-06-05T22:05:09.194+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:09 INFO TaskSchedulerImpl: Removed TaskSet 98.0, whose tasks have all completed, from pool
[2025-06-05T22:05:09.195+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:09 INFO DAGScheduler: ResultStage 98 (csv at NativeMethodAccessorImpl.java:0) finished in 0.315 s
[2025-06-05T22:05:09.195+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:09 INFO DAGScheduler: Job 56 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-05T22:05:09.196+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 98: Stage finished
[2025-06-05T22:05:09.196+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:09 INFO DAGScheduler: Job 56 finished: csv at NativeMethodAccessorImpl.java:0, took 0.318236 s
[2025-06-05T22:05:09.196+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:09 INFO FileFormatWriter: Start to commit write Job 4b59e43b-8af3-4a46-97f5-8e7c14acc231.
[2025-06-05T22:05:09.220+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:09 INFO FileFormatWriter: Write Job 4b59e43b-8af3-4a46-97f5-8e7c14acc231 committed. Elapsed time: 23 ms.
[2025-06-05T22:05:09.220+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:09 INFO FileFormatWriter: Finished processing stats for write job 4b59e43b-8af3-4a46-97f5-8e7c14acc231.
[2025-06-05T22:05:09.221+0200] {spark_submit.py:521} INFO - > Gnr un fichier unique CSV     : /home/leoja/airflow/data/quadrants.csv
[2025-06-05T22:05:09.222+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:09 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-06-05T22:05:09.234+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:09 INFO SparkUI: Stopped Spark web UI at http://10.255.255.254:4040
[2025-06-05T22:05:09.252+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-06-05T22:05:09.274+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:09 INFO MemoryStore: MemoryStore cleared
[2025-06-05T22:05:09.274+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:09 INFO BlockManager: BlockManager stopped
[2025-06-05T22:05:09.337+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:09 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-06-05T22:05:09.340+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:09 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-06-05T22:05:09.355+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:09 INFO SparkContext: Successfully stopped SparkContext
[2025-06-05T22:05:09.791+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:09 INFO ShutdownHookManager: Shutdown hook called
[2025-06-05T22:05:09.792+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-200081f5-d33f-45f0-9354-93d74f1862f5
[2025-06-05T22:05:09.795+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-200081f5-d33f-45f0-9354-93d74f1862f5/pyspark-1cad4e53-8bb4-411e-ae5b-c804b0e99d93
[2025-06-05T22:05:09.799+0200] {spark_submit.py:521} INFO - 25/06/05 22:05:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-85457beb-7f7a-40de-aaff-387f2b1b0821
[2025-06-05T22:05:09.869+0200] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-06-05T22:05:09.877+0200] {taskinstance.py:1205} INFO - Marking task as SUCCESS. dag_id=macro_trading_dag, task_id=compute_economic_quadrants, execution_date=20250605T200106, start_date=20250605T200448, end_date=20250605T200509
[2025-06-05T22:05:09.919+0200] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2025-06-05T22:05:09.934+0200] {taskinstance.py:3482} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-06-05T22:05:09.944+0200] {local_task_job_runner.py:222} INFO - ::endgroup::
